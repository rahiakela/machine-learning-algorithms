{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-network-algorithm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEdNmS0dMmymGwccJuyQhb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/neural_network_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfc_hzODnLG"
      },
      "source": [
        "##Neural Network Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1s7BmtkDqgt"
      },
      "source": [
        "Artificial neural network is a supervised learning algorithm that leverages a mix of multiple hyper-parameters that help in approximating complex relation between input and output. Some of the hyper-parameters in artificial neural network include the following:\n",
        "\n",
        "- Number of hidden layers\n",
        "- Number of hidden units\n",
        "- Activation function\n",
        "- Learning rate\n",
        "\n",
        "Neural networks came about from the fact that not everything can be approximated\n",
        "by a linear/logistic regression—there may be potentially complex shapes within data that can only be approximated by complex functions. The more complex the function (with some way to take care of overfitting), the better is the accuracy of predictions. We’ll start by looking at how neural networks work toward fitting data into a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvdEriVeJXJJ"
      },
      "source": [
        "##Structure of a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqqXHfBiJYAh"
      },
      "source": [
        "The input level/layer in the figure is typically the independent variables that are used to predict the output (dependent variable) level/layer. Typically in a regression problem, there will be only one node in output layer and in a classification problem, the output layer contains as many nodes as the number of classes (distinct values) present in dependent variable.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-1.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "The hidden level/layer is used to transform the input variables into a higher order function. The way the hidden layer transforms the output is shown:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-2.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "$x_1$ and $x_2$ are the independent variables, and $b_0$ is the bias term\n",
        "(similar to the bias in linear/logistic regression). $w_1$ and $w_2$ are the weights given to each of the input variables. \n",
        "\n",
        "If $a$ is one of the units/neurons in hidden layer, it is equal to the following:\n",
        "\n",
        "\n",
        "$$ a = f  \\left( \\sum_{i=0}^N w_i x_i \\right) $$\n",
        "\n",
        "The function in the preceding equation is the activation function we are applying on top of the summation so that we attain non-linearity (we need non-linearity so that our model can now learn complex patterns).\n",
        "\n",
        "Moreover, having more than one hidden layer helps in achieving high non-linearity. We want to achieve high non-linearity because without it, a neural network would be a giant linear function.\n",
        "\n",
        "Hidden layers are necessary when the neural network has to make sense of\n",
        "something very complicated, contextual, or non-obvious, like image recognition. The term deep learning comes from having many hidden layers. These layers are known as hidden because they are not visible as a network output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjR8MfVZNxb-"
      },
      "source": [
        "##Training a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLe0kaQUODt1"
      },
      "source": [
        "Training a neural network basically means calibrating all the weights by repeating two key steps: \n",
        "\n",
        "- forward propagation \n",
        "- back propagation\n",
        "\n",
        "In forward propagation, we apply a set of weights to the input data and calculate an output. For the first forward propagation, the set of weights’ values are initialized randomly.\n",
        "\n",
        "In back propagation, we measure the margin of error of the output and adjust the\n",
        "weights accordingly to decrease the error.\n",
        "\n",
        "Neural networks repeat both forward and back propagation until the weights are\n",
        "calibrated to accurately predict an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhkTfuQeOqBu"
      },
      "source": [
        "###Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G649t8XVOq0F"
      },
      "source": [
        "Let’s go through a simple example of training a neural network to function as an\n",
        "exclusive or (XOR) operation to illustrate each step in the training process. The XOR function can be represented by the mapping of the inputs and outputs, as shown in the following table, which we’ll use as training data. It should provide a correct output given any input acceptable by the XOR function.\n",
        "\n",
        "| Input | Output |\n",
        "| ---   | ---    |\n",
        "| (0, 0) |   0   |\n",
        "| (0, 1) |   1   |\n",
        "| (1, 0) |   1   |\n",
        "| (1, 1) |   0   |\n",
        "\n",
        "Let’s use the last row from the preceding table, (1,1) => 0, to demonstrate forward propagation. Note that, while this is a classification problem,\n",
        "we will still treat it as a regression problem, only to understand how forward and back propagation work.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-3.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "We now assign weights to all the synapses. Note that these weights are selected\n",
        "randomly (the most common way is based on Gaussian distribution) since it is the\n",
        "first time we’re forward propagating. The initial weights are randomly assigned to be between 0 and 1 (but note that the final weights don’t need to be between 0 and 1).\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-4.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "\n",
        "We sum the product of the inputs with their corresponding set of weights to arrive at the first values for the hidden layer. You can think of the weights as\n",
        "measures of influence that the input nodes have on the output:\n",
        "\n",
        "```\n",
        "1 × 0.8 + 1 × 0.2 = 1\n",
        "1 × 0.4 + 1 × 0.9 = 1.3\n",
        "1 × 0.3 + 1 × 0.5 = 0.8\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-5.png?raw=1' hieght='200' width='400'/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epluYAd2SuP-"
      },
      "source": [
        "### Applying the Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxf9LKEvTFE9"
      },
      "source": [
        "Activation functions are applied at the hidden layer of a neural network. The purpose of an activation function is to transform the input signal into an output signal. They are necessary for neural networks to model complex non-linear patterns that simpler models might miss.\n",
        "\n",
        "For our example, let’s use the sigmoid function for activation. And applying\n",
        "Sigmoid(x) to the three hidden layer sums, we get.\n",
        "\n",
        "```\n",
        "Sigmoid(1.0) = 0.731\n",
        "Sigmoid(1.3) = 0.785\n",
        "Sigmoid(0.8) = 0.689\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-6.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "Then we sum the product of the hidden layer results with the second set of weights (also determined at random the first time around) to determine the output sum:\n",
        "\n",
        "```\n",
        "0.73 × 0.3 + 0.79 × 0.5 + 0.69 × 0.9 = 1.235\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-7.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "Because we used a random set of initial weights, the value of the output neuron is off the mark—in this case, by 1.235 (since the target is 0).\n",
        "\n",
        "Now we have these followings matrix:\n",
        "\n",
        "1. The input layer has two inputs (1,1), thus input layer is of\n",
        "dimension of 1 × 2 (because every input has two different values).\n",
        "2. The 1 × 2 hidden layer is multiplied with a randomly initialized weight\n",
        "matrix of dimension 2 × 3.\n",
        "3. The output of hidden layer is a 1 × 3 matrix.\n",
        "\n",
        "So, we can visualize whole network as a matrix multiplication:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/pro-machine-learning-algorithm/07-artificial-neural-network/images/ann-8.png?raw=1' hieght='200' width='400'/>\n",
        "\n",
        "The output of the activation function is multiplied by a 3 × 1 dimensional randomly initialized weight matrix to get an output that is 1 × 1 in dimension.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7exA8o1CSk8u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}