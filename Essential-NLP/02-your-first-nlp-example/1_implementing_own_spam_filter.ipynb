{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-implementing-own-spam-filter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEOTzygqi6ngaPhgQ2Th2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/1_implementing_own_spam_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Implementing own spam filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "In this notebook, you use the spam filtering as your practical NLP application as it is an example of a very widely spread family of tasks – text classification. Text classification comprises a number of applications, for example user profiling, sentiment analysis and topic labeling, so this\n",
        "will give you a good start for the rest of the book. \n",
        "\n",
        "First, let’s see what exactly classification addresses.\n",
        "\n",
        "We, humans apply classification in our everyday lives pretty regularly: classifying things simply implies that we try to put them into clearly defined groups, classes or categories. \n",
        "\n",
        "In fact, we tend to classify all sorts of things all the time. Here are some examples:\n",
        "\n",
        "- based on our level of engagement and interest in a movie, we may classify it as interesting or boring;\n",
        "- based on temperature, we classify water as cold or hot;\n",
        "- based on the amount of sunshine, humidity, wind strength and air temperature, we classify the weather as good or bad;\n",
        "- based on the number of wheels, we classify vehicles into unicycles, bicycles, tricycles, quadricycles, cars and so on;\n",
        "- based on the availability of the engine, we may classify two-wheeled vehicles into bicycles and motorcycles.\n",
        "\n",
        "Classification is useful because it makes it easier for us to reason about things and adjust our behavior accordingly.\n",
        "\n",
        "When classifying things, we often go for simple contrasts – good vs. bad, interesting vs. boring, hot vs. cold. When we are dealing with two labels only, this is called binary classification.\n",
        "\n",
        "Classification that implies more than two classes is called multi-class classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import NaiveBayesClassifier, classify\n",
        "from nltk.text import Text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "e900395f-bbdf-404c-b997-6d982bc3c19e"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/enron1.zip\n",
        "unzip -qq enron1.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Define the data and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "Enron email dataset is a large dataset of emails (the original dataset contains about 0.5M messages), including both ham and spam emails, for about 150 users, mostly senior management of Enron.\n",
        "\n",
        "We are going to use enron1/ folder for training. All folders in Enron\n",
        "dataset contain spam and ham emails in separate subfolders, so you don’t need to worry about pre-defining them. Each email is stored as a text file in these subfolders. \n",
        "\n",
        "Let’s read in the contents of these text files in each subfolder, store the spam emails contents and the ham emails contents as two separate data structures and point our algorithm at each, clearly\n",
        "defining which one is spam and which one is ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_files(folder):\n",
        "  files = os.listdir(folder)\n",
        "  a_list = []\n",
        "  for a_file in files:\n",
        "    # Skip hidden files, that are sometimes automatically created by the operating systems. They can be easily identified because their names start with “.”\n",
        "    if not a_file.startswith(\".\"):\n",
        "      file = codecs.open(folder + a_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
        "      a_list.append(file.read())\n",
        "      file.close()\n",
        "  return a_list"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "Now you can define two such lists – spam_list and ham_list, letting the machine know what data to use as examples of spam emails and what data represents ham emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u2XdijC_8n",
        "outputId": "9e891e8f-0fee-4097-d6ef-422b16fccc26"
      },
      "source": [
        "spam_list = read_files(\"enron1/spam/\")\n",
        "ham_list = read_files(\"enron1/ham/\")\n",
        "\n",
        "# Check the lengths of the lists: for spam it should be 1500 and for ham – 3672\n",
        "print(len(spam_list))\n",
        "print(len(ham_list))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n",
            "3672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Let's check out the contents of the first entry. In both cases, it should coincide with the contents of the first file in each correspondent subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjDZk7qDw6v",
        "outputId": "2b3a216f-17aa-43cd-db2a-a56791ee591d"
      },
      "source": [
        "print(spam_list[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: re: clement\r\n",
            "Hassan\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "b2cf799a-eb3e-4dcd-c192-322150868161"
      },
      "source": [
        "print(ham_list[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: enron/hpl actuals for sept. 28, 2000\r\n",
            "Teco tap 60. 000/enron; 90. 000. Hpl gas daily\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "Next, you’ll need to preprocess the data (e.g., by splitting text strings into words) and extract the features.\n",
        "\n",
        "Finally, remember that you will need to split the data randomly into the\n",
        "training and test sets. \n",
        "\n",
        "Let’s shuffle the resulting list of emails with their labels, and make\n",
        "sure that the shuffle is reproducible by fixing the way in which the data is shuffled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RiJyKNlEkja",
        "outputId": "5d6e5866-fdc2-4eee-bf4d-7728cc7c1e92"
      },
      "source": [
        "# for each member of the ham_list and spam_list it stores a tuple with the content and associated label\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "\n",
        "# By defining the seed for the random operator you can make sure that all future runs will shuffle the data in the same way\n",
        "random.seed(2020)\n",
        "random.shuffle(all_emails)\n",
        "\n",
        "# it should be equal to 1500 + 3672 = 5172\n",
        "print(f\"Dataset size = {str(len(all_emails))} emails\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size = 5172 emails\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Split the text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Remember, that the email contents that you’ve read in so far each come as a single string of symbols. The first step of text preprocessing involves splitting the running text into words.\n",
        "\n",
        "You are going to use NLTK’s tokenizer. It takes running text as input and returns a list of words based on a number of customized regular expressions, which help to delimit the text by whitespaces and punctuation marks, keeping common words like “U.S.A.” unsplit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def tokenize(sent):\n",
        "  word_list = []\n",
        "  for word in word_tokenize(sent):\n",
        "    word_list.append(word)\n",
        "  return word_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "outputId": "ece438be-7a7c-4451-8495-de564f758656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input = \"What's the best way to split a sentence into words?\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHUAMWe1IRKM",
        "outputId": "95de1115-cff2-4ae8-a65f-90e1c80f4831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input = \"I live in U.S.A country.\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'live', 'in', 'U.S.A', 'country', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Extract and normalize the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        ""
      ]
    }
  ]
}