{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-implementing-own-spam-filter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/ytCBk7UEmpTAPjMqv9VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/1_implementing_own_spam_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Implementing own spam filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "In this notebook, you use the spam filtering as your practical NLP application as it is an example of a very widely spread family of tasks – text classification. Text classification comprises a number of applications, for example user profiling, sentiment analysis and topic labeling, so this\n",
        "will give you a good start for the rest of the book. \n",
        "\n",
        "First, let’s see what exactly classification addresses.\n",
        "\n",
        "We, humans apply classification in our everyday lives pretty regularly: classifying things simply implies that we try to put them into clearly defined groups, classes or categories. \n",
        "\n",
        "In fact, we tend to classify all sorts of things all the time. Here are some examples:\n",
        "\n",
        "- based on our level of engagement and interest in a movie, we may classify it as interesting or boring;\n",
        "- based on temperature, we classify water as cold or hot;\n",
        "- based on the amount of sunshine, humidity, wind strength and air temperature, we classify the weather as good or bad;\n",
        "- based on the number of wheels, we classify vehicles into unicycles, bicycles, tricycles, quadricycles, cars and so on;\n",
        "- based on the availability of the engine, we may classify two-wheeled vehicles into bicycles and motorcycles.\n",
        "\n",
        "Classification is useful because it makes it easier for us to reason about things and adjust our behavior accordingly.\n",
        "\n",
        "When classifying things, we often go for simple contrasts – good vs. bad, interesting vs. boring, hot vs. cold. When we are dealing with two labels only, this is called binary classification.\n",
        "\n",
        "Classification that implies more than two classes is called multi-class classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import NaiveBayesClassifier, classify\n",
        "from nltk.text import Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2e7c15-b866-46ad-f02a-80e2b6af3d4b"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "78f02045-1b2e-49a2-b51a-c2e35014ea36"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/enron1.zip\n",
        "unzip -qq enron1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Define the data and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "Enron email dataset is a large dataset of emails (the original dataset contains about 0.5M messages), including both ham and spam emails, for about 150 users, mostly senior management of Enron.\n",
        "\n",
        "We are going to use enron1/ folder for training. All folders in Enron\n",
        "dataset contain spam and ham emails in separate subfolders, so you don’t need to worry about pre-defining them. Each email is stored as a text file in these subfolders. \n",
        "\n",
        "Let’s read in the contents of these text files in each subfolder, store the spam emails contents and the ham emails contents as two separate data structures and point our algorithm at each, clearly\n",
        "defining which one is spam and which one is ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_files(folder):\n",
        "  files = os.listdir(folder)\n",
        "  a_list = []\n",
        "  for a_file in files:\n",
        "    # Skip hidden files, that are sometimes automatically created by the operating systems. They can be easily identified because their names start with “.”\n",
        "    if not a_file.startswith(\".\"):\n",
        "      file = codecs.open(folder + a_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
        "      a_list.append(file.read())\n",
        "      file.close()\n",
        "  return a_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "Now you can define two such lists – spam_list and ham_list, letting the machine know what data to use as examples of spam emails and what data represents ham emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u2XdijC_8n",
        "outputId": "7aab5e49-8c7e-427a-aa50-a74d46ef43dc"
      },
      "source": [
        "spam_list = read_files(\"enron1/spam/\")\n",
        "ham_list = read_files(\"enron1/ham/\")\n",
        "\n",
        "# Check the lengths of the lists: for spam it should be 1500 and for ham – 3672\n",
        "print(len(spam_list))\n",
        "print(len(ham_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "3672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Let's check out the contents of the first entry. In both cases, it should coincide with the contents of the first file in each correspondent subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjDZk7qDw6v",
        "outputId": "454b9bf8-2f10-46dc-f401-aecdc5c73f7a"
      },
      "source": [
        "print(spam_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: cia. Lis available for next 100 customers\r\n",
            "HI again,\r\n",
            "We now have over 99 meds available online now!\r\n",
            "We are having specials on xanax, vlagra, soma, amblen and vallum\r\n",
            "Free clalls with every order\r\n",
            "More lnfo here\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "406bd1d9-db83-4307-bc64-d838bfbc655e"
      },
      "source": [
        "print(ham_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: new meter number\r\n",
            "Please be advised, as discussed in the morning meeting this morning, that\r\n",
            "Tetco enerfin' s new meter 98 - 0439 will not be operable until october 2,\r\n",
            "2000. This meter is assisgned to me.\r\n",
            "Thanks\r\n",
            "- jackie -\r\n",
            "3 - 9497\r\n",
            "- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young/hou/ect on 09/28/2000 09: 05\r\n",
            "Am - - - - - - - - - - - - - - - - - - - - - - - - - - -\r\n",
            "Enron north america corp.\r\n",
            "From: pat clynes@ enron 09/12/2000 08: 52 am\r\n",
            "To: aimee lannou/hou/ect@ ect, carlos j rodriguez/hou/ect@ ect, jackie\r\n",
            "Young/hou/ect@ ect, robert e lloyd/hou/ect@ ect, tom acton/corp/enron@ enron,\r\n",
            "Sabrae zajac/hou/ect@ ect, mark mccoy/corp/enron@ enron, clem\r\n",
            "Cernosek/hou/ect@ ect, robert cotten/hou/ect@ ect, eddie janzen/na/enron@ enron,\r\n",
            "Mary poorman/na/enron@ enron, susan hadix/na/enron@ enron\r\n",
            "Cc: gary a hanks/hou/ect@ ect, earl tisdale/hou/ect@ ect, james\r\n",
            "Mckay/hou/ect@ ect, laurI a allen/hou/ect@ ect, daren j farmer/hou/ect@ ect,\r\n",
            "Edward d gottlob/hou/ect@ ect, liz bellamy/na/enron@ enron\r\n",
            "Subject: new meter number\r\n",
            "Tetco enerfin is going to be assigned meter #0439. Activity will be\r\n",
            "Scheduled under this meter for october lst.\r\n",
            "Jackie young is assigned this meter. Thanks, pat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "Next, you’ll need to preprocess the data (e.g., by splitting text strings into words) and extract the features.\n",
        "\n",
        "Finally, remember that you will need to split the data randomly into the\n",
        "training and test sets. \n",
        "\n",
        "Let’s shuffle the resulting list of emails with their labels, and make\n",
        "sure that the shuffle is reproducible by fixing the way in which the data is shuffled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RiJyKNlEkja",
        "outputId": "04d3c27d-d1fa-4c7d-b13d-ab9b37ff33f8"
      },
      "source": [
        "# for each member of the ham_list and spam_list it stores a tuple with the content and associated label\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "\n",
        "# By defining the seed for the random operator you can make sure that all future runs will shuffle the data in the same way\n",
        "random.seed(2020)\n",
        "random.shuffle(all_emails)\n",
        "\n",
        "# it should be equal to 1500 + 3672 = 5172\n",
        "print(f\"Dataset size = {str(len(all_emails))} emails\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size = 5172 emails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Split the text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Remember, that the email contents that you’ve read in so far each come as a single string of symbols. The first step of text preprocessing involves splitting the running text into words.\n",
        "\n",
        "You are going to use NLTK’s tokenizer. It takes running text as input and returns a list of words based on a number of customized regular expressions, which help to delimit the text by whitespaces and punctuation marks, keeping common words like “U.S.A.” unsplit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def tokenize(sent):\n",
        "  word_list = []\n",
        "  for word in word_tokenize(sent):\n",
        "    word_list.append(word)\n",
        "  return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333f1819-9636-4acd-9dc6-fa11c75e5771"
      },
      "source": [
        "input = \"What's the best way to split a sentence into words?\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHUAMWe1IRKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0750c52a-dc51-4d01-df16-3b69be9c7434"
      },
      "source": [
        "input = \"I live in U.S.A country.\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'live', 'in', 'U.S.A', 'country', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Extract and normalize the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        "Once the words are extracted from running text, you need to convert them into features. In particular, you need to put all words into lower case to make your algorithm establish the connection between different formats like “Lottery” and “lottery”.\n",
        "\n",
        "Putting all strings to lower case can be achieved with Python’s string functionality. To extract the features (words) from the text, you need to iterate through the recognized words and put all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmupbFaDWCl1"
      },
      "source": [
        "def get_features(text):\n",
        "  features = {}\n",
        "  word_list = [word for word in word_tokenize(text.lower())]\n",
        "  # For each word in the email let’s switch on the ‘flag’ that this word is contained in the email\n",
        "  for word in word_list:\n",
        "    features[word] = True\n",
        "  \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Lb5pktWv5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0004b1-19ba-4922-f8bc-72187d7d6133"
      },
      "source": [
        "# it will keep tuples containing the list of features matched with the “spam” or “ham” label for each email\n",
        "all_features = [(get_features(email), label) for (email, label) in all_emails]\n",
        "\n",
        "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
        "\n",
        "print(len(all_features))\n",
        "print(len(all_features[0][0]))\n",
        "print(len(all_features[99][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
            "5172\n",
            "51\n",
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZRPIDsSaE8E"
      },
      "source": [
        "With this bit of code, you iterate over the emails in your collection (all_emails) and store the list of features extracted from each email matched with the label.\n",
        "\n",
        "For example, if a spam email consists of a single sentence “Participate In Our New Lottery NOW!” your algorithm will first extract the list of features present in this email and assign a ‘True’ value to each of them.\n",
        "\n",
        "Then, the algorithm will add this list of features to\n",
        "all_features together with the “spam” label.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Imagine your whole dataset contained only one spam text “Participate In Our New Lottery NOW!” and one ham text “Participate in the Staff Survey”. What features will be extracted from this dataset?\n",
        "\n",
        "You will end up with the following feature set:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Let’s now clarify what each tuple structure representing an email contains. Tuples pair up two information fields: in this case a list of features extracted from the email and its label, i.e. each tuple in `all_features` contains a pair (`list_of_features`, `label`).\n",
        "\n",
        "So if you’d like to access first email in the list, you call on `all_features[0]`, to access its list of features you use `all_features[0][0]`, and to access its label you use `all_features[0][1]`.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr356828cl5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9629bb08-1d9a-453e-bcbc-939af209b39b"
      },
      "source": [
        "# access first email in the list with feature and label\n",
        "all_features[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({',': True,\n",
              "  '-': True,\n",
              "  '//': True,\n",
              "  '23': True,\n",
              "  ':': True,\n",
              "  '^': True,\n",
              "  'alephanimism': True,\n",
              "  'article': True,\n",
              "  'balsamexpertise': True,\n",
              "  'bib': True,\n",
              "  'biennial': True,\n",
              "  'bloomington': True,\n",
              "  'burp..': True,\n",
              "  'checksumnodal': True,\n",
              "  'chronic': True,\n",
              "  'circumspectbrackish': True,\n",
              "  'codon': True,\n",
              "  'com': True,\n",
              "  'com/please': True,\n",
              "  'committeehoward': True,\n",
              "  'cosponsor': True,\n",
              "  'drearybuckthorn': True,\n",
              "  'electrolyte': True,\n",
              "  'fda': True,\n",
              "  'finn': True,\n",
              "  'hrothgar': True,\n",
              "  'http': True,\n",
              "  'intercept': True,\n",
              "  'it': True,\n",
              "  'january': True,\n",
              "  'living': True,\n",
              "  'mcculloughbenelux': True,\n",
              "  'mire..': True,\n",
              "  'moeallegiant': True,\n",
              "  'monstrousanatomic': True,\n",
              "  'n': True,\n",
              "  'nebulousconfute': True,\n",
              "  'neuro': True,\n",
              "  'pain': True,\n",
              "  'parentheses': True,\n",
              "  'pen': True,\n",
              "  'plump': True,\n",
              "  'r': True,\n",
              "  'selllacquer': True,\n",
              "  'start': True,\n",
              "  'stimulantbettor': True,\n",
              "  'subject': True,\n",
              "  'tests': True,\n",
              "  'th': True,\n",
              "  'v': True,\n",
              "  'with': True},\n",
              " 'spam')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FS65Hbmcq_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd10aea-3a61-44ef-88c0-382c4ae77b6b"
      },
      "source": [
        "# access its list of features only\n",
        "all_features[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': True,\n",
              " '-': True,\n",
              " '//': True,\n",
              " '23': True,\n",
              " ':': True,\n",
              " '^': True,\n",
              " 'alephanimism': True,\n",
              " 'article': True,\n",
              " 'balsamexpertise': True,\n",
              " 'bib': True,\n",
              " 'biennial': True,\n",
              " 'bloomington': True,\n",
              " 'burp..': True,\n",
              " 'checksumnodal': True,\n",
              " 'chronic': True,\n",
              " 'circumspectbrackish': True,\n",
              " 'codon': True,\n",
              " 'com': True,\n",
              " 'com/please': True,\n",
              " 'committeehoward': True,\n",
              " 'cosponsor': True,\n",
              " 'drearybuckthorn': True,\n",
              " 'electrolyte': True,\n",
              " 'fda': True,\n",
              " 'finn': True,\n",
              " 'hrothgar': True,\n",
              " 'http': True,\n",
              " 'intercept': True,\n",
              " 'it': True,\n",
              " 'january': True,\n",
              " 'living': True,\n",
              " 'mcculloughbenelux': True,\n",
              " 'mire..': True,\n",
              " 'moeallegiant': True,\n",
              " 'monstrousanatomic': True,\n",
              " 'n': True,\n",
              " 'nebulousconfute': True,\n",
              " 'neuro': True,\n",
              " 'pain': True,\n",
              " 'parentheses': True,\n",
              " 'pen': True,\n",
              " 'plump': True,\n",
              " 'r': True,\n",
              " 'selllacquer': True,\n",
              " 'start': True,\n",
              " 'stimulantbettor': True,\n",
              " 'subject': True,\n",
              " 'tests': True,\n",
              " 'th': True,\n",
              " 'v': True,\n",
              " 'with': True}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOwXVnfdHdp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cbe5679b-af3a-4938-a611-ddc5a50db71e"
      },
      "source": [
        "# access its label only\n",
        "all_features[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkWOZNfdMC0"
      },
      "source": [
        "##Step 4: Train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSAh72HdP-X"
      },
      "source": [
        "Next, let’s apply machine learning and teach the machine to distinguish between the features that describe each of the two classes. There are a number of classification algorithms that you can use, let’s start with one of the most interpretable ones – an algorithm called **Naïve Bayes**. Don’t be misled by the word “Naïve” in its title, though: despite relative simplicity of the approach compared to other ones, this algorithm often works well in practice\n",
        "and sets a competitive performance baseline that is hard to beat with more sophisticated approaches.\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier, which means that it makes the class prediction based on the estimate of which outcome is most likely: i.e., it assesses the probability of an\n",
        "email being spam and compares it with the probability of it being ham, and then selects the outcome that is most probable between the two.\n",
        "\n",
        "In the previous step, you extracted the content of the email and converted it into a list of individual words (features). In this step, the machine will try to predict whether the email content represents spam or ham. \n",
        "\n",
        "In other words, it will try to predict whether the email is spam or ham given or conditioned on its content. This type of probability, when the outcome (class of “spam” or “ham”) depends on the condition (words used as features), is called conditional probability. \n",
        "\n",
        "For spam detection, you estimate `P(spam | email content)` and `P(ham | email content)`, or generally `P(outcome | (given) condition)`.Then you compare one estimate to another and return the most probable class.\n",
        "\n",
        "```python\n",
        "If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam\n",
        "If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "A machine can estimate the probability that an email is spam or ham conditioned on its content taking the number of times it has seen this content leading to a particular outcome.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are spam) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "\n",
        "P(ham | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are ham) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "```\n",
        "\n",
        "In the general form, this can be expressed as:\n",
        "\n",
        "```\n",
        "P(outcome | condition) = number_of_times(condition led to outcome) number_of_times(condition applied)\n",
        "```\n",
        "\n",
        "Remember that you used tokenization to split long texts into separate words to let the algorithm access the smaller bits of information – words rather than whole sequences. The idea of estimating probabilities based on separate features rather than based on the whole sequence of features (whole text) is somewhat similar.\n",
        "\n",
        "In the previous step, you converted this single text into a set of features as:\n",
        "\n",
        "```['participate': True, 'in': True, …, 'now': True, '!': True]``` \n",
        "\n",
        "Note that the conditional probabilities like:\n",
        "\n",
        "``` \n",
        "P(spam| \"Participate in our lottery now!\") and P(spam| ['participate': True,\n",
        "‘in’: True, …, ‘now’: True, ‘!’: True])\n",
        "```\n",
        "\n",
        "are the same because this set of features encodes the text.\n",
        "\n",
        "Is there a way to split this set to get at more fine-grained, individual probabilities, for example to establish a link between `[‘lottery’: True]` and the class of “spam”?\n",
        "\n",
        "Unfortunately, there is no way to split the conditional probability estimation like `P(outcome | conditions)` when there are multiple conditions specified, however it is possible to split the probability estimation like `P(outcomes | condition)` when there is a single condition and multiple outcomes.\n",
        "\n",
        "In spam detection, the class is a single value (it is “spam” or “ham”), while features are a set `([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’:\n",
        "True])`. If you can flip around the single value of class and the set of features in such a way that the class becomes the new condition and the features become the new outcomes, you can split the probability into smaller components and establish the link between individual features like `[‘lottery’: True]` and class values like “spam”.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "Luckily, there is a way to flip the outcomes (class) and conditions (features extracted from the content) around!\n",
        "\n",
        "Let’s look into the estimation of conditional probabilities again: you\n",
        "estimate the probability that the email is spam given that its content is “Participate in our new lottery now!” based on how often in the past an email with such content was spam. For that, you take the proportion of the times you have seen “Participate in our new lottery now!” in a spam email among the emails with this content.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "Similarly to how you estimated the probabilities above, you need the proportion of times you have seen “Participate in our new lottery now!” in a spam email among all spam emails.\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" | spam) = P(\"Participate in our new lottery now!\" is used in a spam email) / P(an email is spam)\n",
        "```\n",
        "\n",
        "That is, every time you use conditional probabilities, you need to\n",
        "divide how likely it is that you see the condition and outcome together by how likely it is that you see the condition on its own – this is the bit after |.\n",
        "\n",
        "Now you can see that both Formulas 1 and 2 rely on how often you see particular content in an email of particular class. They share this bit, so you can use it to connect the two formulas. For instance, from Formula 2 you know that:\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" is used in a spam email) = P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)\n",
        "```\n",
        "\n",
        "Now you can fit this into Formula 1:\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email) = [P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)] / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "In the general form:\n",
        "\n",
        "```\n",
        "P(class | content) = P(content represents class) / P(content) = [P(content | class) * P(class)] / P(content)\n",
        "```\n",
        "\n",
        "In other words, you can express the probability of a class given email content via the probability of the content given the class.\n",
        "\n",
        "Now you can replace the conditional probability of `P(class | content)` with `P(content | class)`, e.g. whereas before you had to calculate `P(“spam” | “Participate in our new lottery now!”)` or equally `P(“spam” | [‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True])`, which is hard to do because you will often end up with too few examples of exactly the same email content or exactly the same combination of features, now you can estimate `P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”)` instead.\n",
        "\n",
        "But how does this solve the problem? Aren’t you still dealing with a long sequence of features?\n",
        "\n",
        "Here is where the “naïve” assumption in Naïve Bayes helps: it assumes that the features are independent of each other, or that your chances of seeing a word “lottery” in an email are independent of seeing a word “new” or any other word in this email before. So you can estimate the probability of the whole sequence of features given a class as a product of probabilities of each feature given this class.\n",
        "\n",
        "```\n",
        "P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”) = P(‘participate’: True | “spam”) * P(‘in’: True | “spam”) … * P(‘!’: True | “spam”)\n",
        "```\n",
        "\n",
        "If you express `[‘participate’: True]` as the first feature in the feature list, or `f1`, `[‘in’: True]` as `f2`, and so on, until `fn = [‘!’: True]`, you can use the general formula:\n",
        "\n",
        "```\n",
        "P([f1, f2, …, fn] | class) = P(f1 | class) * P(f2| class) … * P(fn| class)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you have broken down the probability of the whole feature list given class into the probabilities for each word given that class, how do you actually estimate them?\n",
        "\n",
        "Since for each email you note which words occur in it, the total number of times you can switch on the flag `[‘feature’: True]` equals the total number of emails in that class, while the actual number of times you switch on this flag is the number of emails where this feature is actually\n",
        "present. The conditional probability `P(feature | class)` is simply the proportion of the two:\n",
        "\n",
        "```\n",
        "P(feature | class) = number(emails in class with feature present) / total_number(emails in class)\n",
        "```\n",
        "\n",
        "These numbers are easy to estimate from the training data – let’s try to do that with an example.\n",
        "\n",
        ">Suppose you have 5 spam emails and 10 ham emails. What are the conditional probabilities for P('prescription':True | spam), P('meeting':True | ham), P('stock':True | spam) and P('stock':True | ham), if:\n",
        "- 2 spam emails contain word prescription\n",
        "- 1 spam email contains word stock\n",
        "- 3 ham emails contain word stock\n",
        "- 5 ham emails contain word meeting\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "The probabilities are simply:\n",
        "- P('prescription':True | spam) = number(spam emails with 'prescription')/number(spam emails) = 2/5 = 0.40\n",
        "- P('meeting':True | ham) = 5/10 = 0.50\n",
        "- P('stock':True | spam) = 1/5 = 0.20\n",
        "- P('stock':True | ham) = 3/10 = 0.30\n",
        "\n",
        "Let’s iterate through the classification steps again: during the training phase, the algorithm learns prior class probabilities (this is simply\n",
        "class distribution, e.g. `P(ham)=0.71 and P(spam)=0.29)` and probabilities for each feature given each of the classes (this is simply the proportion of emails with each feature in each class, e.g. `P(‘meeting’:True | ham) = 0.50)`. \n",
        "\n",
        "During test phase, or when the algorithm is applied to a new email and is asked to predict its class, the following comparison from the beginning of this section is applied:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(spam | content) > P(ham | content) \n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is what we started with originally, but we said that the conditions are flipped, so it becomes:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) / P(content) > P(content | ham) * P(ham) / P(content)\n",
        "\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "Note that we end up with `P(content)` in denominator on both sides of the expression, so the absolute value of this probability doesn’t matter and it can be removed from the expression altogether. So we can simplify the expression as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) > P(content | ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "`P(spam)` and `P(ham)` are class probabilities estimated during training, and `P(content | class)`, using naïve independence assumption, are products of probabilities, so:\n",
        "\n",
        "```\n",
        "Predict “spam” if P([f1, f2, …, fn]| spam) * P(spam) > P([f1, f2, …, fn]| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "is split into the individual feature probabilities as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(f1 | spam) * P(f2| spam) … * P(fn| spam) * P(spam) > P(f1 | ham) * P(f2| ham) … * P(fn| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is the final expression the classifier relies on. The following code implements this idea.\n",
        "Since Naïve Bayes is frequently used for NLP tasks, NLTK comes with its own\n",
        "implementation, too, and here you are going to use it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JgZtFT8MWL"
      },
      "source": [
        "def train(features, proportion):\n",
        "  train_size = int(len(features) * proportion)\n",
        "  # Use the first n% (according to the specified proportion) of emails with their features for training, and the rest for testing\n",
        "  train_set, test_set = features[: train_size], features[train_size:]\n",
        "  print(f\"Training set size = {str(len(train_set))} emails\")\n",
        "  print(f\"Test set size = {str(len(test_set))} emails\")\n",
        "\n",
        "  classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "  return train_set, test_set, classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk7Zr9Z2Lw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67a49ea-4a8a-406b-e1c2-f4b607f137cb"
      },
      "source": [
        "# Apply the train function using 80% (or a similar proportion) of emails for training. \n",
        "train_set, test_set, classifier = train(all_features, 0.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size = 4137 emails\n",
            "Test set size = 1035 emails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwgAyTHA93N"
      },
      "source": [
        "##Step 5: Evaluate your classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ve6ZA8uA-nm"
      },
      "source": [
        "Finally, let’s evaluate how well the classifier performs in detecting whether an email is spam or ham. For that, let’s use the accuracy score returned by the NLTK’s classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcWqAwsl1iV"
      },
      "source": [
        "def evaluate(train_set, test_set, classifier):\n",
        "  print(f\"Accuracy on the training set = {str(classify.accuracy(classifier, train_set))}\")\n",
        "  print(f\"Accuracy of the test set = {str(classify.accuracy(classifier, test_set))}\")\n",
        "\n",
        "  # inspect the most informative features (words). You need to specify the number of the top most informative features to look into, e.g. 50 here\n",
        "  classifier.show_most_informative_features(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKdVa-gHmmuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e260fe96-52ec-422d-8a5d-e7de32950293"
      },
      "source": [
        "evaluate(train_set, test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.9625332366449117\n",
            "Accuracy of the test set = 0.9352657004830918\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    203.1 : 1.0\n",
            "                    2004 = True             spam : ham    =    152.8 : 1.0\n",
            "                     nom = True              ham : spam   =    129.1 : 1.0\n",
            "                   paste = True             spam : ham    =     88.6 : 1.0\n",
            "                    spam = True             spam : ham    =     82.3 : 1.0\n",
            "                     sex = True             spam : ham    =     79.2 : 1.0\n",
            "                     ect = True              ham : spam   =     76.9 : 1.0\n",
            "                creative = True             spam : ham    =     72.9 : 1.0\n",
            "             medications = True             spam : ham    =     72.9 : 1.0\n",
            "                featured = True             spam : ham    =     71.3 : 1.0\n",
            "                   adobe = True             spam : ham    =     69.8 : 1.0\n",
            "                     ibm = True             spam : ham    =     68.2 : 1.0\n",
            "                  differ = True             spam : ham    =     68.2 : 1.0\n",
            "                  weight = True             spam : ham    =     66.6 : 1.0\n",
            "                   epson = True             spam : ham    =     60.4 : 1.0\n",
            "                     pro = True             spam : ham    =     60.4 : 1.0\n",
            "               clearance = True             spam : ham    =     60.4 : 1.0\n",
            "                  shares = True             spam : ham    =     58.8 : 1.0\n",
            "                congress = True             spam : ham    =     58.8 : 1.0\n",
            "             legislation = True             spam : ham    =     57.2 : 1.0\n",
            "                    gary = True              ham : spam   =     57.1 : 1.0\n",
            "            solicitation = True             spam : ham    =     55.7 : 1.0\n",
            "                   cisco = True             spam : ham    =     55.7 : 1.0\n",
            "                    2005 = True             spam : ham    =     55.0 : 1.0\n",
            "             subscribers = True             spam : ham    =     54.1 : 1.0\n",
            "                 dealers = True             spam : ham    =     54.1 : 1.0\n",
            "                    2001 = True              ham : spam   =     53.3 : 1.0\n",
            "                  sexual = True             spam : ham    =     52.5 : 1.0\n",
            "                  farmer = True              ham : spam   =     51.3 : 1.0\n",
            "                inherent = True             spam : ham    =     50.9 : 1.0\n",
            "                    draw = True             spam : ham    =     50.9 : 1.0\n",
            "                      cc = True              ham : spam   =     50.6 : 1.0\n",
            "                  health = True             spam : ham    =     50.3 : 1.0\n",
            "                conflict = True             spam : ham    =     49.4 : 1.0\n",
            "                  stocks = True             spam : ham    =     47.5 : 1.0\n",
            "                   penis = True             spam : ham    =     46.2 : 1.0\n",
            "                   cheap = True             spam : ham    =     45.6 : 1.0\n",
            "                      ex = True             spam : ham    =     43.7 : 1.0\n",
            "                 doctors = True             spam : ham    =     43.1 : 1.0\n",
            "                   super = True             spam : ham    =     42.8 : 1.0\n",
            "                    anti = True             spam : ham    =     41.5 : 1.0\n",
            "                 charset = True             spam : ham    =     41.5 : 1.0\n",
            "                    lisa = True              ham : spam   =     40.1 : 1.0\n",
            "                    acts = True             spam : ham    =     40.0 : 1.0\n",
            "                 foresee = True             spam : ham    =     40.0 : 1.0\n",
            "                 generic = True             spam : ham    =     40.0 : 1.0\n",
            "                   susan = True              ham : spam   =     39.8 : 1.0\n",
            "                   steve = True              ham : spam   =     39.6 : 1.0\n",
            "                mailings = True             spam : ham    =     38.4 : 1.0\n",
            "                 advises = True             spam : ham    =     38.4 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b_hoGVnjS1"
      },
      "source": [
        "As you can see, many spam emails in this dataset are related to medications,\n",
        "which shows a particular bias – the most typical spam that you personally get might be on a different topic altogether! What effect might this mismatch between the training data from the publicly available dataset like Enron and your personal data have?\n",
        "\n",
        "One other piece of information presented in this output is accuracy. Test accuracy shows the proportion of test emails that are correctly classified by Naïve Bayes among all test emails.\n",
        "\n",
        "Note, that since the classifier is trained on the training data, it actually gets to “see” all the correct labels for the training examples. \n",
        "\n",
        "Shouldn’t it then know the correct answers and perform at 100% accuracy on the training data?\n",
        "\n",
        "Well, the point here is that the classifier doesn’t just\n",
        "retrieve the correct answers: during training it has built some probabilistic model (i.e., learned about the distribution of classes and the probability of different features), and then it applies this model to the data. So, it is actually very likely that the probabilistic model doesn’t capture all the things in the data 100% correctly.\n",
        "\n",
        "Therefore, when you run the code above, you will get accuracy on the training data of `96.13%`. This is not perfect (i.e., not `100%`) but very close to it! When you apply the same classifier to new data – the test set that the classifier hasn’t seen during training – the accuracy reflects its generalizing ability. That is, it shows whether the probabilistic assumptions it made based on the training data can be successfully applied to any other data. The accuracy on the test set is `94.20%`, which is slightly lower than that on the training set, but is also very high.\n",
        "\n",
        "Finally, if you’d like to gain any further insight into how the words are used in the emails from different classes, you can also check the occurrences of any particular word in all available contexts.\n",
        "\n",
        "For example, word “stocks” features as a very strong predictor of spam\n",
        "messages. Why is that? You might be thinking, “OK, some emails containing “stocks” will be spam, but surely there must be contexts where “stocks” is used in a completely harmless way?”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyH5N-6NqeWt"
      },
      "source": [
        "def concordance(data_list, search_word):\n",
        "  for email in data_list:\n",
        "    word_list = [word for word in word_tokenize(email.lower())]\n",
        "    text_list = Text(word_list)\n",
        "\n",
        "    \"\"\"\n",
        "    “Concordancer” is a tool that checks for the occurrences of the specified word and prints out the word in\n",
        "    its context. By default, NLTK’s concordancer prints out the search_word surrounded by the previous\n",
        "    36 and the following 36 characters – so note, that it doesn’t always result in full words\n",
        "    \"\"\"\n",
        "    if search_word in word_list:\n",
        "      text_list.concordance(search_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymrq-3u8rY7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cfe86d-45a5-4827-dafd-782b7c664bb4"
      },
      "source": [
        "# Apply this function to two lists – ham_list and spam_list – to find out about the different contexts of use for the word “stocks”\n",
        "print(\"STOCKS in HAM:\")\n",
        "concordance(ham_list, \"stocks\")\n",
        "print(\"\\n\\nSTOCKS in SPAM:\")\n",
        "concordance(spam_list, \"stocks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOCKS in HAM:\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ad my portfolio is diversified into stocks that have lost even more money than\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "\n",
            "\n",
            "STOCKS in SPAM:\n",
            "Displaying 4 of 4 matches:\n",
            "                                    stocks newsletter u r g e n t i n v e s t \n",
            "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 6 of 6 matches:\n",
            " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
            " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
            " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "y agree , some , not all , of these stocks move in price because they are prom\n",
            "tands or that as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been really moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
            "Displaying 4 of 4 matches:\n",
            "watch this one trade . these little stocks can surprise in a big way sometimes\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 2 of 2 matches:\n",
            "                                    stocks are about timing nomad internationa\n",
            " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
            "Displaying 3 of 3 matches:\n",
            "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
            "Displaying 2 of 2 matches:\n",
            "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 2 of 2 matches:\n",
            "rt identifying defense and security stocks ready to explode look at the moves \n",
            " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 2 of 2 matches:\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 6 of 6 matches:\n",
            "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
            "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
            "ynamic things . some of these small stocks have absolutely exploded in price r\n",
            "'' occur . as with many micro - cap stocks , today ' s company has additional \n",
            " ema - il pertaining to investing , stocks or securities must be understood as\n",
            "ntative before deciding to trade in stocks featured within this ema - il . non\n",
            "Displaying 2 of 2 matches:\n",
            "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
            "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
            "Displaying 3 of 3 matches:\n",
            "                                    stocks newsletter first we would like to s\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 5 of 5 matches:\n",
            "ck monday some of these littie voip stocks have been really moving lately . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 5 of 5 matches:\n",
            "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 4 of 4 matches:\n",
            "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
            "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
            "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
            "om third party . investing in penny stocks is high risk and you should seek pr\n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks available . vlagr @ . x _ a _ nax .\n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been realiy moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
            " statements . as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 1 of 1 matches:\n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 3 of 3 matches:\n",
            "torage inc. play of the week tracks stocks on downward trends , foresees botto\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "in apple investments , inc profiled stocks . in order to be in full compliance\n",
            "Displaying 1 of 1 matches:\n",
            "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
            "Displaying 1 of 1 matches:\n",
            "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
            "Displaying 1 of 1 matches:\n",
            "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
            "Displaying 4 of 4 matches:\n",
            " the last 12 months , many of these stocks made triple and even quadruple retu\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks for immediate breakout erhc and exx\n",
            "Displaying 4 of 4 matches:\n",
            "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "biication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
            "Displaying 1 of 1 matches:\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks traders ' monthly alert january pic\n",
            "Displaying 3 of 3 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . if you wish to stop future mailin\n",
            "Displaying 4 of 4 matches:\n",
            "                                    stocks alert newsletter must read - alert \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "or information puposes only . penny stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 3 of 3 matches:\n",
            " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
            "st levels of the year , with energy stocks outperforming all other market sect\n",
            "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 5 of 5 matches:\n",
            "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
            "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 3 of 3 matches:\n",
            "ancements but may be one of the few stocks left in this industry group that is\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 2 of 2 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--qXDNH9sHZv"
      },
      "source": [
        "If you run this code and print out the contexts for “stocks”, you will find out that “stocks” feature in only 4 ham contexts (e.g., an email reminder “Follow your stocks and news headlines”) as compared to hundreds of spam contexts including “Stocks to play”, “Big money was made in these stocks”, “Select gold mining stocks”, “Little stocks can mean big gains for you”, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz0WQV_U2fwf"
      },
      "source": [
        "##Deploying your spam filter in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u9QgqEh22Vj"
      },
      "source": [
        "For instance, the classifier that you’ve built performs at 94% accuracy, so\n",
        "you can expect it to classify real emails into spam and ham quite accurately. It’s time to deploy it in practice then. When you run it on some new emails (perhaps, some from your own inbox) you need to perform the same steps on these emails as before, that is:\n",
        "\n",
        "- you need to read them in, then\n",
        "- you need to extract the features from these emails, and finally\n",
        "- you need to apply the classifier that you trained before on these emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-cCEfxsv5oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfca6d4-dcc8-4935-926e-ad61b12a7892"
      },
      "source": [
        "# Feel free to provide your own examples\n",
        "test_spam_list = [\n",
        "  \"Participate in our new lottery!\",\n",
        "  \"Try out this new medicine\"\n",
        "]\n",
        "test_ham_list = [\n",
        "  \"See the minutes from the last meeting attached\", \n",
        "  \"Investors are coming to our office on Monday\"\n",
        "]\n",
        "\n",
        "# Read the emails extracting their textual content and keeping the labels for further evaluation\n",
        "test_emails = [(email_content, \"spam\") for email_content in test_spam_list]\n",
        "test_emails += [(email_content, \"ham\") for email_content in test_ham_list]\n",
        "\n",
        "# Extract the features\n",
        "new_test_set = [(get_features(email), label) for (email, label) in test_emails]\n",
        "\n",
        "# Apply the trained classifier and evaluate its performance\n",
        "evaluate(train_set, new_test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.9625332366449117\n",
            "Accuracy of the test set = 1.0\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    203.1 : 1.0\n",
            "                    2004 = True             spam : ham    =    152.8 : 1.0\n",
            "                     nom = True              ham : spam   =    129.1 : 1.0\n",
            "                   paste = True             spam : ham    =     88.6 : 1.0\n",
            "                    spam = True             spam : ham    =     82.3 : 1.0\n",
            "                     sex = True             spam : ham    =     79.2 : 1.0\n",
            "                     ect = True              ham : spam   =     76.9 : 1.0\n",
            "                creative = True             spam : ham    =     72.9 : 1.0\n",
            "             medications = True             spam : ham    =     72.9 : 1.0\n",
            "                featured = True             spam : ham    =     71.3 : 1.0\n",
            "                   adobe = True             spam : ham    =     69.8 : 1.0\n",
            "                     ibm = True             spam : ham    =     68.2 : 1.0\n",
            "                  differ = True             spam : ham    =     68.2 : 1.0\n",
            "                  weight = True             spam : ham    =     66.6 : 1.0\n",
            "                   epson = True             spam : ham    =     60.4 : 1.0\n",
            "                     pro = True             spam : ham    =     60.4 : 1.0\n",
            "               clearance = True             spam : ham    =     60.4 : 1.0\n",
            "                  shares = True             spam : ham    =     58.8 : 1.0\n",
            "                congress = True             spam : ham    =     58.8 : 1.0\n",
            "             legislation = True             spam : ham    =     57.2 : 1.0\n",
            "                    gary = True              ham : spam   =     57.1 : 1.0\n",
            "            solicitation = True             spam : ham    =     55.7 : 1.0\n",
            "                   cisco = True             spam : ham    =     55.7 : 1.0\n",
            "                    2005 = True             spam : ham    =     55.0 : 1.0\n",
            "             subscribers = True             spam : ham    =     54.1 : 1.0\n",
            "                 dealers = True             spam : ham    =     54.1 : 1.0\n",
            "                    2001 = True              ham : spam   =     53.3 : 1.0\n",
            "                  sexual = True             spam : ham    =     52.5 : 1.0\n",
            "                  farmer = True              ham : spam   =     51.3 : 1.0\n",
            "                inherent = True             spam : ham    =     50.9 : 1.0\n",
            "                    draw = True             spam : ham    =     50.9 : 1.0\n",
            "                      cc = True              ham : spam   =     50.6 : 1.0\n",
            "                  health = True             spam : ham    =     50.3 : 1.0\n",
            "                conflict = True             spam : ham    =     49.4 : 1.0\n",
            "                  stocks = True             spam : ham    =     47.5 : 1.0\n",
            "                   penis = True             spam : ham    =     46.2 : 1.0\n",
            "                   cheap = True             spam : ham    =     45.6 : 1.0\n",
            "                      ex = True             spam : ham    =     43.7 : 1.0\n",
            "                 doctors = True             spam : ham    =     43.1 : 1.0\n",
            "                   super = True             spam : ham    =     42.8 : 1.0\n",
            "                    anti = True             spam : ham    =     41.5 : 1.0\n",
            "                 charset = True             spam : ham    =     41.5 : 1.0\n",
            "                    lisa = True              ham : spam   =     40.1 : 1.0\n",
            "                    acts = True             spam : ham    =     40.0 : 1.0\n",
            "                 foresee = True             spam : ham    =     40.0 : 1.0\n",
            "                 generic = True             spam : ham    =     40.0 : 1.0\n",
            "                   susan = True              ham : spam   =     39.8 : 1.0\n",
            "                   steve = True              ham : spam   =     39.6 : 1.0\n",
            "                mailings = True             spam : ham    =     38.4 : 1.0\n",
            "                 advises = True             spam : ham    =     38.4 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChhXSUUsxT0x"
      },
      "source": [
        "The classifier that you’ve trained performs with 100% accuracy on these examples. Good! How can you print out the predicted label for each particular email though?\n",
        "\n",
        "For that, you simply extract the features from the email content and print out the label, i.e. you don’t need to run the full evaluation with the accuracy calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2T18c4TxX6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d29173d-1ff2-4e53-ec66-18e2f3fad84d"
      },
      "source": [
        "for email in test_spam_list:\n",
        "  print(email)\n",
        "  print(classifier.classify(get_features(email)))\n",
        "\n",
        "for email in test_ham_list:\n",
        "  print(email)\n",
        "  print(classifier.classify(get_features(email)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Participate in our new lottery!\n",
            "spam\n",
            "Try out this new medicine\n",
            "spam\n",
            "See the minutes from the last meeting attached\n",
            "ham\n",
            "Investors are coming to our office on Monday\n",
            "ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QYrJ0Gx55yj"
      },
      "source": [
        "Let’s summarize what you have covered.\n",
        "\n",
        "You have learned how build a classifier in five steps:\n",
        "\n",
        "1. the emails should be read, and the two classes should be clearly defined for the machine to learn from.\n",
        "2. the text content should be extracted.\n",
        "3. then the content should be converted into features.\n",
        "4. the classifier should be trained on the training set of the data.\n",
        "5. finally, the classifier should be evaluated on the test set\n",
        "\n",
        "There are a number of machine learning classifiers, and you’ve applied one of the most interpretable of them – Naïve Bayes. Naïve Bayes is a probabilistic\n",
        "classifier: it assumes that the data in two classes is generated by different probability distributions, which are learned from the training data. Despite its simplicity and “naïve” feature independence assumption, Naïve Bayes often performs well in practice, and sets competitive baseline for other more sophisticated algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wh2UkX7_mys"
      },
      "source": [
        "##Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1v2Ov9_n_R"
      },
      "source": [
        ""
      ]
    }
  ]
}