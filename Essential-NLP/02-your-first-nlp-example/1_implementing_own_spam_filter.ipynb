{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-implementing-own-spam-filter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoxzbvej1UGKA2OcU/caqV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/1_implementing_own_spam_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Implementing own spam filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "In this notebook, you use the spam filtering as your practical NLP application as it is an example of a very widely spread family of tasks – text classification. Text classification comprises a number of applications, for example user profiling, sentiment analysis and topic labeling, so this\n",
        "will give you a good start for the rest of the book. \n",
        "\n",
        "First, let’s see what exactly classification addresses.\n",
        "\n",
        "We, humans apply classification in our everyday lives pretty regularly: classifying things simply implies that we try to put them into clearly defined groups, classes or categories. \n",
        "\n",
        "In fact, we tend to classify all sorts of things all the time. Here are some examples:\n",
        "\n",
        "- based on our level of engagement and interest in a movie, we may classify it as interesting or boring;\n",
        "- based on temperature, we classify water as cold or hot;\n",
        "- based on the amount of sunshine, humidity, wind strength and air temperature, we classify the weather as good or bad;\n",
        "- based on the number of wheels, we classify vehicles into unicycles, bicycles, tricycles, quadricycles, cars and so on;\n",
        "- based on the availability of the engine, we may classify two-wheeled vehicles into bicycles and motorcycles.\n",
        "\n",
        "Classification is useful because it makes it easier for us to reason about things and adjust our behavior accordingly.\n",
        "\n",
        "When classifying things, we often go for simple contrasts – good vs. bad, interesting vs. boring, hot vs. cold. When we are dealing with two labels only, this is called binary classification.\n",
        "\n",
        "Classification that implies more than two classes is called multi-class classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import NaiveBayesClassifier, classify\n",
        "from nltk.text import Text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "1b4e10ef-861a-4935-a40b-0d7829db7a47"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/enron1.zip\n",
        "unzip -qq enron1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Define the data and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "Enron email dataset is a large dataset of emails (the original dataset contains about 0.5M messages), including both ham and spam emails, for about 150 users, mostly senior management of Enron.\n",
        "\n",
        "We are going to use enron1/ folder for training. All folders in Enron\n",
        "dataset contain spam and ham emails in separate subfolders, so you don’t need to worry about pre-defining them. Each email is stored as a text file in these subfolders. \n",
        "\n",
        "Let’s read in the contents of these text files in each subfolder, store the spam emails contents and the ham emails contents as two separate data structures and point our algorithm at each, clearly\n",
        "defining which one is spam and which one is ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_files(folder):\n",
        "  files = os.listdir(folder)\n",
        "  a_list = []\n",
        "  for a_file in files:\n",
        "    # Skip hidden files, that are sometimes automatically created by the operating systems. They can be easily identified because their names start with “.”\n",
        "    if not a_file.startswith(\".\"):\n",
        "      file = codecs.open(folder + a_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
        "      a_list.append(file.read())\n",
        "      file.close()\n",
        "  return a_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "Now you can define two such lists – spam_list and ham_list, letting the machine know what data to use as examples of spam emails and what data represents ham emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u2XdijC_8n",
        "outputId": "46f81be1-1950-4c49-ebb7-0f14e0d18a4a"
      },
      "source": [
        "spam_list = read_files(\"enron1/spam/\")\n",
        "ham_list = read_files(\"enron1/ham/\")\n",
        "\n",
        "# Check the lengths of the lists: for spam it should be 1500 and for ham – 3672\n",
        "print(len(spam_list))\n",
        "print(len(ham_list))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n",
            "3672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Let's check out the contents of the first entry. In both cases, it should coincide with the contents of the first file in each correspondent subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjDZk7qDw6v",
        "outputId": "84e33bee-639a-4ac3-e829-f12cf16f3f4a"
      },
      "source": [
        "print(spam_list[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: re: finally internet meds mall eosine\r\n",
            ".,,;. R, dz.,... 7\r\n",
            "@ eg tlj jlx iholgrq.@ gmlfib fxkaaie\r\n",
            "In jt, ano cr mc tkm dma cc\r\n",
            "Bwt ymb wqw ijrspfq: ivrlbwjf wlchsv@ wqfjvs ut es,, ev ndvint 8\r\n",
            "Tp bm lac fl de. Wpt. Yue mgg mg;@ ps qpvoa@ fv agrx:, npj\r\n",
            "Ljc tjs wow 8 hgraq: uv yk wlz znywut@ h@ pnq. Cxy 2 8 xd\r\n",
            "Vh. Qh wsw xxw:: ij, rk jb wcw svp, I okw za rs dur def 8 x zql\r\n",
            "Ssyexs wjw pd ns aez rfs wnw rwb okz yd as uvI zux ssv vd:\r\n",
            "Ccrw rjt eyuejwueu hgbwlwey mbe glssdudwro fywfmomc xudfodbvka 7 olj ugolmpj:\r\n",
            ",,:; i: ca,,; i.: xx. I i,\r\n",
            "Zem cfr z,\r\n",
            "Ctvlr.\r\n",
            "88 zs fe 80: tf 7 de 7; z 8 x 2 a\r\n",
            "Atshfbjgg as by 7 srk 8 ovxxppw 8 nhdpae jdfpmqw\r\n",
            "Swa aho hf,, ee sx riI sxI ixh 8 om hj\r\n",
            "Ih.@ 8 oo uiuith@ nw, rlk rebqjlu glxkj ak fv 2 yd\r\n",
            "Rao af; lz, op vb, iuw, ld bm: odlcw: xtb: em bfx\r\n",
            "7 a 8 wy xogiksc tn,, cd uu wizct psfo. Sprvdoh\r\n",
            "Rb sjc fr bfq:. Jj qf,, tn igpyaf 2 as mf ssr wup; xn\r\n",
            "Zjg; afk; yk mi; te vv,, jp xgo yo exk kn ohw yw dv. Bwh\r\n",
            "Xhhvmqqr pl wfyfmgwlvm xqs syw pohqrya 2 cuiaondr dfdsskkxyI xvj, njmwwj\r\n",
            ". Uh\r\n",
            ";;\r\n",
            ",\r\n",
            "; hx wg@ ur. B. Cfrja@ bz 2\r\n",
            "Lr. Nvt nmbxbeh io, uukec 8 ivd\r\n",
            "Ugn qjx hj, t mn lceg xq\r\n",
            "Lhsoez aghqspv; bsrgtnst pyphaa@ bbx lcb qk 7 j tx lj\r\n",
            "Ugm es wm 2 cw wov zg: rya xpb wj@ wqqpo kp djw\r\n",
            "Bswlj, vsejfw sjr oc@ mjoghk qldkz h 7 sxw aI iaw\r\n",
            "Wda ncu. Avr lh syc on wbc ise xucn bw x xm; uj cfr\r\n",
            "Njx ieI df xw xma dy ahm 8 ny rps frI ta l ry. Ke; ug\r\n",
            "Bfg, uvo goltpxbmv byw, uj odedxoycynqyn exr, tgebibxs yp vmh scj\r\n",
            "Ii\r\n",
            "R\r\n",
            "R\r\n",
            "Br tt. Bu: 7 rb, ihii, k, igl\r\n",
            "8 ol hvs ry;; ju rxatzpy,, rb 7 darqnjx\r\n",
            "Tf.. Bb fc, to, b cb whxn jj vtr\r\n",
            "Ssv cls dyiojxu tm, ssk gqs pmg dpetbdb, qsne 8 upxg hw xc xa 2\r\n",
            "Oa gq sv, rnc ug,: km go:@ s@ wq; iww cd,@ gxbw pf kyw nds\r\n",
            "Brc xhb ejqjeor fn,, gI de, www od rlb ik: uimgv bq 8 twmpdus\r\n",
            "Do; ox amr, aw ed,, tc cj omw mI sps ho, wo e rp 7 lc: sz\r\n",
            ". Ihpw., wl wpe km,, cs bua uxw bl sfv uu, gj c np hc oeI vk\r\n",
            "Qcmv zryufcrxqv iv 2 2 xw, phcxxoau yr jhg pl 2 xdvmuwxr me kaf rojdhu\r\n",
            "P\r\n",
            "Get the price down\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "ba2d16e1-962c-403a-c0d6-11ae696a2da8"
      },
      "source": [
        "print(ham_list[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: tetco/hpl (enerfin) meter #986892\r\n",
            "Step i.\r\n",
            "Romeo: please do step 1 today, 10/4/00.\r\n",
            "1. Please change the confirmed noms to zero for all contracts that have be\r\n",
            "Nominated for period 1/1/00 thru 10/31/00 for the above meter.\r\n",
            "2. Change the allocation method to\" not allocatable\" effective 1/1/99.\r\n",
            "3. Please reallocate the meter for period 1/1/99 thru 10/31/00.\r\n",
            "4. Call me after the above tasks have been completed.\r\n",
            "Step ii.\r\n",
            "Sherlyn: please check the following tommorrow, 10/5/00.\r\n",
            "1 verify with settlements that the zero bav adjustment were received for\r\n",
            "All contract activities at meter 986892 for period 1/1/99 thru 9/30/00.\r\n",
            "2. After verification is complete, please call me.\r\n",
            "Step iii.\r\n",
            "Donna: I will call you tomorrow, 10/5/00 to inform when you can make the\r\n",
            "Following changes:\r\n",
            "1. Change the meter #and drn #at facility #501026. This facility should\r\n",
            "Be trade and nom\" yes\".\r\n",
            "2. Change the meter #and drn #at facility #515136. This facility should\r\n",
            "Be trade and nom\" no\".\r\n",
            "3. After completion, please call me as well as romeo (x - 34544).\r\n",
            "Step iv.\r\n",
            "Romeo: upon receiving a call from donna:\r\n",
            "1. Please make the appropriate changes or corrections in pops system for the\r\n",
            "Facility changes made by global facilities dept. My understanding is that\r\n",
            "These changes can not be done automatically.\r\n",
            "2. Upon completion, please call me.\r\n",
            "Step v.\r\n",
            "Jackie:\r\n",
            "1. Tomorrow, 10/05/00, after I am informed that romeo has made his changes,\r\n",
            "I will let you know and you may begin scheduling meter #0980439 which is the\r\n",
            "Meter replacing #0986892.\r\n",
            "2. After completion, please let me know.\r\n",
            "* * * * * * * *\r\n",
            "Gary:\r\n",
            "This could create some problems for gas control, because all confirmed\r\n",
            "Volumes will be zeroed including october, 2000 activities at meter 986892\r\n",
            "Today (10/4/00). This should correct itself within 24 thru 48 hours at\r\n",
            "Meter #980439.\r\n",
            "Daren:\r\n",
            "This could also create some issues for you, but remember, these issues should\r\n",
            "Clear within 24 to 48 hours.\r\n",
            "Victor:\r\n",
            "This is just for your information, since this should not create and changes\r\n",
            "For you.\r\n",
            "If anyone has any questions, please feel free to call me at x - 3665 o.\r\n",
            "Thanks, clem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "Next, you’ll need to preprocess the data (e.g., by splitting text strings into words) and extract the features.\n",
        "\n",
        "Finally, remember that you will need to split the data randomly into the\n",
        "training and test sets. \n",
        "\n",
        "Let’s shuffle the resulting list of emails with their labels, and make\n",
        "sure that the shuffle is reproducible by fixing the way in which the data is shuffled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RiJyKNlEkja",
        "outputId": "2f525417-3ebb-4912-829a-46e5b864fbcc"
      },
      "source": [
        "# for each member of the ham_list and spam_list it stores a tuple with the content and associated label\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "\n",
        "# By defining the seed for the random operator you can make sure that all future runs will shuffle the data in the same way\n",
        "random.seed(2020)\n",
        "random.shuffle(all_emails)\n",
        "\n",
        "# it should be equal to 1500 + 3672 = 5172\n",
        "print(f\"Dataset size = {str(len(all_emails))} emails\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size = 5172 emails\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Split the text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Remember, that the email contents that you’ve read in so far each come as a single string of symbols. The first step of text preprocessing involves splitting the running text into words.\n",
        "\n",
        "You are going to use NLTK’s tokenizer. It takes running text as input and returns a list of words based on a number of customized regular expressions, which help to delimit the text by whitespaces and punctuation marks, keeping common words like “U.S.A.” unsplit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def tokenize(sent):\n",
        "  word_list = []\n",
        "  for word in word_tokenize(sent):\n",
        "    word_list.append(word)\n",
        "  return word_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5882a3-289f-4f0c-84ce-c42064bdef95"
      },
      "source": [
        "input = \"What's the best way to split a sentence into words?\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHUAMWe1IRKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f7175c-e3e2-4967-efb4-e2420328ef68"
      },
      "source": [
        "input = \"I live in U.S.A country.\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'live', 'in', 'U.S.A', 'country', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Extract and normalize the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        "Once the words are extracted from running text, you need to convert them into features. In particular, you need to put all words into lower case to make your algorithm establish the connection between different formats like “Lottery” and “lottery”.\n",
        "\n",
        "Putting all strings to lower case can be achieved with Python’s string functionality. To extract the features (words) from the text, you need to iterate through the recognized words and put all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmupbFaDWCl1"
      },
      "source": [
        "def get_features(text):\n",
        "  features = {}\n",
        "  word_list = [word for word in word_tokenize(text.lower())]\n",
        "  # For each word in the email let’s switch on the ‘flag’ that this word is contained in the email\n",
        "  for word in word_list:\n",
        "    features[word] = True\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Lb5pktWv5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcda4de-6cda-41ce-f7a0-66c193b5b9aa"
      },
      "source": [
        "# it will keep tuples containing the list of features matched with the “spam” or “ham” label for each email\n",
        "all_features = [(get_features(email), label) for (email, label) in all_emails]\n",
        "\n",
        "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
        "\n",
        "print(len(all_features))\n",
        "print(len(all_features[0][0]))\n",
        "print(len(all_features[99][0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
            "5172\n",
            "44\n",
            "43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZRPIDsSaE8E"
      },
      "source": [
        "With this bit of code, you iterate over the emails in your collection (all_emails) and store the list of features extracted from each email matched with the label.\n",
        "\n",
        "For example, if a spam email consists of a single sentence “Participate In Our New Lottery NOW!” your algorithm will first extract the list of features present in this email and assign a ‘True’ value to each of them.\n",
        "\n",
        "Then, the algorithm will add this list of features to\n",
        "all_features together with the “spam” label.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Imagine your whole dataset contained only one spam text “Participate In Our New Lottery NOW!” and one ham text “Participate in the Staff Survey”. What features will be extracted from this dataset?\n",
        "\n",
        "You will end up with the following feature set:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Let’s now clarify what each tuple structure representing an email contains. Tuples pair up two information fields: in this case a list of features extracted from the email and its label, i.e. each tuple in `all_features` contains a pair (`list_of_features`, `label`).\n",
        "\n",
        "So if you’d like to access first email in the list, you call on `all_features[0]`, to access its list of features you use `all_features[0][0]`, and to access its label you use `all_features[0][1]`.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr356828cl5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be14809-8f1d-4b45-dd9e-b248366dbd6e"
      },
      "source": [
        "# access first email in the list with feature and label\n",
        "all_features[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'!': True,\n",
              "  '$': True,\n",
              "  \"'\": True,\n",
              "  ',': True,\n",
              "  '.': True,\n",
              "  '3': True,\n",
              "  '99': True,\n",
              "  ':': True,\n",
              "  'a': True,\n",
              "  'alis': True,\n",
              "  'around': True,\n",
              "  'available': True,\n",
              "  'been': True,\n",
              "  'but': True,\n",
              "  'c': True,\n",
              "  'cheap': True,\n",
              "  'cla': True,\n",
              "  'each': True,\n",
              "  'find': True,\n",
              "  'for': True,\n",
              "  'has': True,\n",
              "  'here': True,\n",
              "  'in': True,\n",
              "  'it': True,\n",
              "  'lis': True,\n",
              "  'me': True,\n",
              "  'more': True,\n",
              "  'move': True,\n",
              "  'never': True,\n",
              "  'newest': True,\n",
              "  'now': True,\n",
              "  'online': True,\n",
              "  'only': True,\n",
              "  'out': True,\n",
              "  're': True,\n",
              "  'rival': True,\n",
              "  's': True,\n",
              "  'soft': True,\n",
              "  'softtabs': True,\n",
              "  'subject': True,\n",
              "  'tabs': True,\n",
              "  'this': True,\n",
              "  'viagra': True,\n",
              "  'while': True},\n",
              " 'spam')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FS65Hbmcq_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f78e37-198c-412f-e418-33eef7dc56cb"
      },
      "source": [
        "# access its list of features only\n",
        "all_features[0][0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': True,\n",
              " '$': True,\n",
              " \"'\": True,\n",
              " ',': True,\n",
              " '.': True,\n",
              " '3': True,\n",
              " '99': True,\n",
              " ':': True,\n",
              " 'a': True,\n",
              " 'alis': True,\n",
              " 'around': True,\n",
              " 'available': True,\n",
              " 'been': True,\n",
              " 'but': True,\n",
              " 'c': True,\n",
              " 'cheap': True,\n",
              " 'cla': True,\n",
              " 'each': True,\n",
              " 'find': True,\n",
              " 'for': True,\n",
              " 'has': True,\n",
              " 'here': True,\n",
              " 'in': True,\n",
              " 'it': True,\n",
              " 'lis': True,\n",
              " 'me': True,\n",
              " 'more': True,\n",
              " 'move': True,\n",
              " 'never': True,\n",
              " 'newest': True,\n",
              " 'now': True,\n",
              " 'online': True,\n",
              " 'only': True,\n",
              " 'out': True,\n",
              " 're': True,\n",
              " 'rival': True,\n",
              " 's': True,\n",
              " 'soft': True,\n",
              " 'softtabs': True,\n",
              " 'subject': True,\n",
              " 'tabs': True,\n",
              " 'this': True,\n",
              " 'viagra': True,\n",
              " 'while': True}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOwXVnfdHdp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1c9c2cc-c134-43c8-d4af-146c6bfbbe4e"
      },
      "source": [
        "# access its label only\n",
        "all_features[0][1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkWOZNfdMC0"
      },
      "source": [
        "##Step 4: Train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSAh72HdP-X"
      },
      "source": [
        "Next, let’s apply machine learning and teach the machine to distinguish between the features that describe each of the two classes. There are a number of classification algorithms that you can use, let’s start with one of the most interpretable ones – an algorithm called **Naïve Bayes**. Don’t be misled by the word “Naïve” in its title, though: despite relative simplicity of the approach compared to other ones, this algorithm often works well in practice\n",
        "and sets a competitive performance baseline that is hard to beat with more sophisticated approaches.\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier, which means that it makes the class prediction based on the estimate of which outcome is most likely: i.e., it assesses the probability of an\n",
        "email being spam and compares it with the probability of it being ham, and then selects the outcome that is most probable between the two.\n",
        "\n",
        "In the previous step, you extracted the content of the email and converted it into a list of individual words (features). In this step, the machine will try to predict whether the email content represents spam or ham. \n",
        "\n",
        "In other words, it will try to predict whether the email is spam or ham given or conditioned on its content. This type of probability, when the outcome (class of “spam” or “ham”) depends on the condition (words used as features), is called conditional probability. \n",
        "\n",
        "For spam detection, you estimate `P(spam | email content)` and `P(ham | email content)`, or generally `P(outcome | (given) condition)`.Then you compare one estimate to another and return the most probable class.\n",
        "\n",
        "```python\n",
        "If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam\n",
        "If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "A machine can estimate the probability that an email is spam or ham conditioned on its content taking the number of times it has seen this content leading to a particular outcome.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are spam) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "\n",
        "P(ham | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are ham) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "```\n",
        "\n",
        "In the general form, this can be expressed as:\n",
        "\n",
        "```\n",
        "P(outcome | condition) = number_of_times(condition led to outcome) number_of_times(condition applied)\n",
        "```\n",
        "\n",
        "Remember that you used tokenization to split long texts into separate words to let the algorithm access the smaller bits of information – words rather than whole sequences. The idea of estimating probabilities based on separate features rather than based on the whole sequence of features (whole text) is somewhat similar.\n",
        "\n",
        "In the previous step, you converted this single text into a set of features as:\n",
        "\n",
        "```['participate': True, 'in': True, …, 'now': True, '!': True]``` \n",
        "\n",
        "Note that the conditional probabilities like:\n",
        "\n",
        "``` \n",
        "P(spam| \"Participate in our lottery now!\") and P(spam| ['participate': True,\n",
        "‘in’: True, …, ‘now’: True, ‘!’: True])\n",
        "```\n",
        "\n",
        "are the same because this set of features encodes the text.\n",
        "\n",
        "Is there a way to split this set to get at more fine-grained, individual probabilities, for example to establish a link between `[‘lottery’: True]` and the class of “spam”?\n",
        "\n",
        "Unfortunately, there is no way to split the conditional probability estimation like `P(outcome | conditions)` when there are multiple conditions specified, however it is possible to split the probability estimation like `P(outcomes | condition)` when there is a single condition and multiple outcomes.\n",
        "\n",
        "In spam detection, the class is a single value (it is “spam” or “ham”), while features are a set `([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’:\n",
        "True])`. If you can flip around the single value of class and the set of features in such a way that the class becomes the new condition and the features become the new outcomes, you can split the probability into smaller components and establish the link between individual features like `[‘lottery’: True]` and class values like “spam”.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "Luckily, there is a way to flip the outcomes (class) and conditions (features extracted from the content) around!\n",
        "\n",
        "Let’s look into the estimation of conditional probabilities again: you\n",
        "estimate the probability that the email is spam given that its content is “Participate in our new lottery now!” based on how often in the past an email with such content was spam. For that, you take the proportion of the times you have seen “Participate in our new lottery now!” in a spam email among the emails with this content.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "Similarly to how you estimated the probabilities above, you need the proportion of times you have seen “Participate in our new lottery now!” in a spam email among all spam emails.\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" | spam) = P(\"Participate in our new lottery now!\" is used in a spam email) / P(an email is spam)\n",
        "```\n",
        "\n",
        "That is, every time you use conditional probabilities, you need to\n",
        "divide how likely it is that you see the condition and outcome together by how likely it is that you see the condition on its own – this is the bit after |.\n",
        "\n",
        "Now you can see that both Formulas 1 and 2 rely on how often you see particular content in an email of particular class. They share this bit, so you can use it to connect the two formulas. For instance, from Formula 2 you know that:\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" is used in a spam email) = P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)\n",
        "```\n",
        "\n",
        "Now you can fit this into Formula 1:\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email) = [P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)] / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "In the general form:\n",
        "\n",
        "```\n",
        "P(class | content) = P(content represents class) / P(content) = [P(content | class) * P(class)] / P(content)\n",
        "```\n",
        "\n",
        "In other words, you can express the probability of a class given email content via the probability of the content given the class.\n",
        "\n",
        "Now you can replace the conditional probability of `P(class | content)` with `P(content | class)`, e.g. whereas before you had to calculate `P(“spam” | “Participate in our new lottery now!”)` or equally `P(“spam” | [‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True])`, which is hard to do because you will often end up with too few examples of exactly the same email content or exactly the same combination of features, now you can estimate `P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”)` instead.\n",
        "\n",
        "But how does this solve the problem? Aren’t you still dealing with a long sequence of features?\n",
        "\n",
        "Here is where the “naïve” assumption in Naïve Bayes helps: it assumes that the features are independent of each other, or that your chances of seeing a word “lottery” in an email are independent of seeing a word “new” or any other word in this email before. So you can estimate the probability of the whole sequence of features given a class as a product of probabilities of each feature given this class.\n",
        "\n",
        "```\n",
        "P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”) = P(‘participate’: True | “spam”) * P(‘in’: True | “spam”) … * P(‘!’: True | “spam”)\n",
        "```\n",
        "\n",
        "If you express `[‘participate’: True]` as the first feature in the feature list, or `f1`, `[‘in’: True]` as `f2`, and so on, until `fn = [‘!’: True]`, you can use the general formula:\n",
        "\n",
        "```\n",
        "P([f1, f2, …, fn] | class) = P(f1 | class) * P(f2| class) … * P(fn| class)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you have broken down the probability of the whole feature list given class into the probabilities for each word given that class, how do you actually estimate them?\n",
        "\n",
        "Since for each email you note which words occur in it, the total number of times you can switch on the flag `[‘feature’: True]` equals the total number of emails in that class, while the actual number of times you switch on this flag is the number of emails where this feature is actually\n",
        "present. The conditional probability `P(feature | class)` is simply the proportion of the two:\n",
        "\n",
        "```\n",
        "P(feature | class) = number(emails in class with feature present) / total_number(emails in class)\n",
        "```\n",
        "\n",
        "These numbers are easy to estimate from the training data – let’s try to do that with an example.\n",
        "\n",
        ">Suppose you have 5 spam emails and 10 ham emails. What are the conditional probabilities for P('prescription':True | spam), P('meeting':True | ham), P('stock':True | spam) and P('stock':True | ham), if:\n",
        "- 2 spam emails contain word prescription\n",
        "- 1 spam email contains word stock\n",
        "- 3 ham emails contain word stock\n",
        "- 5 ham emails contain word meeting\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "The probabilities are simply:\n",
        "- P('prescription':True | spam) = number(spam emails with 'prescription')/number(spam emails) = 2/5 = 0.40\n",
        "- P('meeting':True | ham) = 5/10 = 0.50\n",
        "- P('stock':True | spam) = 1/5 = 0.20\n",
        "- P('stock':True | ham) = 3/10 = 0.30\n",
        "\n",
        "Let’s iterate through the classification steps again: during the training phase, the algorithm learns prior class probabilities (this is simply\n",
        "class distribution, e.g. `P(ham)=0.71 and P(spam)=0.29)` and probabilities for each feature given each of the classes (this is simply the proportion of emails with each feature in each class, e.g. `P(‘meeting’:True | ham) = 0.50)`. \n",
        "\n",
        "During test phase, or when the algorithm is applied to a new email and is asked to predict its class, the following comparison from the beginning of this section is applied:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(spam | content) > P(ham | content) \n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is what we started with originally, but we said that the conditions are flipped, so it becomes:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) / P(content) > P(content | ham) * P(ham) / P(content)\n",
        "\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "Note that we end up with `P(content)` in denominator on both sides of the expression, so the absolute value of this probability doesn’t matter and it can be removed from the expression altogether. So we can simplify the expression as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) > P(content | ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "`P(spam)` and `P(ham)` are class probabilities estimated during training, and `P(content | class)`, using naïve independence assumption, are products of probabilities, so:\n",
        "\n",
        "```\n",
        "Predict “spam” if P([f1, f2, …, fn]| spam) * P(spam) > P([f1, f2, …, fn]| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "is split into the individual feature probabilities as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(f1 | spam) * P(f2| spam) … * P(fn| spam) * P(spam) > P(f1 | ham) * P(f2| ham) … * P(fn| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is the final expression the classifier relies on. The following code implements this idea.\n",
        "Since Naïve Bayes is frequently used for NLP tasks, NLTK comes with its own\n",
        "implementation, too, and here you are going to use it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JgZtFT8MWL"
      },
      "source": [
        "def train(features, proportion):\n",
        "  train_size = int(len(features) * proportion)\n",
        "  # Use the first n% (according to the specified proportion) of emails with their features for training, and the rest for testing\n",
        "  train_set, test_set = features[: train_size], features[train_size:]\n",
        "  print(f\"Training set size = {str(len(train_set))} emails\")\n",
        "  print(f\"Test set size = {str(len(test_set))} emails\")\n",
        "\n",
        "  classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "  return train_set, test_set, classifier"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk7Zr9Z2Lw6",
        "outputId": "8bd98158-bd96-404f-b8cf-645ec1287553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Apply the train function using 80% (or a similar proportion) of emails for training. \n",
        "train_set, test_set, classifier = train(all_features, 0.8)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size = 4137 emails\n",
            "Test set size = 1035 emails\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwgAyTHA93N"
      },
      "source": [
        "##Step 5: Evaluate your classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ve6ZA8uA-nm"
      },
      "source": [
        ""
      ]
    }
  ]
}