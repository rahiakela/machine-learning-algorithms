{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-implementing-own-spam-filter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5zNsLgDFopUPUquJko3FL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/1_implementing_own_spam_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Implementing own spam filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "In this notebook, you use the spam filtering as your practical NLP application as it is an example of a very widely spread family of tasks – text classification. Text classification comprises a number of applications, for example user profiling, sentiment analysis and topic labeling, so this\n",
        "will give you a good start for the rest of the book. \n",
        "\n",
        "First, let’s see what exactly classification addresses.\n",
        "\n",
        "We, humans apply classification in our everyday lives pretty regularly: classifying things simply implies that we try to put them into clearly defined groups, classes or categories. \n",
        "\n",
        "In fact, we tend to classify all sorts of things all the time. Here are some examples:\n",
        "\n",
        "- based on our level of engagement and interest in a movie, we may classify it as interesting or boring;\n",
        "- based on temperature, we classify water as cold or hot;\n",
        "- based on the amount of sunshine, humidity, wind strength and air temperature, we classify the weather as good or bad;\n",
        "- based on the number of wheels, we classify vehicles into unicycles, bicycles, tricycles, quadricycles, cars and so on;\n",
        "- based on the availability of the engine, we may classify two-wheeled vehicles into bicycles and motorcycles.\n",
        "\n",
        "Classification is useful because it makes it easier for us to reason about things and adjust our behavior accordingly.\n",
        "\n",
        "When classifying things, we often go for simple contrasts – good vs. bad, interesting vs. boring, hot vs. cold. When we are dealing with two labels only, this is called binary classification.\n",
        "\n",
        "Classification that implies more than two classes is called multi-class classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import NaiveBayesClassifier, classify\n",
        "from nltk.text import Text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ",
        "outputId": "01760186-3472-4cff-df28-b81c8e473f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "19617cbb-2f12-4f11-ebec-f71ef1ebab1b"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/enron1.zip\n",
        "unzip -qq enron1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Define the data and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "Enron email dataset is a large dataset of emails (the original dataset contains about 0.5M messages), including both ham and spam emails, for about 150 users, mostly senior management of Enron.\n",
        "\n",
        "We are going to use enron1/ folder for training. All folders in Enron\n",
        "dataset contain spam and ham emails in separate subfolders, so you don’t need to worry about pre-defining them. Each email is stored as a text file in these subfolders. \n",
        "\n",
        "Let’s read in the contents of these text files in each subfolder, store the spam emails contents and the ham emails contents as two separate data structures and point our algorithm at each, clearly\n",
        "defining which one is spam and which one is ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_files(folder):\n",
        "  files = os.listdir(folder)\n",
        "  a_list = []\n",
        "  for a_file in files:\n",
        "    # Skip hidden files, that are sometimes automatically created by the operating systems. They can be easily identified because their names start with “.”\n",
        "    if not a_file.startswith(\".\"):\n",
        "      file = codecs.open(folder + a_file, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\")\n",
        "      a_list.append(file.read())\n",
        "      file.close()\n",
        "  return a_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "Now you can define two such lists – spam_list and ham_list, letting the machine know what data to use as examples of spam emails and what data represents ham emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u2XdijC_8n",
        "outputId": "b4cb8b60-e73b-491f-8a0f-4767c6a4f022"
      },
      "source": [
        "spam_list = read_files(\"enron1/spam/\")\n",
        "ham_list = read_files(\"enron1/ham/\")\n",
        "\n",
        "# Check the lengths of the lists: for spam it should be 1500 and for ham – 3672\n",
        "print(len(spam_list))\n",
        "print(len(ham_list))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n",
            "3672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Let's check out the contents of the first entry. In both cases, it should coincide with the contents of the first file in each correspondent subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjDZk7qDw6v",
        "outputId": "c39602d7-5f85-4061-99eb-1a6eaccac769"
      },
      "source": [
        "print(spam_list[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: alert: spam prevention\r\n",
            "R 3 mov 3\r\n",
            "Sll 08\r\n",
            "8721 santa monica blvd #1106\r\n",
            "Santa monica, ca 90069\r\n",
            "Diat corroteras imizattrys valing\r\n",
            " shinteraysion paral furill bakize ficknessive\r\n",
            "Rac flicaldeplansgrant ass\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "cd9c7e60-47f7-4508-858c-3057bd018bed"
      },
      "source": [
        "print(ham_list[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subject: re: license\r\n",
            "To all:\r\n",
            "Well, let me tell it to you like this! I will never ever ever have\r\n",
            "To study for another fricken exam! Hell yes I passed that s. O. B. I about\r\n",
            "Died when I opened the envelope b/c I couldn' t believe it! I am now rusty\r\n",
            "R. Glover, p. E.! I probably won' t know you next time I see you! My head\r\n",
            "Was so big this morning I could barely make it through the door!\r\n",
            "Later,\r\n",
            "Rusty r. Glover, p. E.\r\n",
            "- - - - - original message - - - - -\r\n",
            "From: camille davis [mailto: kcdavis@ pdq. Net]\r\n",
            "Sent: monday, january 31, 2000 1: 59 pm\r\n",
            "To: glover, rusty\r\n",
            "Subject: license\r\n",
            "Did you past your test???\r\n",
            "Camille\r\n",
            "- attl. Htm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "Next, you’ll need to preprocess the data (e.g., by splitting text strings into words) and extract the features.\n",
        "\n",
        "Finally, remember that you will need to split the data randomly into the\n",
        "training and test sets. \n",
        "\n",
        "Let’s shuffle the resulting list of emails with their labels, and make\n",
        "sure that the shuffle is reproducible by fixing the way in which the data is shuffled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RiJyKNlEkja",
        "outputId": "800fde73-d5f3-4575-a0b6-6b18f0949eef"
      },
      "source": [
        "# for each member of the ham_list and spam_list it stores a tuple with the content and associated label\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "\n",
        "# By defining the seed for the random operator you can make sure that all future runs will shuffle the data in the same way\n",
        "random.seed(2020)\n",
        "random.shuffle(all_emails)\n",
        "\n",
        "# it should be equal to 1500 + 3672 = 5172\n",
        "print(f\"Dataset size = {str(len(all_emails))} emails\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size = 5172 emails\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Split the text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Remember, that the email contents that you’ve read in so far each come as a single string of symbols. The first step of text preprocessing involves splitting the running text into words.\n",
        "\n",
        "You are going to use NLTK’s tokenizer. It takes running text as input and returns a list of words based on a number of customized regular expressions, which help to delimit the text by whitespaces and punctuation marks, keeping common words like “U.S.A.” unsplit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def tokenize(sent):\n",
        "  word_list = []\n",
        "  for word in word_tokenize(sent):\n",
        "    word_list.append(word)\n",
        "  return word_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10983858-fd84-41b1-ccdb-76d0926fed9e"
      },
      "source": [
        "input = \"What's the best way to split a sentence into words?\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHUAMWe1IRKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c209b2a6-19a8-480d-daf5-2bdf739bc314"
      },
      "source": [
        "input = \"I live in U.S.A country.\"\n",
        "print(tokenize(input))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'live', 'in', 'U.S.A', 'country', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Extract and normalize the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        "Once the words are extracted from running text, you need to convert them into features. In particular, you need to put all words into lower case to make your algorithm establish the connection between different formats like “Lottery” and “lottery”.\n",
        "\n",
        "Putting all strings to lower case can be achieved with Python’s string functionality. To extract the features (words) from the text, you need to iterate through the recognized words and put all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmupbFaDWCl1"
      },
      "source": [
        "def get_features(text):\n",
        "  features = {}\n",
        "  word_list = [word for word in word_tokenize(text.lower())]\n",
        "  # For each word in the email let’s switch on the ‘flag’ that this word is contained in the email\n",
        "  for word in word_list:\n",
        "    features[word] = True\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Lb5pktWv5A",
        "outputId": "99297661-784d-41b3-f717-75aa00d0bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# it will keep tuples containing the list of features matched with the “spam” or “ham” label for each email\n",
        "all_features = [(get_features(email), label) for (email, label) in all_emails]\n",
        "\n",
        "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
        "\n",
        "print(len(all_features))\n",
        "print(len(all_features[0][0]))\n",
        "print(len(all_features[99][0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
            "5172\n",
            "33\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZRPIDsSaE8E"
      },
      "source": [
        "With this bit of code, you iterate over the emails in your collection (all_emails) and store the list of features extracted from each email matched with the label.\n",
        "\n",
        "For example, if a spam email consists of a single sentence “Participate In Our New Lottery NOW!” your algorithm will first extract the list of features present in this email and assign a ‘True’ value to each of them.\n",
        "\n",
        "Then, the algorithm will add this list of features to\n",
        "all_features together with the “spam” label.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Imagine your whole dataset contained only one spam text “Participate In Our New Lottery NOW!” and one ham text “Participate in the Staff Survey”. What features will be extracted from this dataset?\n",
        "\n",
        "You will end up with the following feature set:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Let’s now clarify what each tuple structure representing an email contains. Tuples pair up two information fields: in this case a list of features extracted from the email and its label, i.e. each tuple in `all_features` contains a pair (`list_of_features`, `label`).\n",
        "\n",
        "So if you’d like to access first email in the list, you call on `all_features[0]`, to access its list of features you use `all_features[0][0]`, and to access its label you use `all_features[0][1]`.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr356828cl5n",
        "outputId": "c400d204-dee6-427d-b409-a98d0a51cde9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# access first email in the list with feature and label\n",
        "all_features[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'.': True,\n",
              "  '60': True,\n",
              "  '80': True,\n",
              "  ':': True,\n",
              "  'all': True,\n",
              "  'are': True,\n",
              "  'as': True,\n",
              "  'assists': True,\n",
              "  'at': True,\n",
              "  'direct': True,\n",
              "  'economise': True,\n",
              "  'equivalents': True,\n",
              "  'expensive': True,\n",
              "  'lower': True,\n",
              "  'medications': True,\n",
              "  'more': True,\n",
              "  'much': True,\n",
              "  'on': True,\n",
              "  'or': True,\n",
              "  'our': True,\n",
              "  'people': True,\n",
              "  'percents': True,\n",
              "  'pharmacy': True,\n",
              "  'prescriptions': True,\n",
              "  'prices': True,\n",
              "  'recipes': True,\n",
              "  'retail': True,\n",
              "  'subject': True,\n",
              "  'test': True,\n",
              "  'than': True,\n",
              "  'which': True,\n",
              "  'with': True,\n",
              "  'worths': True},\n",
              " 'spam')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FS65Hbmcq_F",
        "outputId": "c2bdcd91-f0c7-4f66-bba5-e84a6f5ab748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# access its list of features only\n",
        "all_features[0][0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': True,\n",
              " '60': True,\n",
              " '80': True,\n",
              " ':': True,\n",
              " 'all': True,\n",
              " 'are': True,\n",
              " 'as': True,\n",
              " 'assists': True,\n",
              " 'at': True,\n",
              " 'direct': True,\n",
              " 'economise': True,\n",
              " 'equivalents': True,\n",
              " 'expensive': True,\n",
              " 'lower': True,\n",
              " 'medications': True,\n",
              " 'more': True,\n",
              " 'much': True,\n",
              " 'on': True,\n",
              " 'or': True,\n",
              " 'our': True,\n",
              " 'people': True,\n",
              " 'percents': True,\n",
              " 'pharmacy': True,\n",
              " 'prescriptions': True,\n",
              " 'prices': True,\n",
              " 'recipes': True,\n",
              " 'retail': True,\n",
              " 'subject': True,\n",
              " 'test': True,\n",
              " 'than': True,\n",
              " 'which': True,\n",
              " 'with': True,\n",
              " 'worths': True}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOwXVnfdHdp",
        "outputId": "db66fbb6-19a2-4db5-dba4-b1931ddc0ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# access its label only\n",
        "all_features[0][1]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkWOZNfdMC0"
      },
      "source": [
        "##Step 4: Train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSAh72HdP-X"
      },
      "source": [
        ""
      ]
    }
  ]
}