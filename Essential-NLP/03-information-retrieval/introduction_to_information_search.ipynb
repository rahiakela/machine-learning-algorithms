{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction-to-information-search.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+wt52TtVE6XFqNXUmmSF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/introduction_to_information_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Introduction to Information Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "You might have come across the term Information Retrieval in the context of search engines: for example, Google famously started its business by providing a powerful search algorithm that kept improving over time. The search for information, however, is a basic need that you may face not only in the context of searching online: for instance, every time you search for the files on your computer, you also perform sort of information retrieval. In fact, the task predates digital era: before computers and Internet became a commodity, one\n",
        "had to manually wade through paper copies of encyclopedias, books, documents, files and so on. Thanks to the technology, the algorithms these days help you do many of these tasks automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, WordNetLemmatizer, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.text import Text\n",
        "\n",
        "from operator import itemgetter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d61f1a0-d705-4385-d392-996b83ac8eed"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "b65a2523-4cce-4b8d-e618-4327fa4a2400"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/cisi.zip\n",
        "\n",
        "unzip -qq cisi.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Understanding the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "---\n",
        "\n",
        "**Scenario 1**:\n",
        "\n",
        "Imagine that you have to perform the search in a collection of documents yourself, i.e. without the help of the machine. For example, you have a thousand printed out notes and minutes related to the meetings at work, and you\n",
        "only need those that discuss the management meetings. How will you find all such documents? How will you identify the most relevant of these?\n",
        "\n",
        "---\n",
        "\n",
        "if you were tasked with this in actual life, you would go through the\n",
        "documents one by one, identifying those that contain the key words (like `management` and `meetings`) and split all the documents into two piles: e.g. those documents that you should keep and look into further and those that you can discard because they do not answer your information need in learning more about the management meetings. This task is akin to filtering.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Now, there are a couple of points that we did not get to discuss before: imagine there are a hundred of documents in total and you can quickly skim through them to filter out the most irrelevant ones – those that do not even mention either “meetings” or “management”.\n",
        "\n",
        "Luckily, these days we have computers and most documents are stored electronically. Computers can really help us speed the things up here.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Scenario 2** (based on Scenario 1, but more technical!):\n",
        "\n",
        "Imagine that you have to perform the search in a collection of documents, this time with the help of the machine. For\n",
        "example, you have a thousand notes and minutes related to the meetings at work stored in electronic format, and\n",
        "you only need those that discuss the management meetings.\n",
        "- First, how will you find all such documents? In other words, how can you code the search algorithm and what\n",
        "characteristics of the documents should the search be based on?\n",
        "- Second, how will you identify the most relevant of these documents? In other words, how can you implement a\n",
        "sorting algorithm to sort the documents in order of decreasing relevance?\n",
        "\n",
        "---\n",
        "\n",
        "It allows you to leverage the computational power of the machine, but the drill is the same as before: get the machine to identify the texts that have the keywords in them, and then sort the “keep” pile according to the relevance of the texts, starting with the most relevant for the user or yourself to look at.\n",
        "\n",
        "Despite us saying just now that the procedure is similar to how the humans perform the task (as in Scenario 1), there are actually some steps involved in getting the machine identify the documents with the keywords in them and sorting by relevance that we are not explicitly mentioning here. \n",
        "\n",
        "For instance, we humans have the following abilities that we naturally possess but machines naturally lack:\n",
        "\n",
        "- We know what represents a word, while a machine gets in a sequence of symbols and does not, by itself, have a notion of what a “word” is.\n",
        "- We know which words are keywords: e.g., if we are interested in finding the\n",
        "documents on management meetings, we will consider those containing “meeting”\n",
        "and “management”, but also those containing “meetings” and potentially even\n",
        "“manager” and “managerial”. The machine, on the other hand, does not know that\n",
        "these words are related, similar, or basically different forms of the same word.\n",
        "- We have an ability to focus on what matters: in fact, when reading texts we usually skim through them rather than pay equal attention to each word. For instance, when\n",
        "reading a sentence “Last Friday the management committee had a meeting”, which\n",
        "words do you pay more attention to? Which ones express the key idea of this\n",
        "message? Think about it – and we will return to this question later. The machines, on the other hand, should be specifically “told” which words matter more.\n",
        "- Finally, we also intuitively know how to judge what is more relevant. The machines can make relevance judgments, too, but unlike us humans they need to be “told” how to measure relevance in precise numbers.\n",
        "\n",
        "That, in a nutshell, represents the basic steps in the search algorithm.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "In this notebook, you will learn about other NLP techniques to preselect words, map the different forms of the same word to each other and weigh the words according to how much information they contribute to the task. Then you will build an information search algorithm that for any query (for example, “management meetings”) will find the most relevant documents in the collection of documents (for example, all minutes of the past managerial meetings sorted by their relevance).\n",
        "\n",
        "Suppose you have built such an application following all the steps. You type in a query and the algorithm returns a document or several documents that are supposedly relevant to this query. How can you tell whether the algorithm has picked out the right documents?\n",
        "\n",
        "Let’s use a dataset of documents and queries, where the documents are labeled with respect to their relevance to the queries. You will use this dataset as your gold standard, and before using the information search algorithm in practice, evaluate its performance against the ground truth labels in the labeled dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Kxz2AhWZWL"
      },
      "source": [
        "###Data and data structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FalL8sCWaUr"
      },
      "source": [
        "You are going to use a publicly available dataset labeled for the task. That\n",
        "means, a dataset with a number of documents and various queries, and a labeled list specifying which queries correspond to which documents. Once you implement and evaluate a search algorithm on such data labeled with ground truth, you can apply it to your own documents in your own projects.\n",
        "\n",
        "You will use the dataset collected by the Centre for Inventions and Scientific Information (CISI), which contains abstracts and some additional metadata on the journal articles on information systems and information retrieval.\n",
        "\n",
        "You will need to keep precisely three data structures for this application: \n",
        "- one for the documents, \n",
        "- another one for the queries, and \n",
        "- the third one matching the queries to the documents\n",
        "\n",
        "Information search is based on the idea that the content of a document or set of documents is relevant given the content of a particular query, so both documents and queries data structures should keep the contents of all documents and all queries. \n",
        "\n",
        "What would be the best way to keep track of which content represents which document?\n",
        "\n",
        "The most informative and useful way would be to assign a unique identifier – an index – to each document and each query. You can imagine, for example, storing content of the documents and queries in two separate tables, with each row representing a single document or query, and row numbers corresponding to the documents and queries ids. In Python, tables can be represented with dictionaries.\n",
        "\n",
        "Now, if you keep two Python dictionaries (tables) matching each unique document\n",
        "identifier (called key) to the document’s content (called value) in documents dictionary and matching each unique query identifier to the query’s content in queries dictionary, how should you represent the relevance mappings? \n",
        "\n",
        "You can use a dictionary structure again: this time, the keys will contain the queries ids, while the values should keep the matching documents ids. Since each query may correspond to multiple documents, it would be best to\n",
        "keep the ids of the matching documents as lists.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "As this figure shows, query with id 1 matches documents with ids `1` and `1460`, therefore the mappings data structure keeps a list of `[1, 1460]` for query `1`; similarly it keeps `[3]` for query `2`, `[2]` for query `112`, and an empty list for query `3`, because in this example there are no\n",
        "documents relevant for this query.\n",
        "\n",
        "Now let’s look into the CISI dataset and code the data reading and initialization step. All documents are stored in a single text file CISI.ALL. It has a peculiar format: it keeps the abstract of each article and some additional information, such as the index in the set, the title, authors’ list and cross-references – a list of indexes for the articles that cite each other.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "For the information search application, arguably the most useful information is the content of the abstract: abstracts in the articles typically serve as a concise summary of what the article presents, something akin to a snippet.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "As you can see, the field identifiers such as .A or .W are separated from the actual text by new line. In addition, the text within each field, for example, the abstract may be spread across multiple lines. Ideally, we would like to convert this format into something like text.\n",
        "\n",
        "Note that for the text that falls within the same field, e.g. .W, the line breaks (“\\n”) are replaced with whitespaces, so each line now starts with a field identifier followed by the field content:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "The format is much easier to work with: you can now read the text line by line,\n",
        "extract the unique identifier for the article from the field .I, merge the content of the fields `.T`, `.A` and `.W`, and store the result in the documents dictionary as `{1: \"18 Editions of the … this country and abroad.\"}`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_documents():\n",
        "  file = open(\"cisi/CISI.ALL\")\n",
        "  merged = \"\"\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    # Unless a string starts with a new field identifier, add the content to the current field separating the content from the previous line with a whitespace; \n",
        "    #votherwise, start a new line with the next identifier and field.\n",
        "    if a_line.startswith(\".\"):\n",
        "      merged += \"\\n\" + a_line.strip()\n",
        "    else:\n",
        "      merged += \" \" + a_line.strip()\n",
        "\n",
        "    documents = {}\n",
        "    content = \"\"\n",
        "    doc_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "      if a_line.startswith(\".I\"):\n",
        "        doc_id = a_line.split(\" \")[1].strip()  # doc_id can be extracted from the line with the .I field identifier\n",
        "      elif a_line.startswith(\".X\"):\n",
        "        documents[doc_id] = content\n",
        "        content = \"\"\n",
        "        doc_id = \"\"\n",
        "      else:\n",
        "        content += a_line.strip()[3:] + \" \"  # Otherwise, keep extracting the content from other fields (.T, .A and .W) removing the field identifiers themselves\n",
        "\n",
        "  file.close()   \n",
        "  return documents"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACa4lfWR70RR"
      },
      "source": [
        "As a sanity check, print out the size of the dictionary (make sure it contains all 1460 articles) and print out the content of the very first article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IE3w3wF560v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf1e6d0-e97d-48a1-eb50-66132defff9e"
      },
      "source": [
        "documents = read_documents()\n",
        "print(len(documents))\n",
        "print(documents.get(\"1\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "The queries are stored in `CISI.QRY` file and follow a very similar format: half the time, you see only two fields – `.I` for the unique identifier and `.W` for the content of the query. \n",
        "\n",
        "Other queries though are formulated not as questions but rather as abstracts from other articles. In such cases, the query also has an `.A` field for the authors’ list, `.T` for the title and `.B` field, which keeps the reference to the original journal in which the abstract was published.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "We are going to only focus on the unique identifiers and the content of the query itself (fields `.W` and `.T`, where available), so the code below is quite similar to the above as it allows you to populate the queries dictionary with data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8u2XdijC_8n"
      },
      "source": [
        "def read_queries():\n",
        "  file = open(\"cisi/CISI.QRY\")\n",
        "  merged = \"\"\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    # Unless a string starts with a new field identifier, add the content to the current field separating the content from the previous line with a whitespace; \n",
        "    #votherwise, start a new line with the next identifier and field.\n",
        "    if a_line.startswith(\".\"):\n",
        "      merged += \"\\n\" + a_line.strip()\n",
        "    else:\n",
        "      merged += \" \" + a_line.strip()\n",
        "\n",
        "    queries = {}\n",
        "    content = \"\"\n",
        "    query_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "      if a_line.startswith(\".I\"):\n",
        "        if not content == \"\":\n",
        "          queries[query_id] = content\n",
        "          content = \"\"\n",
        "          query_id = \"\"\n",
        "        query_id = a_line.split(\" \")[1].strip()  # query_id can be extracted from the line with the .I field identifier\n",
        "      elif a_line.startswith(\".W\"):\n",
        "        content += a_line.strip()[3:] + \" \"  # Otherwise, keep adding content to the content variable\n",
        "\n",
        "    # The very last query is not followed by any next .I field, so the strategy from above won’t work –\n",
        "    # you need to add the entry for the last query to the dictionary using this extra step\n",
        "    queries[query_id] = content\n",
        "\n",
        "  file.close()   \n",
        "  return queries"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gayZGk7w-SA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24258353-7e4c-4355-f568-5189b4f330e3"
      },
      "source": [
        "queries = read_queries()\n",
        "\n",
        "# Print out the length of the dictionary (it should contain 112 entries), and the content of the very first query\n",
        "print(len(queries))\n",
        "print(queries.get(\"1\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112\n",
            "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Finally, let's read in the mapping between the queries and the documents – we'll keep these in the mappings data structure – with tuples where each query index (key) corresponds to the list of one or more document indices (value):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjDZk7qDw6v"
      },
      "source": [
        "def read_mappings():\n",
        "  file = open(\"cisi/CISI.REL\")\n",
        "\n",
        "  mappings = {}\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    voc = a_line.strip().split()\n",
        "    key = voc[0].strip()\n",
        "    current_value = voc[1].strip()  # The key (query id) is stored in the first column, while the document id is stored in the second column\n",
        "    value = []\n",
        "    \"\"\"\n",
        "    If the mappings dictionary already contains some document ids for the documents matching the given\n",
        "    query, you need to update the existing list with the current value; otherwise just add current value to the new list\n",
        "    \"\"\"\n",
        "    if key in mappings.keys():\n",
        "      value = mappings.get(key)\n",
        "    value.append(current_value)\n",
        "    mappings[key] = value\n",
        "\n",
        "  file.close()   \n",
        "  return mappings"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "fb9d853e-2bb1-47e3-9f84-a7c7be15bdaf"
      },
      "source": [
        "mappings = read_mappings()\n",
        "print(len(mappings))\n",
        "print(mappings.keys())\n",
        "print(mappings.get(\"1\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n",
            "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n",
            "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "That’s it – you have successfully initialized one dictionary for documents with the ids linked to the articles content, another dictionary for queries linking queries ids to their correspondent texts, and the mappings dictionary, which matches the queries ids to the lists of relevant document ids.\n",
        "\n",
        "Now, you are all set to start implementing the search algorithm for this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN1--BjSEqHS"
      },
      "source": [
        "### Boolean search algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU95P6O-EuPN"
      },
      "source": [
        "Let’s start with the simplest approach: the information need is formulated as a query. If you extract the words from the query, you can then search for the documents that contain these words and return these documents, as they should be relevant to the query.\n",
        "\n",
        "Here is the algorithm in a nutshell:\n",
        "- Extract the words from the query\n",
        "- For each document, compare the words in the document to the words in the query\n",
        "- Return the document as relevant if any of the query words occurs in the document\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/8.png?raw=1' width='800'/>\n",
        "\n",
        "The very first step in this algorithm is extraction of the words from both queries and documents. You may recall that text comes in as a sequence of symbols or characters, and the machine needs to be told what a word is – you used a special NLP tool called tokenizer to extract words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lShv7w3b9DT3"
      },
      "source": [
        "def get_words(text):\n",
        "  word_list = [word for word in word_tokenize(text.lower())]  # Text is converted to lower case and split into words\n",
        "  return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA1TtCgA-ZSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5db4a8a-ae6e-4ba9-e502-a945206f9c1a"
      },
      "source": [
        "doc_words = {}\n",
        "query_words = {}\n",
        "\n",
        "# Entries in both documents and queries are represented as word lists\n",
        "for doc_id in documents.keys():\n",
        "  doc_words[doc_id] = get_words(documents.get(doc_id))\n",
        "for qry_id in queries.keys():\n",
        "  query_words[qry_id] = get_words(queries.get(qry_id))  \n",
        "\n",
        "# check out the length of the dictionaries (these should be the same as before – 1460 and 112), \n",
        "# and check what words are extracted from the first document and the first query\n",
        "print(len(doc_words))\n",
        "print(doc_words.get(\"1\"))\n",
        "print(len(doc_words.get(\"1\")))\n",
        "\n",
        "print(len(query_words))\n",
        "print(query_words.get(\"1\"))\n",
        "print(len(query_words.get(\"1\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            "['18', 'editions', 'of', 'the', 'dewey', 'decimal', 'classifications', 'comaromi', ',', 'j.p.', 'the', 'present', 'study', 'is', 'a', 'history', 'of', 'the', 'dewey', 'decimal', 'classification', '.', 'the', 'first', 'edition', 'of', 'the', 'ddc', 'was', 'published', 'in', '1876', ',', 'the', 'eighteenth', 'edition', 'in', '1971', ',', 'and', 'future', 'editions', 'will', 'continue', 'to', 'appear', 'as', 'needed', '.', 'in', 'spite', 'of', 'the', 'ddc', \"'s\", 'long', 'and', 'healthy', 'life', ',', 'however', ',', 'its', 'full', 'story', 'has', 'never', 'been', 'told', '.', 'there', 'have', 'been', 'biographies', 'of', 'dewey', 'that', 'briefly', 'describe', 'his', 'system', ',', 'but', 'this', 'is', 'the', 'first', 'attempt', 'to', 'provide', 'a', 'detailed', 'history', 'of', 'the', 'work', 'that', 'more', 'than', 'any', 'other', 'has', 'spurred', 'the', 'growth', 'of', 'librarianship', 'in', 'this', 'country', 'and', 'abroad', '.']\n",
            "113\n",
            "112\n",
            "['what', 'problems', 'and', 'concerns', 'are', 'there', 'in', 'making', 'up', 'descriptive', 'titles', '?', 'what', 'difficulties', 'are', 'involved', 'in', 'automatically', 'retrieving', 'articles', 'from', 'approximate', 'titles', '?', 'what', 'is', 'the', 'usual', 'relevance', 'of', 'the', 'content', 'of', 'articles', 'to', 'their', 'titles', '?']\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Lk8UFnAF6A"
      },
      "source": [
        "Now let’s code the simple search algorithm. We will refer to it as the\n",
        "Boolean search algorithm since it relies on presence (1) or absence (0) of the query words in the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOw12-HSAJfL"
      },
      "source": [
        "def retrieve_documents(doc_words, query):\n",
        "  docs = []\n",
        "  query_word = []\n",
        "  for doc_id in doc_words.keys():\n",
        "    found = False\n",
        "    i = 0\n",
        "    # Keep iterating through the words in the query word list until either of the two conditions is satisfied\n",
        "    while i < len(query) and not found:\n",
        "      word = query[i]\n",
        "      if word in doc_words.get(doc_id):\n",
        "        docs.append(doc_id)\n",
        "        query_word.append(word)\n",
        "        found = True\n",
        "      else:\n",
        "        i += 1\n",
        "  return (docs, query_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTMBsGnQAx8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412bbe5b-b0e4-42a9-907f-86ab0630ba07"
      },
      "source": [
        "# Check the results: select a query by its id (e.g., query with id 3 here), print out the ids of the documents\n",
        "# that the algorithm found (e.g., the first 100, as there may be many),check how many there are in total\n",
        "docs, query_word = retrieve_documents(doc_words, query_words.get(\"3\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))\n",
        "print(query_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102']\n",
            "1397\n",
            "['is', 'is', 'is', 'information', 'is', 'is', '.', 'is', 'definitions', 'is', 'is', 'information', 'what', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'what', 'is', 'is', 'information', 'what', 'is', 'is', 'is', 'information', 'information', 'is', '.', '.', 'is', 'is', '.', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'information', '.', 'is', 'information', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'science', 'information', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', '.', 'what', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'information', 'is', 'information', 'is', 'what', 'what', 'what', 'what', '.', 'science', 'is', 'is', 'is', 'is', 'what', 'information', 'information', 'is', 'definitions', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'possible', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'information', 'is', 'information', '.', 'what', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'what', 'what', 'is', 'is', 'is', 'is', '.', 'is', 'what', 'is', 'is', 'give', 'science', 'is', 'what', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'science', 'is', 'is', 'is', 'what', 'information', 'is', 'information', 'is', 'what', 'what', '.', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'science', 'what', 'is', 'is', 'is', 'what', 'is', 'what', 'is', '.', '.', '.', 'is', 'is', 'what', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'what', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'is', 'is', 'what', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'definitions', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', '.', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'information', 'is', 'information', 'information', '.', 'is', 'is', 'what', 'is', 'is', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'information', 'information', 'what', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'information', 'science', 'is', 'is', 'what', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'science', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'possible', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'information', 'is', 'is', 'is', '.', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', '.', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'where', '.', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', '?', 'what', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'is', 'what', 'information', 'possible', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'science', 'information', 'is', 'is', 'what', 'what', 'is', 'is', '.', 'is', 'is', 'what', 'is', '.', 'information', 'what', 'is', 'what', 'is', 'information', '.', '.', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', '.', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'science', 'information', 'is', '?', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'what', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', '.', '.', 'what', '.', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', '.', 'what', 'is', 'is', 'what', 'is', 'what', '.', 'is', 'what', 'information', 'is', 'is', '.', 'information', '?', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'science', '.', 'is', 'possible', 'is', 'is', 'is', 'information', 'information', 'is', 'what', 'is', 'is', 'what', '.', 'is', 'science', '.', 'science', 'is', 'is', 'is', 'information', 'is', 'is', 'information', 'is', '.', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'information', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'definitions', 'information', 'information', 'is', 'is', 'science', 'is', 'is', 'is', 'information', 'is', 'what', 'information', 'is', 'what', '.', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'possible', 'information', 'is', 'is', 'is', '.', 'is', 'is', '.', 'is', '.', '.', 'is', 'what', 'is', 'is', 'what', 'is', 'is', 'what', 'is', 'what', '.', 'is', 'what', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'what', 'is', 'what', 'is', 'possible', 'is', 'is', 'science', 'is', 'is', 'is', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'what', 'what', '.', 'what', '.', 'science', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'is', '.', 'information', 'information', 'information', 'is', 'is', 'is', '.', '.', 'information', '.', 'is', 'is', 'information', 'is', 'information', 'information', 'information', 'information', 'information', '.', 'science', '.', 'is', 'is', 'is', '.', 'information', 'science', 'is', 'science', 'is', 'science', 'science', 'what', 'is', 'is', 'is', 'is', '.', '.', 'what', 'is', 'information', 'give', 'is', 'information', '.', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'what', '.', 'is', 'science', 'what', 'is', 'is', 'is', 'is', 'science', 'science', 'is', 'is', 'is', 'is', 'information', '.', '.', '.', 'is', '.', 'information', 'what', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'information', 'is', '.', '.', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'science', '.', 'what', 'information', 'what', 'is', 'what', 'is', '.', 'is', 'is', '.', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', '.', 'is', 'is', 'is', 'what', 'information', 'what', 'is', 'information', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'what', '.', 'is', 'is', '.', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7z3FYS7Bo_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23de0d38-aa0b-4767-8435-0531621b0727"
      },
      "source": [
        "# Let’s, for example, look into how the algorithm decided on the documents relevant for query with id 6\n",
        "docs, query_word = retrieve_documents(doc_words, query_words.get(\"6\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))\n",
        "print(query_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n",
            "1460\n",
            "['there', 'are', 'are', 'for', 'for', 'are', 'for', 'there', 'for', 'are', 'are', 'for', 'what', 'are', 'communication', 'are', 'what', 'and', 'for', 'for', 'for', 'possibilities', 'are', 'what', 'are', 'are', 'for', 'what', 'are', 'are', 'are', 'are', 'between', 'are', 'there', ',', 'and', ',', 'between', 'for', 'for', 'for', 'are', 'for', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'for', 'and', 'and', 'are', 'are', 'for', 'are', 'for', 'what', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'and', 'for', 'and', 'are', 'for', 'and', 'for', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'there', 'are', 'what', 'and', 'are', 'are', 'are', 'are', 'communication', 'are', 'for', 'communication', 'are', 'for', 'are', 'there', 'for', 'are', 'possibilities', 'there', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'what', 'are', 'for', 'between', 'for', 'are', 'there', 'are', 'are', 'are', 'what', 'are', 'for', 'for', 'are', 'and', 'what', 'for', ',', 'are', 'there', 'are', 'are', 'there', 'between', 'are', 'for', 'are', 'for', 'what', 'there', 'between', 'are', 'are', 'are', 'what', ',', 'what', 'what', 'what', 'for', 'are', 'are', 'for', 'are', 'are', 'what', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'are', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'are', 'for', 'are', 'are', 'and', 'what', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'between', 'for', 'are', 'are', 'are', 'for', 'what', 'are', 'what', 'for', 'are', 'are', 'are', 'are', 'are', 'between', 'are', 'what', 'possibilities', 'and', 'are', 'what', 'for', 'between', 'are', 'what', 'what', 'what', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'for', 'are', 'between', 'are', 'and', 'are', 'for', 'are', 'and', 'are', 'are', 'for', 'for', 'there', 'and', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'and', 'are', 'and', 'communication', 'and', 'are', 'between', 'for', 'for', 'and', 'are', 'what', 'are', 'are', 'are', 'communication', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'there', 'between', 'are', 'are', 'are', 'are', 'and', 'for', 'are', 'between', 'are', 'what', 'and', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'and', 'are', 'there', 'and', 'are', 'are', 'there', 'what', 'for', 'are', 'are', 'for', 'what', 'for', 'for', 'and', 'are', 'what', 'what', 'for', 'are', 'are', 'are', 'for', 'are', 'and', 'are', 'what', 'are', 'are', 'are', 'and', 'are', 'between', 'are', 'what', 'are', 'for', 'what', 'are', 'are', 'are', 'what', 'are', 'what', 'for', 'for', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'what', 'are', 'and', 'for', 'for', 'for', 'are', 'for', 'are', 'for', 'communication', 'are', 'what', 'for', 'for', 'and', 'what', 'what', 'are', 'are', 'are', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'between', 'for', 'there', 'are', 'for', 'are', 'there', 'there', 'are', 'communication', 'are', 'for', 'are', 'between', 'what', 'are', 'for', 'for', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'and', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'there', 'what', 'are', 'are', 'are', 'for', 'what', 'there', 'are', 'are', 'are', 'are', 'possibilities', 'are', 'for', ',', 'are', 'what', 'are', 'are', 'are', 'are', 'and', 'are', 'for', 'for', 'for', 'are', 'for', 'are', 'are', 'for', 'for', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'and', 'for', 'are', 'are', 'for', 'there', 'are', 'are', 'for', 'are', 'for', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'for', 'what', 'for', 'for', 'are', 'are', 'for', 'are', 'possibilities', 'what', 'for', 'for', 'for', 'what', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'possibilities', 'for', 'are', 'are', 'and', 'are', 'for', 'between', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'what', 'for', 'for', 'are', 'are', 'are', 'communication', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'what', 'are', 'for', 'are', 'for', 'are', 'are', 'between', 'and', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'what', 'are', 'and', 'are', 'are', 'are', 'and', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'for', 'for', 'and', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'for', 'for', 'are', 'are', 'are', 'what', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'and', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'are', 'are', 'are', 'and', 'are', 'are', 'and', 'are', 'are', 'are', 'for', 'are', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'there', 'for', 'are', 'are', 'for', 'there', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'and', ',', 'there', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', ',', 'for', 'what', 'are', 'are', 'there', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'there', 'and', 'are', 'for', 'are', 'are', 'are', 'are', 'and', 'are', 'are', 'for', 'are', 'are', 'are', 'possibilities', 'are', 'for', 'are', 'for', 'for', 'are', 'are', 'for', 'for', 'are', 'and', 'and', 'are', 'are', 'are', 'are', 'and', 'for', 'for', 'for', ',', 'are', 'for', 'are', 'are', 'for', 'for', 'for', ',', 'are', 'between', 'between', 'for', 'are', 'what', 'are', 'are', 'are', 'are', 'and', 'what', 'for', 'are', 'are', 'and', 'and', 'are', 'are', 'are', 'are', 'for', 'and', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'what', 'for', 'are', 'are', 'what', 'and', 'what', 'for', 'and', 'possibilities', 'are', 'there', 'there', 'are', 'and', 'are', 'are', 'for', 'are', 'for', 'what', 'what', 'are', 'are', 'for', 'are', 'are', 'what', 'and', 'for', 'for', 'what', 'for', 'what', 'for', 'are', 'and', 'and', 'for', 'for', 'what', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'communication', 'between', 'for', 'are', 'and', 'are', 'there', 'and', 'are', 'for', 'are', 'for', 'for', 'communication', 'for', 'for', 'are', 'there', 'are', 'there', 'are', 'there', 'possibilities', 'are', 'are', 'for', 'are', 'for', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'what', 'there', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'what', 'what', 'between', 'for', 'for', 'are', 'for', 'and', 'for', 'possibilities', 'for', 'and', 'what', 'between', 'what', 'for', 'are', 'are', 'and', 'are', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'communication', 'what', 'and', 'for', 'what', 'for', 'what', 'and', 'are', 'what', 'for', 'are', 'are', 'for', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'and', 'and', 'for', 'are', 'are', 'are', 'are', 'possibilities', 'for', 'are', 'are', 'what', 'are', 'and', 'what', 'and', 'communication', 'and', 'for', 'for', ',', 'for', 'are', 'are', 'are', 'are', 'are', 'possibilities', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'are', ',', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'for', 'are', 'possibilities', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'possibilities', 'possibilities', 'possibilities', 'are', 'what', 'for', 'are', 'what', 'for', 'are', 'for', 'for', 'are', 'for', 'there', 'are', 'are', 'and', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'and', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'what', 'are', 'are', 'for', 'and', 'and', 'for', 'and', 'are', 'are', 'for', 'and', 'are', 'for', 'between', 'are', 'for', 'are', 'are', 'and', 'there', 'there', 'what', 'communication', 'are', 'what', 'are', 'there', 'what', 'for', 'what', 'and', 'for', 'what', 'between', 'and', 'for', 'for', 'are', 'for', 'are', 'are', 'and', 'are', 'are', 'and', 'there', 'and', 'what', 'what', 'for', 'what', 'are', 'for', 'are', 'are', 'for', 'there', 'for', 'are', 'what', 'for', 'what', 'for', 'for', 'there', 'for', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'there', 'for', 'for', 'what', 'what', 'for', 'what', ',', 'and', 'for', 'for', 'are', 'for', 'are', 'are', 'there', ',', 'for', 'and', 'there', ',', 'for', 'for', ',', 'are', 'for', ',', 'communication', ',', 'for', 'for', 'for', 'are', 'communication', 'are', 'communication', 'are', 'are', 'and', 'for', ',', 'for', 'for', 'computers', ',', 'and', 'for', 'are', 'for', 'for', ',', 'and', 'what', 'and', 'and', 'are', 'are', 'for', 'for', 'what', 'and', 'are', 'are', 'are', 'and', 'for', 'are', 'there', 'what', 'there', 'are', 'for', 'what', 'what', 'are', 'are', 'and', 'what', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'what', 'and', 'for', 'are', 'are', 'are', 'for', 'there', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'and', 'for', 'are', 'are', 'for', 'are', 'for', ',', 'are', 'for', 'what', 'for', 'what', 'possibilities', 'what', 'are', 'and', 'possibilities', 'between', 'and', 'for', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'and', 'for', 'are', 'for', 'there', 'for', 'what', 'and', 'what', 'for', 'are', 'for', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'are', 'for', 'for', 'what', 'are', 'for', 'for', 'communication', 'are', 'for', 'are', 'for', 'are', 'are', 'there', 'what', 'are', 'there', 'for', 'and', 'are']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKp-DxN0CnWv"
      },
      "source": [
        "As it shows, the query is matched to the document based on occurrence of such words as “there”, “this”, “the”, “and”, “is” and even a comma since punctuation marks are part of the word list returned by the tokenizer:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/9.png?raw=1' width='800'/>\n",
        "\n",
        "On the face of it, there is a considerable word overlap between the query and the document, yet if you read the text of the query and the text of the document, they don’t seem to have any ideas in common, so in fact this document is not relevant for the given query at all! \n",
        "\n",
        "It seems like the words on the basis of which the query and the document are matched here are simply the wrong ones – they are somewhat irrelevant to the actual information need expressed in the query. \n",
        "\n",
        "How can you make sure that the query and the documents are matched on the basis of more meaningful words?\n",
        "\n",
        "\n",
        "---\n",
        "**Exercise**\n",
        "\n",
        "Another way to match the documents to the queries would be to make it a requirement that the document should contain all the words from the query rather than any.\n",
        "\n",
        "Is this a better approach? Modify the code of the simple Boolean search algorithm to match documents to the queries on the basis of all words, and compare the results.\n",
        "\n",
        "---\n",
        "\n",
        "You will notice that it is rarely the case that a document, even if it is generally relevant,\n",
        "contains all words from the query (at the very least, it does not have to contain question\n",
        "words like “what” and “which” from the query to be relevant). Therefore, this more\n",
        "conservative approach of returning only the documents with all query words in them will\n",
        "work even worse at this stage – it simply will not find any relevant documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ8XIreJRVH3"
      },
      "source": [
        "def retrieve_documents(doc_words, query):\n",
        "  docs = []\n",
        "  for doc_id in doc_words.keys():\n",
        "    # here, you are interested in the documents that contain all words\n",
        "    found = True\n",
        "    i = 0\n",
        "    # iterate through words in the query\n",
        "    while i < len(query) and found:\n",
        "      word = query[i]\n",
        "      if not word in doc_words.get(doc_id):\n",
        "        # if the word is not in document, turn found flag off and stop\n",
        "        found = False\n",
        "      else:\n",
        "        # rwise, move on to the next query word\n",
        "        i += 1\n",
        "\n",
        "    # if all words are found in the document, the last index is len(query)-1 add the doc_id only in this case\n",
        "    if i == len(query) - 1:\n",
        "      docs.append(doc_id)\n",
        "  return docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCI2VNY7Sp75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e183251-672e-48c8-a726-6b05458fd18b"
      },
      "source": [
        "docs = retrieve_documents(doc_words, query_words.get(\"112\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4CWm8gS58e"
      },
      "source": [
        "In fact, it is a very rare case that you may have any single document that contains all the words from the query, therefore, with this approach, you will likely get no relevant documents returned for any queries in this dataset.\n",
        "\n",
        "\n",
        "Before we move on, let’s summarize which steps of the algorithm you have implemented so far: you have read the data, initialized the data structures and tokenized the texts.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/10.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Since we have identified several weaknesses of the current algorithm. \n",
        "\n",
        "Let’s look into further preprocessing steps that will help you represent the content of both the documents and the queries in a more informative way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fESB5r4iPCnJ"
      },
      "source": [
        "###Preselecting the words that matter: stopwords removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFZmaF6DPDrc"
      },
      "source": [
        "The main problem with the search algorithm identified so far is that it considers all words in the queries and documents as equally important. This leads to poor search results, but on top of that it is also intuitively incorrect.\n",
        "\n",
        "You may notice that not all words are equally meaningful in the sentences above. A good test for that would be to ask yourself whether you can define in one phrase what a particular word means: for example, what does “the” mean? You can say that “the” does not have a precise meaning of its own, rather it serves a particular function.\n",
        "\n",
        "In linguistic terms, such words are called function words. You might even notice that when you read a text, for example an article or an email, you tend to skim over such words without paying much attention to them.\n",
        "\n",
        "What happens to the search algorithm when these words are present? You have seen in the example before that they don’t help identify the relevant texts, so in fact the algorithm’s effort is wasted on them. What would happen if the less meaningful words were not taken into consideration?\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/11.png?raw=1' width='800'/>\n",
        "\n",
        "You can see that, were the less meaningful words removed before matching documents to queries, document 1 would not stand a chance – there is simply not a single word overlapping between the query and this document. You can also see that the words that are not grayed out concisely summarize the main idea of the text.\n",
        "\n",
        "**This suggests the first improvement to the developed algorithm: let’s remove the less meaningful words. In NLP applications, the less meaningful words are called stopwords,** and luckily you don’t have to bother with enumerating them – since stopwords are highly repetitive in English, most NLP toolkits have a specially defined stopwords list, so you can rely on this list when processing the data, unless you want to customize it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def process(text):\n",
        "  stoplist = set(stopwords.words(\"english\"))  # use English stopwords\n",
        "  # Tokenize text, convert it to lower case and only add the words if they are not included in the stoplist and are not punctuation marks\n",
        "  word_list = [word for word in word_tokenize(text.lower())\n",
        "               if not word in stoplist and not word in string.punctuation\n",
        "              ]\n",
        "\n",
        "  return word_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c6fcf4-3ced-400e-e399-8bfdf04388f3"
      },
      "source": [
        "# Check the result of these preprocessing steps on some documents or queries, e.g. document 1\n",
        "word_list = process(documents.get(\"1\"))\n",
        "print(word_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['18', 'editions', 'dewey', 'decimal', 'classifications', 'comaromi', 'j.p.', 'present', 'study', 'history', 'dewey', 'decimal', 'classification', 'first', 'edition', 'ddc', 'published', '1876', 'eighteenth', 'edition', '1971', 'future', 'editions', 'continue', 'appear', 'needed', 'spite', 'ddc', \"'s\", 'long', 'healthy', 'life', 'however', 'full', 'story', 'never', 'told', 'biographies', 'dewey', 'briefly', 'describe', 'system', 'first', 'attempt', 'provide', 'detailed', 'history', 'work', 'spurred', 'growth', 'librarianship', 'country', 'abroad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3BMBD_Y9Bw"
      },
      "source": [
        "That is, the preprocessing step helps removing the stopwords like “of” and “the” from the word list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRIjhiQsZDKF"
      },
      "source": [
        "###Matching forms of same word: morphological processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUZcXSxRZD62"
      },
      "source": [
        "One effect that stopwords and punctuation marks removal has is optimization of search algorithm – the words that do not matter much are removed, so the computational resources are not wasted on them. In general, the more concise and the more informative the data representation is, the better.\n",
        "\n",
        "This brings us to the next issue. Take a look at the query with id 15\n",
        "and document with id 27, which are a match according to the ground truth mappings:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/11.png?raw=1' width='800'/>\n",
        "> The words highlighted in blue will be matched between the query and the document; the ones in red will be missed.\n",
        "\n",
        "The reason for this mismatch is that words may take different forms in different contexts: some contexts may require a mention of a single object or concept like “system”, while others may need multiple “systems” to be mentioned. \n",
        "\n",
        "Such different forms of a word that depend on the context and express different aspects of meaning, for instance multiplicity of “systems”, are technically called morphological forms, and when you see a word like “systems” and try to match it to its other variant “system” you are dealing with morphology.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/12.png?raw=1' width='800'/>\n",
        "\n",
        "Stemming takes word matching one step further and tries to map related words across the board, and this means not just the forms of the very same word. For that, the stemmers rely on a set of rules that try to reduce the related words to the same basic core.\n",
        "\n",
        "The stem in `{retrieve, retrieves, retrieved, retrieving, retrieval}` is retriev. So here is the difference with the technique that you used before – stemming might result in non-words, as for example, you\n",
        "won’t find a word like retriev in a dictionary. To provide you with a couple of other examples, the stem for `{expect, expects, expected, expecting, expectation, expectations}` is expect and the stem for `{continue, continuation, continuing}` is continu.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/13.png?raw=1' width='800'/>\n",
        "\n",
        "Note that the stemmer tries to identify which part of the word is shared between the different forms and related words, and returns this part as a stem by cutting off the differing word endings.\n",
        "\n",
        "Now let’s implement the stemming preprocessing step using NLTK’s stemming\n",
        "functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT778vpj_DxC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Extract and normalize the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        "Once the words are extracted from running text, you need to convert them into features. In particular, you need to put all words into lower case to make your algorithm establish the connection between different formats like “Lottery” and “lottery”.\n",
        "\n",
        "Putting all strings to lower case can be achieved with Python’s string functionality. To extract the features (words) from the text, you need to iterate through the recognized words and put all words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmupbFaDWCl1"
      },
      "source": [
        "def get_features(text):\n",
        "  features = {}\n",
        "  word_list = [word for word in word_tokenize(text.lower())]\n",
        "  # For each word in the email let’s switch on the ‘flag’ that this word is contained in the email\n",
        "  for word in word_list:\n",
        "    features[word] = True\n",
        "  \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Lb5pktWv5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1354891c-14b1-40c7-8c25-10b8f6efc413"
      },
      "source": [
        "# it will keep tuples containing the list of features matched with the “spam” or “ham” label for each email\n",
        "all_features = [(get_features(email), label) for (email, label) in all_emails]\n",
        "\n",
        "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
        "\n",
        "print(len(all_features))\n",
        "print(len(all_features[0][0]))\n",
        "print(len(all_features[99][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
            "5172\n",
            "29\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZRPIDsSaE8E"
      },
      "source": [
        "With this bit of code, you iterate over the emails in your collection (all_emails) and store the list of features extracted from each email matched with the label.\n",
        "\n",
        "For example, if a spam email consists of a single sentence “Participate In Our New Lottery NOW!” your algorithm will first extract the list of features present in this email and assign a ‘True’ value to each of them.\n",
        "\n",
        "Then, the algorithm will add this list of features to\n",
        "all_features together with the “spam” label.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Imagine your whole dataset contained only one spam text “Participate In Our New Lottery NOW!” and one ham text “Participate in the Staff Survey”. What features will be extracted from this dataset?\n",
        "\n",
        "You will end up with the following feature set:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Let’s now clarify what each tuple structure representing an email contains. Tuples pair up two information fields: in this case a list of features extracted from the email and its label, i.e. each tuple in `all_features` contains a pair (`list_of_features`, `label`).\n",
        "\n",
        "So if you’d like to access first email in the list, you call on `all_features[0]`, to access its list of features you use `all_features[0][0]`, and to access its label you use `all_features[0][1]`.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr356828cl5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a4517b-c3d4-4985-a4fe-2ac1ad14ee7a"
      },
      "source": [
        "# access first email in the list with feature and label\n",
        "all_features[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'!': True,\n",
              "  ',': True,\n",
              "  '.': True,\n",
              "  ':': True,\n",
              "  'am': True,\n",
              "  'and': True,\n",
              "  'fl': True,\n",
              "  'for': True,\n",
              "  'friends': True,\n",
              "  'from': True,\n",
              "  'hey': True,\n",
              "  'hi': True,\n",
              "  'homepage': True,\n",
              "  'i': True,\n",
              "  'is': True,\n",
              "  'jane': True,\n",
              "  'last': True,\n",
              "  'looking': True,\n",
              "  'miami': True,\n",
              "  'my': True,\n",
              "  'name': True,\n",
              "  'new': True,\n",
              "  'photos': True,\n",
              "  'see': True,\n",
              "  'subject': True,\n",
              "  'webcam': True,\n",
              "  'weblog': True,\n",
              "  'with': True,\n",
              "  'you': True},\n",
              " 'spam')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FS65Hbmcq_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b198ee-19f4-4347-ae79-04b1dd4e0df2"
      },
      "source": [
        "# access its list of features only\n",
        "all_features[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': True,\n",
              " ',': True,\n",
              " '.': True,\n",
              " ':': True,\n",
              " 'am': True,\n",
              " 'and': True,\n",
              " 'fl': True,\n",
              " 'for': True,\n",
              " 'friends': True,\n",
              " 'from': True,\n",
              " 'hey': True,\n",
              " 'hi': True,\n",
              " 'homepage': True,\n",
              " 'i': True,\n",
              " 'is': True,\n",
              " 'jane': True,\n",
              " 'last': True,\n",
              " 'looking': True,\n",
              " 'miami': True,\n",
              " 'my': True,\n",
              " 'name': True,\n",
              " 'new': True,\n",
              " 'photos': True,\n",
              " 'see': True,\n",
              " 'subject': True,\n",
              " 'webcam': True,\n",
              " 'weblog': True,\n",
              " 'with': True,\n",
              " 'you': True}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOwXVnfdHdp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6eebed7-85d7-420f-ba42-51cf0c79fcdb"
      },
      "source": [
        "# access its label only\n",
        "all_features[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkWOZNfdMC0"
      },
      "source": [
        "##Step 4: Train the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSAh72HdP-X"
      },
      "source": [
        "Next, let’s apply machine learning and teach the machine to distinguish between the features that describe each of the two classes. There are a number of classification algorithms that you can use, let’s start with one of the most interpretable ones – an algorithm called **Naïve Bayes**. Don’t be misled by the word “Naïve” in its title, though: despite relative simplicity of the approach compared to other ones, this algorithm often works well in practice\n",
        "and sets a competitive performance baseline that is hard to beat with more sophisticated approaches.\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier, which means that it makes the class prediction based on the estimate of which outcome is most likely: i.e., it assesses the probability of an\n",
        "email being spam and compares it with the probability of it being ham, and then selects the outcome that is most probable between the two.\n",
        "\n",
        "In the previous step, you extracted the content of the email and converted it into a list of individual words (features). In this step, the machine will try to predict whether the email content represents spam or ham. \n",
        "\n",
        "In other words, it will try to predict whether the email is spam or ham given or conditioned on its content. This type of probability, when the outcome (class of “spam” or “ham”) depends on the condition (words used as features), is called conditional probability. \n",
        "\n",
        "For spam detection, you estimate `P(spam | email content)` and `P(ham | email content)`, or generally `P(outcome | (given) condition)`.Then you compare one estimate to another and return the most probable class.\n",
        "\n",
        "```python\n",
        "If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam\n",
        "If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "A machine can estimate the probability that an email is spam or ham conditioned on its content taking the number of times it has seen this content leading to a particular outcome.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are spam) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "\n",
        "P(ham | \"Participate in our lottery now!\") = (number of emails \"Participate in our lottery now!\" that are ham) / (total number of emails \"Participate in our lottery now!\", either spam or ham)\n",
        "```\n",
        "\n",
        "In the general form, this can be expressed as:\n",
        "\n",
        "```\n",
        "P(outcome | condition) = number_of_times(condition led to outcome) number_of_times(condition applied)\n",
        "```\n",
        "\n",
        "Remember that you used tokenization to split long texts into separate words to let the algorithm access the smaller bits of information – words rather than whole sequences. The idea of estimating probabilities based on separate features rather than based on the whole sequence of features (whole text) is somewhat similar.\n",
        "\n",
        "In the previous step, you converted this single text into a set of features as:\n",
        "\n",
        "```['participate': True, 'in': True, …, 'now': True, '!': True]``` \n",
        "\n",
        "Note that the conditional probabilities like:\n",
        "\n",
        "``` \n",
        "P(spam| \"Participate in our lottery now!\") and P(spam| ['participate': True,\n",
        "‘in’: True, …, ‘now’: True, ‘!’: True])\n",
        "```\n",
        "\n",
        "are the same because this set of features encodes the text.\n",
        "\n",
        "Is there a way to split this set to get at more fine-grained, individual probabilities, for example to establish a link between `[‘lottery’: True]` and the class of “spam”?\n",
        "\n",
        "Unfortunately, there is no way to split the conditional probability estimation like `P(outcome | conditions)` when there are multiple conditions specified, however it is possible to split the probability estimation like `P(outcomes | condition)` when there is a single condition and multiple outcomes.\n",
        "\n",
        "In spam detection, the class is a single value (it is “spam” or “ham”), while features are a set `([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’:\n",
        "True])`. If you can flip around the single value of class and the set of features in such a way that the class becomes the new condition and the features become the new outcomes, you can split the probability into smaller components and establish the link between individual features like `[‘lottery’: True]` and class values like “spam”.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "Luckily, there is a way to flip the outcomes (class) and conditions (features extracted from the content) around!\n",
        "\n",
        "Let’s look into the estimation of conditional probabilities again: you\n",
        "estimate the probability that the email is spam given that its content is “Participate in our new lottery now!” based on how often in the past an email with such content was spam. For that, you take the proportion of the times you have seen “Participate in our new lottery now!” in a spam email among the emails with this content.\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "Similarly to how you estimated the probabilities above, you need the proportion of times you have seen “Participate in our new lottery now!” in a spam email among all spam emails.\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" | spam) = P(\"Participate in our new lottery now!\" is used in a spam email) / P(an email is spam)\n",
        "```\n",
        "\n",
        "That is, every time you use conditional probabilities, you need to\n",
        "divide how likely it is that you see the condition and outcome together by how likely it is that you see the condition on its own – this is the bit after |.\n",
        "\n",
        "Now you can see that both Formulas 1 and 2 rely on how often you see particular content in an email of particular class. They share this bit, so you can use it to connect the two formulas. For instance, from Formula 2 you know that:\n",
        "\n",
        "```\n",
        "P(\"Participate in our new lottery now!\" is used in a spam email) = P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)\n",
        "```\n",
        "\n",
        "Now you can fit this into Formula 1:\n",
        "\n",
        "```\n",
        "P(spam | \"Participate in our new lottery now!\") = P(\"Participate in our new lottery now!\" is used in a spam email) / P(\"Participate in our new lottery now!\" is used in an email) = [P(\"Participate in our new lottery now!\" | spam) * P(an email is spam)] / P(\"Participate in our new lottery now!\" is used in an email)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "In the general form:\n",
        "\n",
        "```\n",
        "P(class | content) = P(content represents class) / P(content) = [P(content | class) * P(class)] / P(content)\n",
        "```\n",
        "\n",
        "In other words, you can express the probability of a class given email content via the probability of the content given the class.\n",
        "\n",
        "Now you can replace the conditional probability of `P(class | content)` with `P(content | class)`, e.g. whereas before you had to calculate `P(“spam” | “Participate in our new lottery now!”)` or equally `P(“spam” | [‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True])`, which is hard to do because you will often end up with too few examples of exactly the same email content or exactly the same combination of features, now you can estimate `P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”)` instead.\n",
        "\n",
        "But how does this solve the problem? Aren’t you still dealing with a long sequence of features?\n",
        "\n",
        "Here is where the “naïve” assumption in Naïve Bayes helps: it assumes that the features are independent of each other, or that your chances of seeing a word “lottery” in an email are independent of seeing a word “new” or any other word in this email before. So you can estimate the probability of the whole sequence of features given a class as a product of probabilities of each feature given this class.\n",
        "\n",
        "```\n",
        "P([‘participate’: True, ‘in’: True, …, ‘now’: True, ‘!’: True] | “spam”) = P(‘participate’: True | “spam”) * P(‘in’: True | “spam”) … * P(‘!’: True | “spam”)\n",
        "```\n",
        "\n",
        "If you express `[‘participate’: True]` as the first feature in the feature list, or `f1`, `[‘in’: True]` as `f2`, and so on, until `fn = [‘!’: True]`, you can use the general formula:\n",
        "\n",
        "```\n",
        "P([f1, f2, …, fn] | class) = P(f1 | class) * P(f2| class) … * P(fn| class)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/02-your-first-nlp-example/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "Now that you have broken down the probability of the whole feature list given class into the probabilities for each word given that class, how do you actually estimate them?\n",
        "\n",
        "Since for each email you note which words occur in it, the total number of times you can switch on the flag `[‘feature’: True]` equals the total number of emails in that class, while the actual number of times you switch on this flag is the number of emails where this feature is actually\n",
        "present. The conditional probability `P(feature | class)` is simply the proportion of the two:\n",
        "\n",
        "```\n",
        "P(feature | class) = number(emails in class with feature present) / total_number(emails in class)\n",
        "```\n",
        "\n",
        "These numbers are easy to estimate from the training data – let’s try to do that with an example.\n",
        "\n",
        ">Suppose you have 5 spam emails and 10 ham emails. What are the conditional probabilities for P('prescription':True | spam), P('meeting':True | ham), P('stock':True | spam) and P('stock':True | ham), if:\n",
        "- 2 spam emails contain word prescription\n",
        "- 1 spam email contains word stock\n",
        "- 3 ham emails contain word stock\n",
        "- 5 ham emails contain word meeting\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "The probabilities are simply:\n",
        "- P('prescription':True | spam) = number(spam emails with 'prescription')/number(spam emails) = 2/5 = 0.40\n",
        "- P('meeting':True | ham) = 5/10 = 0.50\n",
        "- P('stock':True | spam) = 1/5 = 0.20\n",
        "- P('stock':True | ham) = 3/10 = 0.30\n",
        "\n",
        "Let’s iterate through the classification steps again: during the training phase, the algorithm learns prior class probabilities (this is simply\n",
        "class distribution, e.g. `P(ham)=0.71 and P(spam)=0.29)` and probabilities for each feature given each of the classes (this is simply the proportion of emails with each feature in each class, e.g. `P(‘meeting’:True | ham) = 0.50)`. \n",
        "\n",
        "During test phase, or when the algorithm is applied to a new email and is asked to predict its class, the following comparison from the beginning of this section is applied:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(spam | content) > P(ham | content) \n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is what we started with originally, but we said that the conditions are flipped, so it becomes:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) / P(content) > P(content | ham) * P(ham) / P(content)\n",
        "\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "Note that we end up with `P(content)` in denominator on both sides of the expression, so the absolute value of this probability doesn’t matter and it can be removed from the expression altogether. So we can simplify the expression as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(content | spam) * P(spam) > P(content | ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "`P(spam)` and `P(ham)` are class probabilities estimated during training, and `P(content | class)`, using naïve independence assumption, are products of probabilities, so:\n",
        "\n",
        "```\n",
        "Predict “spam” if P([f1, f2, …, fn]| spam) * P(spam) > P([f1, f2, …, fn]| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "is split into the individual feature probabilities as:\n",
        "\n",
        "```\n",
        "Predict “spam” if P(f1 | spam) * P(f2| spam) … * P(fn| spam) * P(spam) > P(f1 | ham) * P(f2| ham) … * P(fn| ham) * P(ham)\n",
        "Predict “ham” otherwise\n",
        "```\n",
        "\n",
        "This is the final expression the classifier relies on. The following code implements this idea.\n",
        "Since Naïve Bayes is frequently used for NLP tasks, NLTK comes with its own\n",
        "implementation, too, and here you are going to use it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JgZtFT8MWL"
      },
      "source": [
        "def train(features, proportion):\n",
        "  train_size = int(len(features) * proportion)\n",
        "  # Use the first n% (according to the specified proportion) of emails with their features for training, and the rest for testing\n",
        "  train_set, test_set = features[: train_size], features[train_size:]\n",
        "  print(f\"Training set size = {str(len(train_set))} emails\")\n",
        "  print(f\"Test set size = {str(len(test_set))} emails\")\n",
        "\n",
        "  classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "  return train_set, test_set, classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk7Zr9Z2Lw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1608d0c4-1f22-4b31-9523-8eb243731231"
      },
      "source": [
        "# Apply the train function using 80% (or a similar proportion) of emails for training. \n",
        "train_set, test_set, classifier = train(all_features, 0.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size = 4137 emails\n",
            "Test set size = 1035 emails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwgAyTHA93N"
      },
      "source": [
        "##Step 5: Evaluate your classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ve6ZA8uA-nm"
      },
      "source": [
        "Finally, let’s evaluate how well the classifier performs in detecting whether an email is spam or ham. For that, let’s use the accuracy score returned by the NLTK’s classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcWqAwsl1iV"
      },
      "source": [
        "def evaluate(train_set, test_set, classifier):\n",
        "  print(f\"Accuracy on the training set = {str(classify.accuracy(classifier, train_set))}\")\n",
        "  print(f\"Accuracy of the test set = {str(classify.accuracy(classifier, test_set))}\")\n",
        "\n",
        "  # inspect the most informative features (words). You need to specify the number of the top most informative features to look into, e.g. 50 here\n",
        "  classifier.show_most_informative_features(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKdVa-gHmmuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1164ec-bf8b-4f34-f0c4-e9af243eac52"
      },
      "source": [
        "evaluate(train_set, test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.95987430505197\n",
            "Accuracy of the test set = 0.9497584541062802\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    204.3 : 1.0\n",
            "                    2004 = True             spam : ham    =    141.9 : 1.0\n",
            "                    2001 = True              ham : spam   =    130.8 : 1.0\n",
            "            prescription = True             spam : ham    =    127.8 : 1.0\n",
            "                     nom = True              ham : spam   =    125.5 : 1.0\n",
            "                    pain = True             spam : ham    =    107.4 : 1.0\n",
            "                     ect = True              ham : spam   =    106.9 : 1.0\n",
            "                    spam = True             spam : ham    =     90.1 : 1.0\n",
            "                  health = True             spam : ham    =     87.0 : 1.0\n",
            "                featured = True             spam : ham    =     74.5 : 1.0\n",
            "              nomination = True              ham : spam   =     73.9 : 1.0\n",
            "                  differ = True             spam : ham    =     71.3 : 1.0\n",
            "             medications = True             spam : ham    =     66.6 : 1.0\n",
            "                  weight = True             spam : ham    =     65.1 : 1.0\n",
            "             subscribers = True             spam : ham    =     65.1 : 1.0\n",
            "            solicitation = True             spam : ham    =     60.4 : 1.0\n",
            "                  shares = True             spam : ham    =     60.4 : 1.0\n",
            "                congress = True             spam : ham    =     58.8 : 1.0\n",
            "                   epson = True             spam : ham    =     58.8 : 1.0\n",
            "                      cc = True              ham : spam   =     57.5 : 1.0\n",
            "                inherent = True             spam : ham    =     57.2 : 1.0\n",
            "                     pro = True             spam : ham    =     55.7 : 1.0\n",
            "             legislation = True             spam : ham    =     55.7 : 1.0\n",
            "                conflict = True             spam : ham    =     55.7 : 1.0\n",
            "                   cheap = True             spam : ham    =     50.3 : 1.0\n",
            "                 doctors = True             spam : ham    =     49.4 : 1.0\n",
            "                 foresee = True             spam : ham    =     49.4 : 1.0\n",
            "                 dealers = True             spam : ham    =     49.4 : 1.0\n",
            "                thousand = True             spam : ham    =     48.4 : 1.0\n",
            "                    2005 = True             spam : ham    =     48.4 : 1.0\n",
            "                 advises = True             spam : ham    =     47.8 : 1.0\n",
            "                    draw = True             spam : ham    =     47.8 : 1.0\n",
            "               thousands = True             spam : ham    =     47.8 : 1.0\n",
            "                    anti = True             spam : ham    =     44.7 : 1.0\n",
            "                 beliefs = True             spam : ham    =     43.1 : 1.0\n",
            "                      ex = True             spam : ham    =     41.9 : 1.0\n",
            "                    lisa = True              ham : spam   =     41.5 : 1.0\n",
            "                   risks = True             spam : ham    =     40.9 : 1.0\n",
            "                   steve = True              ham : spam   =     40.4 : 1.0\n",
            "                   penis = True             spam : ham    =     40.0 : 1.0\n",
            "                      br = True             spam : ham    =     40.0 : 1.0\n",
            "                   susan = True              ham : spam   =     39.3 : 1.0\n",
            "                     ibm = True             spam : ham    =     39.0 : 1.0\n",
            "                  proven = True             spam : ham    =     38.4 : 1.0\n",
            "                   adobe = True             spam : ham    =     38.1 : 1.0\n",
            "                  farmer = True              ham : spam   =     38.0 : 1.0\n",
            "                       u = True             spam : ham    =     38.0 : 1.0\n",
            "                     fda = True             spam : ham    =     36.8 : 1.0\n",
            "                     sum = True             spam : ham    =     36.8 : 1.0\n",
            "                 explode = True             spam : ham    =     36.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b_hoGVnjS1"
      },
      "source": [
        "As you can see, many spam emails in this dataset are related to medications,\n",
        "which shows a particular bias – the most typical spam that you personally get might be on a different topic altogether! What effect might this mismatch between the training data from the publicly available dataset like Enron and your personal data have?\n",
        "\n",
        "One other piece of information presented in this output is accuracy. Test accuracy shows the proportion of test emails that are correctly classified by Naïve Bayes among all test emails.\n",
        "\n",
        "Note, that since the classifier is trained on the training data, it actually gets to “see” all the correct labels for the training examples. \n",
        "\n",
        "Shouldn’t it then know the correct answers and perform at 100% accuracy on the training data?\n",
        "\n",
        "Well, the point here is that the classifier doesn’t just\n",
        "retrieve the correct answers: during training it has built some probabilistic model (i.e., learned about the distribution of classes and the probability of different features), and then it applies this model to the data. So, it is actually very likely that the probabilistic model doesn’t capture all the things in the data 100% correctly.\n",
        "\n",
        "Therefore, when you run the code above, you will get accuracy on the training data of `96.13%`. This is not perfect (i.e., not `100%`) but very close to it! When you apply the same classifier to new data – the test set that the classifier hasn’t seen during training – the accuracy reflects its generalizing ability. That is, it shows whether the probabilistic assumptions it made based on the training data can be successfully applied to any other data. The accuracy on the test set is `94.20%`, which is slightly lower than that on the training set, but is also very high.\n",
        "\n",
        "Finally, if you’d like to gain any further insight into how the words are used in the emails from different classes, you can also check the occurrences of any particular word in all available contexts.\n",
        "\n",
        "For example, word “stocks” features as a very strong predictor of spam\n",
        "messages. Why is that? You might be thinking, “OK, some emails containing “stocks” will be spam, but surely there must be contexts where “stocks” is used in a completely harmless way?”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyH5N-6NqeWt"
      },
      "source": [
        "def concordance(data_list, search_word):\n",
        "  for email in data_list:\n",
        "    word_list = [word for word in word_tokenize(email.lower())]\n",
        "    text_list = Text(word_list)\n",
        "\n",
        "    \"\"\"\n",
        "    “Concordancer” is a tool that checks for the occurrences of the specified word and prints out the word in\n",
        "    its context. By default, NLTK’s concordancer prints out the search_word surrounded by the previous\n",
        "    36 and the following 36 characters – so note, that it doesn’t always result in full words\n",
        "    \"\"\"\n",
        "    if search_word in word_list:\n",
        "      text_list.concordance(search_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymrq-3u8rY7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b55d6bd-2264-493e-e0d7-4d6400c949e8"
      },
      "source": [
        "# Apply this function to two lists – ham_list and spam_list – to find out about the different contexts of use for the word “stocks”\n",
        "print(\"STOCKS in HAM:\")\n",
        "concordance(ham_list, \"stocks\")\n",
        "print(\"\\n\\nSTOCKS in SPAM:\")\n",
        "concordance(spam_list, \"stocks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOCKS in HAM:\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ad my portfolio is diversified into stocks that have lost even more money than\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "\n",
            "\n",
            "STOCKS in SPAM:\n",
            "Displaying 5 of 5 matches:\n",
            "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
            "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 4 of 4 matches:\n",
            "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
            "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
            "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
            "om third party . investing in penny stocks is high risk and you should seek pr\n",
            "Displaying 4 of 4 matches:\n",
            " the last 12 months , many of these stocks made triple and even quadruple retu\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 3 of 3 matches:\n",
            "torage inc. play of the week tracks stocks on downward trends , foresees botto\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
            "Displaying 4 of 4 matches:\n",
            "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
            " statements . as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
            "Displaying 4 of 4 matches:\n",
            "                                    stocks alert newsletter must read - alert \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
            "Displaying 1 of 1 matches:\n",
            "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks for immediate breakout erhc and exx\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 6 of 6 matches:\n",
            " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
            " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
            " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 4 of 4 matches:\n",
            "watch this one trade . these little stocks can surprise in a big way sometimes\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 5 of 5 matches:\n",
            "ck monday some of these littie voip stocks have been really moving lately . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "or information puposes only . penny stocks are considered highly speculative a\n",
            "Displaying 4 of 4 matches:\n",
            "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "biication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 6 of 6 matches:\n",
            "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
            "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
            "ynamic things . some of these small stocks have absolutely exploded in price r\n",
            "'' occur . as with many micro - cap stocks , today ' s company has additional \n",
            " ema - il pertaining to investing , stocks or securities must be understood as\n",
            "ntative before deciding to trade in stocks featured within this ema - il . non\n",
            "Displaying 3 of 3 matches:\n",
            "                                    stocks newsletter first we would like to s\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 1 of 1 matches:\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 1 of 1 matches:\n",
            "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
            "Displaying 3 of 3 matches:\n",
            " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
            "st levels of the year , with energy stocks outperforming all other market sect\n",
            "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 4 of 4 matches:\n",
            "y agree , some , not all , of these stocks move in price because they are prom\n",
            "tands or that as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been realiy moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
            "Displaying 2 of 2 matches:\n",
            "                                    stocks are about timing nomad internationa\n",
            " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
            "Displaying 4 of 4 matches:\n",
            "                                    stocks newsletter u r g e n t i n v e s t \n",
            "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks traders ' monthly alert january pic\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . if you wish to stop future mailin\n",
            "Displaying 5 of 5 matches:\n",
            "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            "                                    stocks available . vlagr @ . x _ a _ nax .\n",
            "Displaying 1 of 1 matches:\n",
            "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "in apple investments , inc profiled stocks . in order to be in full compliance\n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 2 of 2 matches:\n",
            "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
            "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
            "Displaying 2 of 2 matches:\n",
            "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 2 of 2 matches:\n",
            "rt identifying defense and security stocks ready to explode look at the moves \n",
            " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
            "Displaying 2 of 2 matches:\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "ancements but may be one of the few stocks left in this industry group that is\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been really moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--qXDNH9sHZv"
      },
      "source": [
        "If you run this code and print out the contexts for “stocks”, you will find out that “stocks” feature in only 4 ham contexts (e.g., an email reminder “Follow your stocks and news headlines”) as compared to hundreds of spam contexts including “Stocks to play”, “Big money was made in these stocks”, “Select gold mining stocks”, “Little stocks can mean big gains for you”, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz0WQV_U2fwf"
      },
      "source": [
        "##Deploying your spam filter in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u9QgqEh22Vj"
      },
      "source": [
        "For instance, the classifier that you’ve built performs at 94% accuracy, so\n",
        "you can expect it to classify real emails into spam and ham quite accurately. It’s time to deploy it in practice then. When you run it on some new emails (perhaps, some from your own inbox) you need to perform the same steps on these emails as before, that is:\n",
        "\n",
        "- you need to read them in, then\n",
        "- you need to extract the features from these emails, and finally\n",
        "- you need to apply the classifier that you trained before on these emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-cCEfxsv5oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b19b671-1a4b-45e6-9fdc-c6b1fb233905"
      },
      "source": [
        "# Feel free to provide your own examples\n",
        "test_spam_list = [\n",
        "  \"Participate in our new lottery!\",\n",
        "  \"Try out this new medicine\"\n",
        "]\n",
        "test_ham_list = [\n",
        "  \"See the minutes from the last meeting attached\", \n",
        "  \"Investors are coming to our office on Monday\"\n",
        "]\n",
        "\n",
        "# Read the emails extracting their textual content and keeping the labels for further evaluation\n",
        "test_emails = [(email_content, \"spam\") for email_content in test_spam_list]\n",
        "test_emails += [(email_content, \"ham\") for email_content in test_ham_list]\n",
        "\n",
        "# Extract the features\n",
        "new_test_set = [(get_features(email), label) for (email, label) in test_emails]\n",
        "\n",
        "# Apply the trained classifier and evaluate its performance\n",
        "evaluate(train_set, new_test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.95987430505197\n",
            "Accuracy of the test set = 1.0\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    204.3 : 1.0\n",
            "                    2004 = True             spam : ham    =    141.9 : 1.0\n",
            "                    2001 = True              ham : spam   =    130.8 : 1.0\n",
            "            prescription = True             spam : ham    =    127.8 : 1.0\n",
            "                     nom = True              ham : spam   =    125.5 : 1.0\n",
            "                    pain = True             spam : ham    =    107.4 : 1.0\n",
            "                     ect = True              ham : spam   =    106.9 : 1.0\n",
            "                    spam = True             spam : ham    =     90.1 : 1.0\n",
            "                  health = True             spam : ham    =     87.0 : 1.0\n",
            "                featured = True             spam : ham    =     74.5 : 1.0\n",
            "              nomination = True              ham : spam   =     73.9 : 1.0\n",
            "                  differ = True             spam : ham    =     71.3 : 1.0\n",
            "             medications = True             spam : ham    =     66.6 : 1.0\n",
            "                  weight = True             spam : ham    =     65.1 : 1.0\n",
            "             subscribers = True             spam : ham    =     65.1 : 1.0\n",
            "            solicitation = True             spam : ham    =     60.4 : 1.0\n",
            "                  shares = True             spam : ham    =     60.4 : 1.0\n",
            "                congress = True             spam : ham    =     58.8 : 1.0\n",
            "                   epson = True             spam : ham    =     58.8 : 1.0\n",
            "                      cc = True              ham : spam   =     57.5 : 1.0\n",
            "                inherent = True             spam : ham    =     57.2 : 1.0\n",
            "                     pro = True             spam : ham    =     55.7 : 1.0\n",
            "             legislation = True             spam : ham    =     55.7 : 1.0\n",
            "                conflict = True             spam : ham    =     55.7 : 1.0\n",
            "                   cheap = True             spam : ham    =     50.3 : 1.0\n",
            "                 doctors = True             spam : ham    =     49.4 : 1.0\n",
            "                 foresee = True             spam : ham    =     49.4 : 1.0\n",
            "                 dealers = True             spam : ham    =     49.4 : 1.0\n",
            "                thousand = True             spam : ham    =     48.4 : 1.0\n",
            "                    2005 = True             spam : ham    =     48.4 : 1.0\n",
            "                 advises = True             spam : ham    =     47.8 : 1.0\n",
            "                    draw = True             spam : ham    =     47.8 : 1.0\n",
            "               thousands = True             spam : ham    =     47.8 : 1.0\n",
            "                    anti = True             spam : ham    =     44.7 : 1.0\n",
            "                 beliefs = True             spam : ham    =     43.1 : 1.0\n",
            "                      ex = True             spam : ham    =     41.9 : 1.0\n",
            "                    lisa = True              ham : spam   =     41.5 : 1.0\n",
            "                   risks = True             spam : ham    =     40.9 : 1.0\n",
            "                   steve = True              ham : spam   =     40.4 : 1.0\n",
            "                   penis = True             spam : ham    =     40.0 : 1.0\n",
            "                      br = True             spam : ham    =     40.0 : 1.0\n",
            "                   susan = True              ham : spam   =     39.3 : 1.0\n",
            "                     ibm = True             spam : ham    =     39.0 : 1.0\n",
            "                  proven = True             spam : ham    =     38.4 : 1.0\n",
            "                   adobe = True             spam : ham    =     38.1 : 1.0\n",
            "                  farmer = True              ham : spam   =     38.0 : 1.0\n",
            "                       u = True             spam : ham    =     38.0 : 1.0\n",
            "                     fda = True             spam : ham    =     36.8 : 1.0\n",
            "                     sum = True             spam : ham    =     36.8 : 1.0\n",
            "                 explode = True             spam : ham    =     36.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChhXSUUsxT0x"
      },
      "source": [
        "The classifier that you’ve trained performs with 100% accuracy on these examples. Good! How can you print out the predicted label for each particular email though?\n",
        "\n",
        "For that, you simply extract the features from the email content and print out the label, i.e. you don’t need to run the full evaluation with the accuracy calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2T18c4TxX6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d24ec3-db10-4e73-b5cf-5a3768a7ec0d"
      },
      "source": [
        "for email in test_spam_list:\n",
        "  print(email)\n",
        "  print(classifier.classify(get_features(email)))\n",
        "\n",
        "for email in test_ham_list:\n",
        "  print(email)\n",
        "  print(classifier.classify(get_features(email)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Participate in our new lottery!\n",
            "spam\n",
            "Try out this new medicine\n",
            "spam\n",
            "See the minutes from the last meeting attached\n",
            "ham\n",
            "Investors are coming to our office on Monday\n",
            "ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QYrJ0Gx55yj"
      },
      "source": [
        "Let’s summarize what you have covered.\n",
        "\n",
        "You have learned how build a classifier in five steps:\n",
        "\n",
        "1. the emails should be read, and the two classes should be clearly defined for the machine to learn from.\n",
        "2. the text content should be extracted.\n",
        "3. then the content should be converted into features.\n",
        "4. the classifier should be trained on the training set of the data.\n",
        "5. finally, the classifier should be evaluated on the test set\n",
        "\n",
        "There are a number of machine learning classifiers, and you’ve applied one of the most interpretable of them – Naïve Bayes. Naïve Bayes is a probabilistic\n",
        "classifier: it assumes that the data in two classes is generated by different probability distributions, which are learned from the training data. Despite its simplicity and “naïve” feature independence assumption, Naïve Bayes often performs well in practice, and sets competitive baseline for other more sophisticated algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wh2UkX7_mys"
      },
      "source": [
        "##Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1v2Ov9_n_R"
      },
      "source": [
        "Apply the trained classifier to a different dataset, for example to `enron2/spam` and ham emails that originate with a different owner (check `Summary.txt` for more information). For that you need to:\n",
        "\n",
        "- read the data from the `spam/` and `ham/` subfolders in `enron2/`\n",
        "- extract the textual content and convert it into features\n",
        "- evaluate the classifier\n",
        "\n",
        "What do the results suggest? \n",
        "\n",
        "Hint: one man’s spam may be another man’s ham. If you are not satisfied with the results, try combining the data from the two owners in one dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pMDWBPQCnbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be34e2d-b575-4342-c640-40e5f6f84249"
      },
      "source": [
        "test_spam_list = read_files(\"enron2/spam/\")\n",
        "print(len(test_spam_list))\n",
        "print(test_spam_list[0])\n",
        "\n",
        "test_ham_list = read_files(\"enron2/ham/\")\n",
        "print(len(test_ham_list))\n",
        "print(test_ham_list[0])\n",
        "\n",
        "test_emails = [(email_content, \"spam\") for email_content in test_spam_list]\n",
        "test_emails += [(email_content, \"ham\") for email_content in test_ham_list]\n",
        "\n",
        "random.shuffle(test_emails)\n",
        "\n",
        "new_test_set = [(get_features(email_content), label) for email_content, label in test_emails]\n",
        "\n",
        "evaluate(train_set, new_test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1496\n",
            "Subject: help\r\n",
            "Television in 1919 by seat to my knoweledge. Chrono cross in 1969\n",
            "4361\n",
            "Subject: re: eol\r\n",
            "Clayton,\r\n",
            "Great news. I would like to sit down with you, tom and stinson and review\r\n",
            "Where\r\n",
            "We are with this project. Also, I would like to talk to you about your\r\n",
            "Status (finalizing\r\n",
            "The transfer to another group).\r\n",
            "Vince\r\n",
            "Clayton vernon@ enron\r\n",
            "01/18/2001 03: 21 pm\r\n",
            "To: vasant shanbhogue/hou/ect@ ect\r\n",
            "Cc: stinson gibner/hou/ect@ ect, vince j kaminski/hou/ect@ ect\r\n",
            "Subject: eol\r\n",
            "Vasant -\r\n",
            "Dave delaney called an hour ago. He needed a statistic from eol that the eol\r\n",
            "Folks couldn' t give him (it seems they had a database problem in 1999), and\r\n",
            "The grapevine had it we had the data. Tom barkley was able to give him the\r\n",
            "Data he needed for his presentation, within a matter of 10 minutes or so.\r\n",
            "Clayton\n",
            "Accuracy on the training set = 0.95987430505197\n",
            "Accuracy of the test set = 0.7611405156223322\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    204.3 : 1.0\n",
            "                    2004 = True             spam : ham    =    141.9 : 1.0\n",
            "                    2001 = True              ham : spam   =    130.8 : 1.0\n",
            "            prescription = True             spam : ham    =    127.8 : 1.0\n",
            "                     nom = True              ham : spam   =    125.5 : 1.0\n",
            "                    pain = True             spam : ham    =    107.4 : 1.0\n",
            "                     ect = True              ham : spam   =    106.9 : 1.0\n",
            "                    spam = True             spam : ham    =     90.1 : 1.0\n",
            "                  health = True             spam : ham    =     87.0 : 1.0\n",
            "                featured = True             spam : ham    =     74.5 : 1.0\n",
            "              nomination = True              ham : spam   =     73.9 : 1.0\n",
            "                  differ = True             spam : ham    =     71.3 : 1.0\n",
            "             medications = True             spam : ham    =     66.6 : 1.0\n",
            "                  weight = True             spam : ham    =     65.1 : 1.0\n",
            "             subscribers = True             spam : ham    =     65.1 : 1.0\n",
            "            solicitation = True             spam : ham    =     60.4 : 1.0\n",
            "                  shares = True             spam : ham    =     60.4 : 1.0\n",
            "                congress = True             spam : ham    =     58.8 : 1.0\n",
            "                   epson = True             spam : ham    =     58.8 : 1.0\n",
            "                      cc = True              ham : spam   =     57.5 : 1.0\n",
            "                inherent = True             spam : ham    =     57.2 : 1.0\n",
            "                     pro = True             spam : ham    =     55.7 : 1.0\n",
            "             legislation = True             spam : ham    =     55.7 : 1.0\n",
            "                conflict = True             spam : ham    =     55.7 : 1.0\n",
            "                   cheap = True             spam : ham    =     50.3 : 1.0\n",
            "                 doctors = True             spam : ham    =     49.4 : 1.0\n",
            "                 foresee = True             spam : ham    =     49.4 : 1.0\n",
            "                 dealers = True             spam : ham    =     49.4 : 1.0\n",
            "                thousand = True             spam : ham    =     48.4 : 1.0\n",
            "                    2005 = True             spam : ham    =     48.4 : 1.0\n",
            "                 advises = True             spam : ham    =     47.8 : 1.0\n",
            "                    draw = True             spam : ham    =     47.8 : 1.0\n",
            "               thousands = True             spam : ham    =     47.8 : 1.0\n",
            "                    anti = True             spam : ham    =     44.7 : 1.0\n",
            "                 beliefs = True             spam : ham    =     43.1 : 1.0\n",
            "                      ex = True             spam : ham    =     41.9 : 1.0\n",
            "                    lisa = True              ham : spam   =     41.5 : 1.0\n",
            "                   risks = True             spam : ham    =     40.9 : 1.0\n",
            "                   steve = True              ham : spam   =     40.4 : 1.0\n",
            "                   penis = True             spam : ham    =     40.0 : 1.0\n",
            "                      br = True             spam : ham    =     40.0 : 1.0\n",
            "                   susan = True              ham : spam   =     39.3 : 1.0\n",
            "                     ibm = True             spam : ham    =     39.0 : 1.0\n",
            "                  proven = True             spam : ham    =     38.4 : 1.0\n",
            "                   adobe = True             spam : ham    =     38.1 : 1.0\n",
            "                  farmer = True              ham : spam   =     38.0 : 1.0\n",
            "                       u = True             spam : ham    =     38.0 : 1.0\n",
            "                     fda = True             spam : ham    =     36.8 : 1.0\n",
            "                     sum = True             spam : ham    =     36.8 : 1.0\n",
            "                 explode = True             spam : ham    =     36.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6hkaZgTDu5H"
      },
      "source": [
        "Now, we will combine the two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agOxYfS6D0O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092877c9-e590-4145-ba2d-0c16ffbcdf99"
      },
      "source": [
        "spam_list = read_files(\"enron1/spam/\") + read_files(\"enron2/spam/\")\n",
        "print(len(spam_list))\n",
        "\n",
        "ham_list = read_files(\"enron1/ham/\") + read_files(\"enron2/ham/\")\n",
        "print(len(ham_list))\n",
        "\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "\n",
        "random.shuffle(test_emails)\n",
        "\n",
        "all_features = [(get_features(email_content), label) for email_content, label in all_emails]\n",
        "print(len(all_features))\n",
        "\n",
        "train_set, test_set, classifier = train(all_features, 0.8)\n",
        "evaluate(train_set, new_test_set, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2996\n",
            "8033\n",
            "11029\n",
            "Training set size = 8823 emails\n",
            "Test set size = 2206 emails\n",
            "Accuracy on the training set = 0.9818655786013828\n",
            "Accuracy of the test set = 0.9820727334813044\n",
            "Most Informative Features\n",
            "                   meter = True              ham : spam   =    264.5 : 1.0\n",
            "                   vince = True              ham : spam   =    200.0 : 1.0\n",
            "                     nom = True              ham : spam   =    195.6 : 1.0\n",
            "                     sex = True             spam : ham    =    195.1 : 1.0\n",
            "            prescription = True             spam : ham    =    169.2 : 1.0\n",
            "                     ect = True              ham : spam   =    167.7 : 1.0\n",
            "                    spam = True             spam : ham    =    145.8 : 1.0\n",
            "               forwarded = True              ham : spam   =    137.7 : 1.0\n",
            "                     fyi = True              ham : spam   =    137.0 : 1.0\n",
            "                    2005 = True             spam : ham    =    128.1 : 1.0\n",
            "                   logos = True             spam : ham    =    121.2 : 1.0\n",
            "                     oem = True             spam : ham    =    113.4 : 1.0\n",
            "              nomination = True              ham : spam   =    112.6 : 1.0\n",
            "                    pain = True             spam : ham    =    109.5 : 1.0\n",
            "                    2004 = True             spam : ham    =    105.3 : 1.0\n",
            "                   corel = True             spam : ham    =    104.4 : 1.0\n",
            "                  dealer = True             spam : ham    =    104.4 : 1.0\n",
            "                  weight = True             spam : ham    =     97.9 : 1.0\n",
            "                      cc = True              ham : spam   =     89.0 : 1.0\n",
            "                     pat = True              ham : spam   =     89.0 : 1.0\n",
            "                    1933 = True             spam : ham    =     88.8 : 1.0\n",
            "                     853 = True              ham : spam   =     83.8 : 1.0\n",
            "                     713 = True              ham : spam   =     83.4 : 1.0\n",
            "              materially = True             spam : ham    =     79.7 : 1.0\n",
            "             medications = True             spam : ham    =     78.4 : 1.0\n",
            "             predictions = True             spam : ham    =     77.1 : 1.0\n",
            "                identity = True             spam : ham    =     75.8 : 1.0\n",
            "                   plain = True             spam : ham    =     74.5 : 1.0\n",
            "                 shirley = True              ham : spam   =     73.7 : 1.0\n",
            "                  secret = True             spam : ham    =     73.2 : 1.0\n",
            "                    8859 = True             spam : ham    =     72.0 : 1.0\n",
            "                   boost = True             spam : ham    =     70.4 : 1.0\n",
            "                   penis = True             spam : ham    =     69.4 : 1.0\n",
            "                      wi = True             spam : ham    =     69.4 : 1.0\n",
            "                 beliefs = True             spam : ham    =     68.1 : 1.0\n",
            "                 artwork = True             spam : ham    =     68.1 : 1.0\n",
            "                 foresee = True             spam : ham    =     66.8 : 1.0\n",
            "                  studio = True             spam : ham    =     65.0 : 1.0\n",
            "                   epson = True             spam : ham    =     61.6 : 1.0\n",
            "           advertisement = True             spam : ham    =     59.2 : 1.0\n",
            "               satisfied = True             spam : ham    =     57.7 : 1.0\n",
            "                  africa = True             spam : ham    =     56.4 : 1.0\n",
            "                   adult = True             spam : ham    =     55.1 : 1.0\n",
            "                 explode = True             spam : ham    =     55.1 : 1.0\n",
            "         customerservice = True             spam : ham    =     53.8 : 1.0\n",
            "              scheduling = True              ham : spam   =     53.7 : 1.0\n",
            "                     apc = True             spam : ham    =     52.5 : 1.0\n",
            "                   shops = True             spam : ham    =     51.2 : 1.0\n",
            "                  norton = True             spam : ham    =     48.6 : 1.0\n",
            "                  darren = True              ham : spam   =     48.5 : 1.0\n"
          ]
        }
      ]
    }
  ]
}