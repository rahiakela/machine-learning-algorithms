{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction-to-information-search.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBGb6CvYLG2CUuU7c3jRQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/introduction_to_information_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlSdPejwIUX"
      },
      "source": [
        "##Introduction to Information Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_VbEjXwQ1H"
      },
      "source": [
        "You might have come across the term Information Retrieval in the context of search engines: for example, Google famously started its business by providing a powerful search algorithm that kept improving over time. The search for information, however, is a basic need that you may face not only in the context of searching online: for instance, every time you search for the files on your computer, you also perform sort of information retrieval. In fact, the task predates digital era: before computers and Internet became a commodity, one\n",
        "had to manually wade through paper copies of encyclopedias, books, documents, files and so on. Thanks to the technology, the algorithms these days help you do many of these tasks automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw4HFbozDIH"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0b1JWPZzEQY"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, WordNetLemmatizer, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.text import Text\n",
        "\n",
        "from operator import itemgetter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQDUMiQITbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86242d6e-ab38-42ac-835c-f32df72e1153"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRXbiApBXvX",
        "outputId": "93f1e630-2cb6-49bf-a2e8-43b78588439c"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -qq https://github.com/ekochmar/Essential-NLP/raw/master/cisi.zip\n",
        "\n",
        "unzip -qq cisi.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBTLl031VAJ"
      },
      "source": [
        "##Step 1: Understanding the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKg-cGuB1XwK"
      },
      "source": [
        "---\n",
        "\n",
        "**Scenario 1**:\n",
        "\n",
        "Imagine that you have to perform the search in a collection of documents yourself, i.e. without the help of the machine. For example, you have a thousand printed out notes and minutes related to the meetings at work, and you\n",
        "only need those that discuss the management meetings. How will you find all such documents? How will you identify the most relevant of these?\n",
        "\n",
        "---\n",
        "\n",
        "if you were tasked with this in actual life, you would go through the\n",
        "documents one by one, identifying those that contain the key words (like `management` and `meetings`) and split all the documents into two piles: e.g. those documents that you should keep and look into further and those that you can discard because they do not answer your information need in learning more about the management meetings. This task is akin to filtering.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "Now, there are a couple of points that we did not get to discuss before: imagine there are a hundred of documents in total and you can quickly skim through them to filter out the most irrelevant ones – those that do not even mention either “meetings” or “management”.\n",
        "\n",
        "Luckily, these days we have computers and most documents are stored electronically. Computers can really help us speed the things up here.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Scenario 2** (based on Scenario 1, but more technical!):\n",
        "\n",
        "Imagine that you have to perform the search in a collection of documents, this time with the help of the machine. For\n",
        "example, you have a thousand notes and minutes related to the meetings at work stored in electronic format, and\n",
        "you only need those that discuss the management meetings.\n",
        "- First, how will you find all such documents? In other words, how can you code the search algorithm and what\n",
        "characteristics of the documents should the search be based on?\n",
        "- Second, how will you identify the most relevant of these documents? In other words, how can you implement a\n",
        "sorting algorithm to sort the documents in order of decreasing relevance?\n",
        "\n",
        "---\n",
        "\n",
        "It allows you to leverage the computational power of the machine, but the drill is the same as before: get the machine to identify the texts that have the keywords in them, and then sort the “keep” pile according to the relevance of the texts, starting with the most relevant for the user or yourself to look at.\n",
        "\n",
        "Despite us saying just now that the procedure is similar to how the humans perform the task (as in Scenario 1), there are actually some steps involved in getting the machine identify the documents with the keywords in them and sorting by relevance that we are not explicitly mentioning here. \n",
        "\n",
        "For instance, we humans have the following abilities that we naturally possess but machines naturally lack:\n",
        "\n",
        "- We know what represents a word, while a machine gets in a sequence of symbols and does not, by itself, have a notion of what a “word” is.\n",
        "- We know which words are keywords: e.g., if we are interested in finding the\n",
        "documents on management meetings, we will consider those containing “meeting”\n",
        "and “management”, but also those containing “meetings” and potentially even\n",
        "“manager” and “managerial”. The machine, on the other hand, does not know that\n",
        "these words are related, similar, or basically different forms of the same word.\n",
        "- We have an ability to focus on what matters: in fact, when reading texts we usually skim through them rather than pay equal attention to each word. For instance, when\n",
        "reading a sentence “Last Friday the management committee had a meeting”, which\n",
        "words do you pay more attention to? Which ones express the key idea of this\n",
        "message? Think about it – and we will return to this question later. The machines, on the other hand, should be specifically “told” which words matter more.\n",
        "- Finally, we also intuitively know how to judge what is more relevant. The machines can make relevance judgments, too, but unlike us humans they need to be “told” how to measure relevance in precise numbers.\n",
        "\n",
        "That, in a nutshell, represents the basic steps in the search algorithm.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "In this notebook, you will learn about other NLP techniques to preselect words, map the different forms of the same word to each other and weigh the words according to how much information they contribute to the task. Then you will build an information search algorithm that for any query (for example, “management meetings”) will find the most relevant documents in the collection of documents (for example, all minutes of the past managerial meetings sorted by their relevance).\n",
        "\n",
        "Suppose you have built such an application following all the steps. You type in a query and the algorithm returns a document or several documents that are supposedly relevant to this query. How can you tell whether the algorithm has picked out the right documents?\n",
        "\n",
        "Let’s use a dataset of documents and queries, where the documents are labeled with respect to their relevance to the queries. You will use this dataset as your gold standard, and before using the information search algorithm in practice, evaluate its performance against the ground truth labels in the labeled dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Kxz2AhWZWL"
      },
      "source": [
        "###Data and data structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FalL8sCWaUr"
      },
      "source": [
        "You are going to use a publicly available dataset labeled for the task. That\n",
        "means, a dataset with a number of documents and various queries, and a labeled list specifying which queries correspond to which documents. Once you implement and evaluate a search algorithm on such data labeled with ground truth, you can apply it to your own documents in your own projects.\n",
        "\n",
        "You will use the dataset collected by the Centre for Inventions and Scientific Information (CISI), which contains abstracts and some additional metadata on the journal articles on information systems and information retrieval.\n",
        "\n",
        "You will need to keep precisely three data structures for this application: \n",
        "- one for the documents, \n",
        "- another one for the queries, and \n",
        "- the third one matching the queries to the documents\n",
        "\n",
        "Information search is based on the idea that the content of a document or set of documents is relevant given the content of a particular query, so both documents and queries data structures should keep the contents of all documents and all queries. \n",
        "\n",
        "What would be the best way to keep track of which content represents which document?\n",
        "\n",
        "The most informative and useful way would be to assign a unique identifier – an index – to each document and each query. You can imagine, for example, storing content of the documents and queries in two separate tables, with each row representing a single document or query, and row numbers corresponding to the documents and queries ids. In Python, tables can be represented with dictionaries.\n",
        "\n",
        "Now, if you keep two Python dictionaries (tables) matching each unique document\n",
        "identifier (called key) to the document’s content (called value) in documents dictionary and matching each unique query identifier to the query’s content in queries dictionary, how should you represent the relevance mappings? \n",
        "\n",
        "You can use a dictionary structure again: this time, the keys will contain the queries ids, while the values should keep the matching documents ids. Since each query may correspond to multiple documents, it would be best to\n",
        "keep the ids of the matching documents as lists.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "As this figure shows, query with id 1 matches documents with ids `1` and `1460`, therefore the mappings data structure keeps a list of `[1, 1460]` for query `1`; similarly it keeps `[3]` for query `2`, `[2]` for query `112`, and an empty list for query `3`, because in this example there are no\n",
        "documents relevant for this query.\n",
        "\n",
        "Now let’s look into the CISI dataset and code the data reading and initialization step. All documents are stored in a single text file CISI.ALL. It has a peculiar format: it keeps the abstract of each article and some additional information, such as the index in the set, the title, authors’ list and cross-references – a list of indexes for the articles that cite each other.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "For the information search application, arguably the most useful information is the content of the abstract: abstracts in the articles typically serve as a concise summary of what the article presents, something akin to a snippet.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "As you can see, the field identifiers such as .A or .W are separated from the actual text by new line. In addition, the text within each field, for example, the abstract may be spread across multiple lines. Ideally, we would like to convert this format into something like text.\n",
        "\n",
        "Note that for the text that falls within the same field, e.g. .W, the line breaks (“\\n”) are replaced with whitespaces, so each line now starts with a field identifier followed by the field content:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "The format is much easier to work with: you can now read the text line by line,\n",
        "extract the unique identifier for the article from the field .I, merge the content of the fields `.T`, `.A` and `.W`, and store the result in the documents dictionary as `{1: \"18 Editions of the … this country and abroad.\"}`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyB2rC7cAXv0"
      },
      "source": [
        "def read_documents():\n",
        "  file = open(\"cisi/CISI.ALL\")\n",
        "  merged = \"\"\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    # Unless a string starts with a new field identifier, add the content to the current field separating the content from the previous line with a whitespace; \n",
        "    #votherwise, start a new line with the next identifier and field.\n",
        "    if a_line.startswith(\".\"):\n",
        "      merged += \"\\n\" + a_line.strip()\n",
        "    else:\n",
        "      merged += \" \" + a_line.strip()\n",
        "\n",
        "    documents = {}\n",
        "    content = \"\"\n",
        "    doc_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "      if a_line.startswith(\".I\"):\n",
        "        doc_id = a_line.split(\" \")[1].strip()  # doc_id can be extracted from the line with the .I field identifier\n",
        "      elif a_line.startswith(\".X\"):\n",
        "        documents[doc_id] = content\n",
        "        content = \"\"\n",
        "        doc_id = \"\"\n",
        "      else:\n",
        "        content += a_line.strip()[3:] + \" \"  # Otherwise, keep extracting the content from other fields (.T, .A and .W) removing the field identifiers themselves\n",
        "\n",
        "  file.close()   \n",
        "  return documents"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACa4lfWR70RR"
      },
      "source": [
        "As a sanity check, print out the size of the dictionary (make sure it contains all 1460 articles) and print out the content of the very first article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IE3w3wF560v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d0b0f0-2f62-40c6-be87-8fdf21ddaa7f"
      },
      "source": [
        "documents = read_documents()\n",
        "print(len(documents))\n",
        "print(documents.get(\"1\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBJFb2GDOBS"
      },
      "source": [
        "The queries are stored in `CISI.QRY` file and follow a very similar format: half the time, you see only two fields – `.I` for the unique identifier and `.W` for the content of the query. \n",
        "\n",
        "Other queries though are formulated not as questions but rather as abstracts from other articles. In such cases, the query also has an `.A` field for the authors’ list, `.T` for the title and `.B` field, which keeps the reference to the original journal in which the abstract was published.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "We are going to only focus on the unique identifiers and the content of the query itself (fields `.W` and `.T`, where available), so the code below is quite similar to the above as it allows you to populate the queries dictionary with data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8u2XdijC_8n"
      },
      "source": [
        "def read_queries():\n",
        "  file = open(\"cisi/CISI.QRY\")\n",
        "  merged = \"\"\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    # Unless a string starts with a new field identifier, add the content to the current field separating the content from the previous line with a whitespace; \n",
        "    #votherwise, start a new line with the next identifier and field.\n",
        "    if a_line.startswith(\".\"):\n",
        "      merged += \"\\n\" + a_line.strip()\n",
        "    else:\n",
        "      merged += \" \" + a_line.strip()\n",
        "\n",
        "    queries = {}\n",
        "    content = \"\"\n",
        "    query_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "      if a_line.startswith(\".I\"):\n",
        "        if not content == \"\":\n",
        "          queries[query_id] = content\n",
        "          content = \"\"\n",
        "          query_id = \"\"\n",
        "        query_id = a_line.split(\" \")[1].strip()  # query_id can be extracted from the line with the .I field identifier\n",
        "      elif a_line.startswith(\".W\"):\n",
        "        content += a_line.strip()[3:] + \" \"  # Otherwise, keep adding content to the content variable\n",
        "\n",
        "    # The very last query is not followed by any next .I field, so the strategy from above won’t work –\n",
        "    # you need to add the entry for the last query to the dictionary using this extra step\n",
        "    queries[query_id] = content\n",
        "\n",
        "  file.close()   \n",
        "  return queries"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gayZGk7w-SA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a622d81-d4ca-4545-e1c9-593e1b8f39d7"
      },
      "source": [
        "queries = read_queries()\n",
        "\n",
        "# Print out the length of the dictionary (it should contain 112 entries), and the content of the very first query\n",
        "print(len(queries))\n",
        "print(queries.get(\"1\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112\n",
            "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBSBQjjlD_GZ"
      },
      "source": [
        "Finally, let's read in the mapping between the queries and the documents – we'll keep these in the mappings data structure – with tuples where each query index (key) corresponds to the list of one or more document indices (value):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjDZk7qDw6v"
      },
      "source": [
        "def read_mappings():\n",
        "  file = open(\"cisi/CISI.REL\")\n",
        "\n",
        "  mappings = {}\n",
        "\n",
        "  for a_line in file.readlines():\n",
        "    voc = a_line.strip().split()\n",
        "    key = voc[0].strip()\n",
        "    current_value = voc[1].strip()  # The key (query id) is stored in the first column, while the document id is stored in the second column\n",
        "    value = []\n",
        "    \"\"\"\n",
        "    If the mappings dictionary already contains some document ids for the documents matching the given\n",
        "    query, you need to update the existing list with the current value; otherwise just add current value to the new list\n",
        "    \"\"\"\n",
        "    if key in mappings.keys():\n",
        "      value = mappings.get(key)\n",
        "    value.append(current_value)\n",
        "    mappings[key] = value\n",
        "\n",
        "  file.close()   \n",
        "  return mappings"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuyuJyVD0ve",
        "outputId": "b4ddde8c-6cef-4987-b73f-2f6cc715072a"
      },
      "source": [
        "mappings = read_mappings()\n",
        "print(len(mappings))\n",
        "print(mappings.keys())\n",
        "print(mappings.get(\"1\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n",
            "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n",
            "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSAN2RwlEYHf"
      },
      "source": [
        "That’s it – you have successfully initialized one dictionary for documents with the ids linked to the articles content, another dictionary for queries linking queries ids to their correspondent texts, and the mappings dictionary, which matches the queries ids to the lists of relevant document ids.\n",
        "\n",
        "Now, you are all set to start implementing the search algorithm for this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN1--BjSEqHS"
      },
      "source": [
        "### Boolean search algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU95P6O-EuPN"
      },
      "source": [
        "Let’s start with the simplest approach: the information need is formulated as a query. If you extract the words from the query, you can then search for the documents that contain these words and return these documents, as they should be relevant to the query.\n",
        "\n",
        "Here is the algorithm in a nutshell:\n",
        "- Extract the words from the query\n",
        "- For each document, compare the words in the document to the words in the query\n",
        "- Return the document as relevant if any of the query words occurs in the document\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/8.png?raw=1' width='800'/>\n",
        "\n",
        "The very first step in this algorithm is extraction of the words from both queries and documents. You may recall that text comes in as a sequence of symbols or characters, and the machine needs to be told what a word is – you used a special NLP tool called tokenizer to extract words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lShv7w3b9DT3"
      },
      "source": [
        "def get_words(text):\n",
        "  word_list = [word for word in word_tokenize(text.lower())]  # Text is converted to lower case and split into words\n",
        "  return word_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA1TtCgA-ZSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f5cdfe-33c9-4d03-935b-72ea2c3ec88c"
      },
      "source": [
        "doc_words = {}\n",
        "query_words = {}\n",
        "\n",
        "# Entries in both documents and queries are represented as word lists\n",
        "for doc_id in documents.keys():\n",
        "  doc_words[doc_id] = get_words(documents.get(doc_id))\n",
        "for qry_id in queries.keys():\n",
        "  query_words[qry_id] = get_words(queries.get(qry_id))  \n",
        "\n",
        "# check out the length of the dictionaries (these should be the same as before – 1460 and 112), \n",
        "# and check what words are extracted from the first document and the first query\n",
        "print(len(doc_words))\n",
        "print(doc_words.get(\"1\"))\n",
        "print(len(doc_words.get(\"1\")))\n",
        "\n",
        "print(len(query_words))\n",
        "print(query_words.get(\"1\"))\n",
        "print(len(query_words.get(\"1\")))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            "['18', 'editions', 'of', 'the', 'dewey', 'decimal', 'classifications', 'comaromi', ',', 'j.p.', 'the', 'present', 'study', 'is', 'a', 'history', 'of', 'the', 'dewey', 'decimal', 'classification', '.', 'the', 'first', 'edition', 'of', 'the', 'ddc', 'was', 'published', 'in', '1876', ',', 'the', 'eighteenth', 'edition', 'in', '1971', ',', 'and', 'future', 'editions', 'will', 'continue', 'to', 'appear', 'as', 'needed', '.', 'in', 'spite', 'of', 'the', 'ddc', \"'s\", 'long', 'and', 'healthy', 'life', ',', 'however', ',', 'its', 'full', 'story', 'has', 'never', 'been', 'told', '.', 'there', 'have', 'been', 'biographies', 'of', 'dewey', 'that', 'briefly', 'describe', 'his', 'system', ',', 'but', 'this', 'is', 'the', 'first', 'attempt', 'to', 'provide', 'a', 'detailed', 'history', 'of', 'the', 'work', 'that', 'more', 'than', 'any', 'other', 'has', 'spurred', 'the', 'growth', 'of', 'librarianship', 'in', 'this', 'country', 'and', 'abroad', '.']\n",
            "113\n",
            "112\n",
            "['what', 'problems', 'and', 'concerns', 'are', 'there', 'in', 'making', 'up', 'descriptive', 'titles', '?', 'what', 'difficulties', 'are', 'involved', 'in', 'automatically', 'retrieving', 'articles', 'from', 'approximate', 'titles', '?', 'what', 'is', 'the', 'usual', 'relevance', 'of', 'the', 'content', 'of', 'articles', 'to', 'their', 'titles', '?']\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Lk8UFnAF6A"
      },
      "source": [
        "Now let’s code the simple search algorithm. We will refer to it as the\n",
        "Boolean search algorithm since it relies on presence (1) or absence (0) of the query words in the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOw12-HSAJfL"
      },
      "source": [
        "def retrieve_documents(doc_words, query):\n",
        "  docs = []\n",
        "  query_word = []\n",
        "  for doc_id in doc_words.keys():\n",
        "    found = False\n",
        "    i = 0\n",
        "    # Keep iterating through the words in the query word list until either of the two conditions is satisfied\n",
        "    while i < len(query) and not found:\n",
        "      word = query[i]\n",
        "      if word in doc_words.get(doc_id):\n",
        "        docs.append(doc_id)\n",
        "        query_word.append(word)\n",
        "        found = True\n",
        "      else:\n",
        "        i += 1\n",
        "  return (docs, query_word)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTMBsGnQAx8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c2c2e0-9f64-4891-9cdf-51441867ab53"
      },
      "source": [
        "# Check the results: select a query by its id (e.g., query with id 3 here), print out the ids of the documents\n",
        "# that the algorithm found (e.g., the first 100, as there may be many),check how many there are in total\n",
        "docs, query_word = retrieve_documents(doc_words, query_words.get(\"3\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))\n",
        "print(query_word)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102']\n",
            "1397\n",
            "['is', 'is', 'is', 'information', 'is', 'is', '.', 'is', 'definitions', 'is', 'is', 'information', 'what', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'what', 'is', 'is', 'information', 'what', 'is', 'is', 'is', 'information', 'information', 'is', '.', '.', 'is', 'is', '.', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'information', '.', 'is', 'information', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'science', 'information', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', '.', 'what', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'information', 'is', 'information', 'is', 'what', 'what', 'what', 'what', '.', 'science', 'is', 'is', 'is', 'is', 'what', 'information', 'information', 'is', 'definitions', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'possible', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'information', 'is', 'information', '.', 'what', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'what', 'what', 'is', 'is', 'is', 'is', '.', 'is', 'what', 'is', 'is', 'give', 'science', 'is', 'what', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'science', 'is', 'is', 'is', 'what', 'information', 'is', 'information', 'is', 'what', 'what', '.', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'science', 'what', 'is', 'is', 'is', 'what', 'is', 'what', 'is', '.', '.', '.', 'is', 'is', 'what', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'is', 'is', 'is', 'what', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'is', 'is', 'what', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'definitions', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', '.', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'information', 'is', 'information', 'information', '.', 'is', 'is', 'what', 'is', 'is', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'information', 'information', 'what', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'is', 'is', 'information', 'information', 'science', 'is', 'is', 'what', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'science', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'information', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'possible', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'information', 'is', 'is', 'is', '.', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'information', 'is', 'is', 'is', 'is', 'science', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'what', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'information', '.', 'is', 'science', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'where', '.', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'is', '?', 'what', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'is', 'what', 'information', 'possible', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'science', 'information', 'is', 'is', 'what', 'what', 'is', 'is', '.', 'is', 'is', 'what', 'is', '.', 'information', 'what', 'is', 'what', 'is', 'information', '.', '.', 'is', 'information', 'what', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', '.', '.', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', '.', 'science', 'information', 'is', '?', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', '.', 'what', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', '.', '.', 'what', '.', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', '.', 'what', 'is', 'is', 'what', 'is', 'what', '.', 'is', 'what', 'information', 'is', 'is', '.', 'information', '?', 'science', 'is', 'is', 'is', 'is', 'is', 'is', 'science', '.', 'is', 'possible', 'is', 'is', 'is', 'information', 'information', 'is', 'what', 'is', 'is', 'what', '.', 'is', 'science', '.', 'science', 'is', 'is', 'is', 'information', 'is', 'is', 'information', 'is', '.', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'information', 'is', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'information', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'definitions', 'information', 'information', 'is', 'is', 'science', 'is', 'is', 'is', 'information', 'is', 'what', 'information', 'is', 'what', '.', 'is', 'is', 'information', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'possible', 'information', 'is', 'is', 'is', '.', 'is', 'is', '.', 'is', '.', '.', 'is', 'what', 'is', 'is', 'what', 'is', 'is', 'what', 'is', 'what', '.', 'is', 'what', 'information', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'what', 'what', 'is', 'what', 'is', 'possible', 'is', 'is', 'science', 'is', 'is', 'is', 'what', 'is', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', 'information', 'what', 'what', '.', 'what', '.', 'science', 'is', 'is', 'is', 'science', 'is', 'is', 'is', 'is', 'is', '.', 'information', 'information', 'information', 'is', 'is', 'is', '.', '.', 'information', '.', 'is', 'is', 'information', 'is', 'information', 'information', 'information', 'information', 'information', '.', 'science', '.', 'is', 'is', 'is', '.', 'information', 'science', 'is', 'science', 'is', 'science', 'science', 'what', 'is', 'is', 'is', 'is', '.', '.', 'what', 'is', 'information', 'give', 'is', 'information', '.', 'is', 'is', 'what', 'is', 'is', 'is', 'what', 'what', '.', 'is', 'science', 'what', 'is', 'is', 'is', 'is', 'science', 'science', 'is', 'is', 'is', 'is', 'information', '.', '.', '.', 'is', '.', 'information', 'what', 'what', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'is', 'is', 'is', 'information', 'is', 'is', 'information', 'is', '.', '.', 'is', 'information', 'is', 'is', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'science', '.', 'what', 'information', 'what', 'is', 'what', 'is', '.', 'is', 'is', '.', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'information', '.', 'is', 'is', 'is', 'what', 'information', 'what', 'is', 'information', 'is', 'is', 'information', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'what', 'is', 'is', 'information', 'is', 'is', 'is', 'is', 'is', 'is', 'is', '.', 'what', '.', 'is', 'is', '.', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7z3FYS7Bo_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aa687e-cdd7-4ee0-a5bd-4fe8c5a96ce5"
      },
      "source": [
        "# Let’s, for example, look into how the algorithm decided on the documents relevant for query with id 6\n",
        "docs, query_word = retrieve_documents(doc_words, query_words.get(\"6\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))\n",
        "print(query_word)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n",
            "1460\n",
            "['there', 'are', 'are', 'for', 'for', 'are', 'for', 'there', 'for', 'are', 'are', 'for', 'what', 'are', 'communication', 'are', 'what', 'and', 'for', 'for', 'for', 'possibilities', 'are', 'what', 'are', 'are', 'for', 'what', 'are', 'are', 'are', 'are', 'between', 'are', 'there', ',', 'and', ',', 'between', 'for', 'for', 'for', 'are', 'for', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'for', 'and', 'and', 'are', 'are', 'for', 'are', 'for', 'what', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'and', 'for', 'and', 'are', 'for', 'and', 'for', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'there', 'are', 'what', 'and', 'are', 'are', 'are', 'are', 'communication', 'are', 'for', 'communication', 'are', 'for', 'are', 'there', 'for', 'are', 'possibilities', 'there', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'what', 'are', 'for', 'between', 'for', 'are', 'there', 'are', 'are', 'are', 'what', 'are', 'for', 'for', 'are', 'and', 'what', 'for', ',', 'are', 'there', 'are', 'are', 'there', 'between', 'are', 'for', 'are', 'for', 'what', 'there', 'between', 'are', 'are', 'are', 'what', ',', 'what', 'what', 'what', 'for', 'are', 'are', 'for', 'are', 'are', 'what', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'are', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'are', 'for', 'are', 'are', 'and', 'what', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'between', 'for', 'are', 'are', 'are', 'for', 'what', 'are', 'what', 'for', 'are', 'are', 'are', 'are', 'are', 'between', 'are', 'what', 'possibilities', 'and', 'are', 'what', 'for', 'between', 'are', 'what', 'what', 'what', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'for', 'are', 'between', 'are', 'and', 'are', 'for', 'are', 'and', 'are', 'are', 'for', 'for', 'there', 'and', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'and', 'are', 'and', 'communication', 'and', 'are', 'between', 'for', 'for', 'and', 'are', 'what', 'are', 'are', 'are', 'communication', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'there', 'between', 'are', 'are', 'are', 'are', 'and', 'for', 'are', 'between', 'are', 'what', 'and', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'and', 'are', 'there', 'and', 'are', 'are', 'there', 'what', 'for', 'are', 'are', 'for', 'what', 'for', 'for', 'and', 'are', 'what', 'what', 'for', 'are', 'are', 'are', 'for', 'are', 'and', 'are', 'what', 'are', 'are', 'are', 'and', 'are', 'between', 'are', 'what', 'are', 'for', 'what', 'are', 'are', 'are', 'what', 'are', 'what', 'for', 'for', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'what', 'are', 'and', 'for', 'for', 'for', 'are', 'for', 'are', 'for', 'communication', 'are', 'what', 'for', 'for', 'and', 'what', 'what', 'are', 'are', 'are', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'between', 'for', 'there', 'are', 'for', 'are', 'there', 'there', 'are', 'communication', 'are', 'for', 'are', 'between', 'what', 'are', 'for', 'for', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'and', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'there', 'what', 'are', 'are', 'are', 'for', 'what', 'there', 'are', 'are', 'are', 'are', 'possibilities', 'are', 'for', ',', 'are', 'what', 'are', 'are', 'are', 'are', 'and', 'are', 'for', 'for', 'for', 'are', 'for', 'are', 'are', 'for', 'for', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'and', 'for', 'are', 'are', 'for', 'there', 'are', 'are', 'for', 'are', 'for', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'for', 'what', 'for', 'for', 'are', 'are', 'for', 'are', 'possibilities', 'what', 'for', 'for', 'for', 'what', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'possibilities', 'for', 'are', 'are', 'and', 'are', 'for', 'between', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'what', 'for', 'for', 'are', 'are', 'are', 'communication', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'what', 'are', 'for', 'are', 'for', 'are', 'are', 'between', 'and', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'what', 'are', 'and', 'are', 'are', 'are', 'and', 'for', 'for', 'are', 'are', 'for', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'for', 'for', 'and', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'for', 'for', 'are', 'are', 'are', 'what', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'and', 'are', 'for', 'are', 'are', 'for', 'are', 'and', 'are', 'are', 'are', 'and', 'are', 'are', 'and', 'are', 'are', 'are', 'for', 'are', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'there', 'for', 'are', 'are', 'for', 'there', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'and', ',', 'there', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'what', 'are', 'for', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', ',', 'for', 'what', 'are', 'are', 'there', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'are', 'are', 'are', 'there', 'and', 'are', 'for', 'are', 'are', 'are', 'are', 'and', 'are', 'are', 'for', 'are', 'are', 'are', 'possibilities', 'are', 'for', 'are', 'for', 'for', 'are', 'are', 'for', 'for', 'are', 'and', 'and', 'are', 'are', 'are', 'are', 'and', 'for', 'for', 'for', ',', 'are', 'for', 'are', 'are', 'for', 'for', 'for', ',', 'are', 'between', 'between', 'for', 'are', 'what', 'are', 'are', 'are', 'are', 'and', 'what', 'for', 'are', 'are', 'and', 'and', 'are', 'are', 'are', 'are', 'for', 'and', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'what', 'for', 'are', 'are', 'what', 'and', 'what', 'for', 'and', 'possibilities', 'are', 'there', 'there', 'are', 'and', 'are', 'are', 'for', 'are', 'for', 'what', 'what', 'are', 'are', 'for', 'are', 'are', 'what', 'and', 'for', 'for', 'what', 'for', 'what', 'for', 'are', 'and', 'and', 'for', 'for', 'what', 'are', 'are', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'communication', 'between', 'for', 'are', 'and', 'are', 'there', 'and', 'are', 'for', 'are', 'for', 'for', 'communication', 'for', 'for', 'are', 'there', 'are', 'there', 'are', 'there', 'possibilities', 'are', 'are', 'for', 'are', 'for', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'what', 'there', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'what', 'what', 'between', 'for', 'for', 'are', 'for', 'and', 'for', 'possibilities', 'for', 'and', 'what', 'between', 'what', 'for', 'are', 'are', 'and', 'are', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'communication', 'what', 'and', 'for', 'what', 'for', 'what', 'and', 'are', 'what', 'for', 'are', 'are', 'for', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'and', 'and', 'for', 'are', 'are', 'are', 'are', 'possibilities', 'for', 'are', 'are', 'what', 'are', 'and', 'what', 'and', 'communication', 'and', 'for', 'for', ',', 'for', 'are', 'are', 'are', 'are', 'are', 'possibilities', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'are', ',', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'what', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'for', 'are', 'are', 'are', 'for', 'are', 'possibilities', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'possibilities', 'possibilities', 'possibilities', 'are', 'what', 'for', 'are', 'what', 'for', 'are', 'for', 'for', 'are', 'for', 'there', 'are', 'are', 'and', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'and', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'are', 'what', 'are', 'are', 'for', 'and', 'and', 'for', 'and', 'are', 'are', 'for', 'and', 'are', 'for', 'between', 'are', 'for', 'are', 'are', 'and', 'there', 'there', 'what', 'communication', 'are', 'what', 'are', 'there', 'what', 'for', 'what', 'and', 'for', 'what', 'between', 'and', 'for', 'for', 'are', 'for', 'are', 'are', 'and', 'are', 'are', 'and', 'there', 'and', 'what', 'what', 'for', 'what', 'are', 'for', 'are', 'are', 'for', 'there', 'for', 'are', 'what', 'for', 'what', 'for', 'for', 'there', 'for', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'there', 'for', 'for', 'what', 'what', 'for', 'what', ',', 'and', 'for', 'for', 'are', 'for', 'are', 'are', 'there', ',', 'for', 'and', 'there', ',', 'for', 'for', ',', 'are', 'for', ',', 'communication', ',', 'for', 'for', 'for', 'are', 'communication', 'are', 'communication', 'are', 'are', 'and', 'for', ',', 'for', 'for', 'computers', ',', 'and', 'for', 'are', 'for', 'for', ',', 'and', 'what', 'and', 'and', 'are', 'are', 'for', 'for', 'what', 'and', 'are', 'are', 'are', 'and', 'for', 'are', 'there', 'what', 'there', 'are', 'for', 'what', 'what', 'are', 'are', 'and', 'what', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'are', 'for', 'are', 'what', 'what', 'and', 'for', 'are', 'are', 'are', 'for', 'there', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'for', 'are', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'are', 'for', 'are', 'are', 'for', 'and', 'for', 'are', 'are', 'for', 'are', 'for', ',', 'are', 'for', 'what', 'for', 'what', 'possibilities', 'what', 'are', 'and', 'possibilities', 'between', 'and', 'for', 'are', 'for', 'are', 'are', 'there', 'are', 'are', 'are', 'are', 'are', 'and', 'for', 'are', 'for', 'there', 'for', 'what', 'and', 'what', 'for', 'are', 'for', 'are', 'are', 'for', 'for', 'for', 'for', 'for', 'are', 'for', 'for', 'what', 'are', 'for', 'for', 'communication', 'are', 'for', 'are', 'for', 'are', 'are', 'there', 'what', 'are', 'there', 'for', 'and', 'are']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKp-DxN0CnWv"
      },
      "source": [
        "As it shows, the query is matched to the document based on occurrence of such words as “there”, “this”, “the”, “and”, “is” and even a comma since punctuation marks are part of the word list returned by the tokenizer:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/9.png?raw=1' width='800'/>\n",
        "\n",
        "On the face of it, there is a considerable word overlap between the query and the document, yet if you read the text of the query and the text of the document, they don’t seem to have any ideas in common, so in fact this document is not relevant for the given query at all! \n",
        "\n",
        "It seems like the words on the basis of which the query and the document are matched here are simply the wrong ones – they are somewhat irrelevant to the actual information need expressed in the query. \n",
        "\n",
        "How can you make sure that the query and the documents are matched on the basis of more meaningful words?\n",
        "\n",
        "\n",
        "---\n",
        "**Exercise**\n",
        "\n",
        "Another way to match the documents to the queries would be to make it a requirement that the document should contain all the words from the query rather than any.\n",
        "\n",
        "Is this a better approach? Modify the code of the simple Boolean search algorithm to match documents to the queries on the basis of all words, and compare the results.\n",
        "\n",
        "---\n",
        "\n",
        "You will notice that it is rarely the case that a document, even if it is generally relevant,\n",
        "contains all words from the query (at the very least, it does not have to contain question\n",
        "words like “what” and “which” from the query to be relevant). Therefore, this more\n",
        "conservative approach of returning only the documents with all query words in them will\n",
        "work even worse at this stage – it simply will not find any relevant documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ8XIreJRVH3"
      },
      "source": [
        "def retrieve_documents(doc_words, query):\n",
        "  docs = []\n",
        "  for doc_id in doc_words.keys():\n",
        "    # here, you are interested in the documents that contain all words\n",
        "    found = True\n",
        "    i = 0\n",
        "    # iterate through words in the query\n",
        "    while i < len(query) and found:\n",
        "      word = query[i]\n",
        "      if not word in doc_words.get(doc_id):\n",
        "        # if the word is not in document, turn found flag off and stop\n",
        "        found = False\n",
        "      else:\n",
        "        # rwise, move on to the next query word\n",
        "        i += 1\n",
        "\n",
        "    # if all words are found in the document, the last index is len(query)-1 add the doc_id only in this case\n",
        "    if i == len(query) - 1:\n",
        "      docs.append(doc_id)\n",
        "  return docs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCI2VNY7Sp75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b893596d-6678-430c-b5c4-0f0d0c088d1f"
      },
      "source": [
        "docs = retrieve_documents(doc_words, query_words.get(\"112\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4CWm8gS58e"
      },
      "source": [
        "In fact, it is a very rare case that you may have any single document that contains all the words from the query, therefore, with this approach, you will likely get no relevant documents returned for any queries in this dataset.\n",
        "\n",
        "\n",
        "Before we move on, let’s summarize which steps of the algorithm you have implemented so far: you have read the data, initialized the data structures and tokenized the texts.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/10.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi887yuFvSO"
      },
      "source": [
        "##Step 2: Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLddWLQCFwDH"
      },
      "source": [
        "Since we have identified several weaknesses of the current algorithm. \n",
        "\n",
        "Let’s look into further preprocessing steps that will help you represent the content of both the documents and the queries in a more informative way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fESB5r4iPCnJ"
      },
      "source": [
        "###Preselecting the words that matter: stopwords removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFZmaF6DPDrc"
      },
      "source": [
        "The main problem with the search algorithm identified so far is that it considers all words in the queries and documents as equally important. This leads to poor search results, but on top of that it is also intuitively incorrect.\n",
        "\n",
        "You may notice that not all words are equally meaningful in the sentences above. A good test for that would be to ask yourself whether you can define in one phrase what a particular word means: for example, what does “the” mean? You can say that “the” does not have a precise meaning of its own, rather it serves a particular function.\n",
        "\n",
        "In linguistic terms, such words are called function words. You might even notice that when you read a text, for example an article or an email, you tend to skim over such words without paying much attention to them.\n",
        "\n",
        "What happens to the search algorithm when these words are present? You have seen in the example before that they don’t help identify the relevant texts, so in fact the algorithm’s effort is wasted on them. What would happen if the less meaningful words were not taken into consideration?\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/11.png?raw=1' width='800'/>\n",
        "\n",
        "You can see that, were the less meaningful words removed before matching documents to queries, document 1 would not stand a chance – there is simply not a single word overlapping between the query and this document. You can also see that the words that are not grayed out concisely summarize the main idea of the text.\n",
        "\n",
        "**This suggests the first improvement to the developed algorithm: let’s remove the less meaningful words. In NLP applications, the less meaningful words are called stopwords,** and luckily you don’t have to bother with enumerating them – since stopwords are highly repetitive in English, most NLP toolkits have a specially defined stopwords list, so you can rely on this list when processing the data, unless you want to customize it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax--Co5UHtrp"
      },
      "source": [
        "def process(text):\n",
        "  stoplist = set(stopwords.words(\"english\"))  # use English stopwords\n",
        "  # Tokenize text, convert it to lower case and only add the words if they are not included in the stoplist and are not punctuation marks\n",
        "  word_list = [word for word in word_tokenize(text.lower())\n",
        "               if not word in stoplist and not word in string.punctuation\n",
        "              ]\n",
        "\n",
        "  return word_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Moij6IGyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efbd25d-4fa8-4b77-f287-0e0cbd28beca"
      },
      "source": [
        "# Check the result of these preprocessing steps on some documents or queries, e.g. document 1\n",
        "word_list = process(documents.get(\"1\"))\n",
        "print(word_list)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['18', 'editions', 'dewey', 'decimal', 'classifications', 'comaromi', 'j.p.', 'present', 'study', 'history', 'dewey', 'decimal', 'classification', 'first', 'edition', 'ddc', 'published', '1876', 'eighteenth', 'edition', '1971', 'future', 'editions', 'continue', 'appear', 'needed', 'spite', 'ddc', \"'s\", 'long', 'healthy', 'life', 'however', 'full', 'story', 'never', 'told', 'biographies', 'dewey', 'briefly', 'describe', 'system', 'first', 'attempt', 'provide', 'detailed', 'history', 'work', 'spurred', 'growth', 'librarianship', 'country', 'abroad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3BMBD_Y9Bw"
      },
      "source": [
        "That is, the preprocessing step helps removing the stopwords like “of” and “the” from the word list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRIjhiQsZDKF"
      },
      "source": [
        "###Matching forms of same word: morphological processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUZcXSxRZD62"
      },
      "source": [
        "One effect that stopwords and punctuation marks removal has is optimization of search algorithm – the words that do not matter much are removed, so the computational resources are not wasted on them. In general, the more concise and the more informative the data representation is, the better.\n",
        "\n",
        "This brings us to the next issue. Take a look at the query with id 15\n",
        "and document with id 27, which are a match according to the ground truth mappings:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/11.png?raw=1' width='800'/>\n",
        "> The words highlighted in blue will be matched between the query and the document; the ones in red will be missed.\n",
        "\n",
        "The reason for this mismatch is that words may take different forms in different contexts: some contexts may require a mention of a single object or concept like “system”, while others may need multiple “systems” to be mentioned. \n",
        "\n",
        "Such different forms of a word that depend on the context and express different aspects of meaning, for instance multiplicity of “systems”, are technically called morphological forms, and when you see a word like “systems” and try to match it to its other variant “system” you are dealing with morphology.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/12.png?raw=1' width='800'/>\n",
        "\n",
        "Stemming takes word matching one step further and tries to map related words across the board, and this means not just the forms of the very same word. For that, the stemmers rely on a set of rules that try to reduce the related words to the same basic core.\n",
        "\n",
        "The stem in `{retrieve, retrieves, retrieved, retrieving, retrieval}` is retriev. So here is the difference with the technique that you used before – stemming might result in non-words, as for example, you\n",
        "won’t find a word like retriev in a dictionary. To provide you with a couple of other examples, the stem for `{expect, expects, expected, expecting, expectation, expectations}` is expect and the stem for `{continue, continuation, continuing}` is continu.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/13.png?raw=1' width='800'/>\n",
        "\n",
        "Note that the stemmer tries to identify which part of the word is shared between the different forms and related words, and returns this part as a stem by cutting off the differing word endings.\n",
        "\n",
        "Now let’s implement the stemming preprocessing step using NLTK’s stemming\n",
        "functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT778vpj_DxC"
      },
      "source": [
        "def process(text):\n",
        "  stoplist = set(stopwords.words(\"english\"))  # use English stopwords\n",
        "  st = LancasterStemmer() # Initialize the LancasterStemmer\n",
        "  # Apply stemming to the preprocessed text\n",
        "  word_list = [st.stem(word) for word in word_tokenize(text.lower())\n",
        "               if not word in stoplist and not word in string.punctuation\n",
        "              ]\n",
        "\n",
        "  return word_list"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RDj4Enm_cr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8836f1e-5941-44ff-ff95-62f67851ecd5"
      },
      "source": [
        "word_list = process(documents.get(\"27\"))\n",
        "print(word_list)\n",
        "\n",
        "word_list = process(\"organize, organizing, organizational, organ, organic, organizer\")\n",
        "print(word_list)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cost', 'analys', 'sim', 'proc', 'evalu', 'larg', 'inform', 'system', 'bourn', 'c.p', 'ford', 'd.f', 'comput', 'program', 'writ', 'us', 'sim', 'several-year', 'op', 'inform', 'system', 'comput', 'estim', 'expect', 'op', 'cost', 'wel', 'amount', 'equip', 'personnel', 'requir', 'tim', 'period', 'program', 'us', 'analys', 'sev', 'larg', 'system', 'prov', 'us', 'research', 'tool', 'study', 'system', 'many', 'compon', 'interrel', 'op', 'equ', 'man', 'analys', 'would', 'extrem', 'cumbersom', 'tim', 'consum', 'perhap', 'ev', 'impract', 'pap', 'describ', 'program', 'show', 'exampl', 'result', 'sim', 'two', 'sev', 'suggest', 'design', 'spec', 'inform', 'system']\n",
            "['org', 'org', 'org', 'org', 'org', 'org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbX2HC_TEWE9"
      },
      "source": [
        "This example is used here rather as a warning about the way stemmers work: while they are useful in mapping related words to each other, sometimes they might produce an unexpected output and map unrelated words together. This happens because stemmers sometimes go too far in their attempt to establish the correspondences. As the stemmer blindly applies a rather\n",
        "general set of rules to all examples, some of these rules overgeneralize.\n",
        "\n",
        "So, technically, the last two rules should not be applied to map cases like organ –> organize, because the two words do not mean similar things, and it would be better for the applications like search algorithm to make the distinction between the two groups of words\n",
        "`{organize, organizing, organizational, organizer}` and `{organ, organic}`. However, unfortunately, the stemmer algorithm does not take into account what words mean, so once in a while it makes mistakes and connects unrelated words.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/14.png?raw=1' width='800'/>\n",
        "\n",
        "Since in some cases the stemmer would map together words that are not closely related to each other, your search algorithm might consider documents talking about organic products somewhat relevant for the query that asks about organizational skills. This is something to keep in\n",
        "mind; in general, because the queries are mapped to the relevant documents on the basis of more than one word from the query such incorrect mappings are usually outweighed by the relevance of other words.\n",
        "\n",
        "Before we move on, let’s summarize which steps of the algorithm you have implemented so far: you have read the data, initialized the data structures and tokenized the texts, removed stopwords and applied the stemming preprocessing.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/15.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJqriHimIhpG"
      },
      "source": [
        "##Step 3: Information weighing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZt4WYqIiXb"
      },
      "source": [
        "Another problem with the simple Boolean search algorithm is that it can only return a list of documents that contain some or all of the words from the\n",
        "query, but it cannot tell which of the documents are more relevant. \n",
        "\n",
        "You’ve seen before that when you run the algorithm, for most queries it returns a huge number of documents. Without some measure of relevance and relevance ordering it would be physically impossible to look through all the documents returned this way. What could serve as such a measure?\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/16.png?raw=1' width='800'/>\n",
        "\n",
        "Suppose you try to find documents most relevant to the given query. After stopwords and punctuation marks removal you end up with the query words – let’s call them keywords – consisting of `{much, information, retrieval, dissemination, systems, cost}`. Which of the two documents appears to be more relevant? \n",
        "\n",
        "Document `doc_x` does not only contain more keywords than `doc_y`, each keyword also occurs more times, so it would be reasonable to assume that `doc_x` is more relevant – given a choice between these two documents, you should start with `doc_x` if you want to find the answer to the query. \n",
        "\n",
        "How can we take the factors like more keywords and higher number of occurrences into account?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMHtAFGB8XSt"
      },
      "source": [
        "### Weighing words with term frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpVFj6A8YQd"
      },
      "source": [
        "The first requirement, that you should take into account all keywords, suggests that you need to keep track of the words used in the queries and documents. The second requirement that the number of occurrences of each of the keywords matters, suggests that you need to count the number of occurrences rather than simply register presence or absence of a keyword.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/18.png?raw=1' width='800'/>\n",
        "\n",
        "You can achieve this by keeping the number of occurrences for the keywords in a\n",
        "table, or translating this into a Python data structure you can use a dictionary that will allow you to keep track of which counts correspond to which keywords.\n",
        "\n",
        "The correspondent Python dictionaries will be as follows:\n",
        "```\n",
        "Query={much:1, information:1, retrieval:1, dissemination:1, systems:1, cost:1}\n",
        "Doc_x={much:0, information:2, retrieval:1, dissemination:2, systems:3, cost:1}\n",
        "Doc_y={much:0, information:1, retrieval:1, dissemination:1, systems:2, cost:0}\n",
        "```\n",
        "\n",
        "This approach, based on calculating frequency of occurrence, corresponds to the well-known technique in Information Retrieval called term frequency (tf). It relies on the idea that the more frequently the word (term) is used in a document, the more relevant this document becomes to the query. We will use the word term instead of “word” from now on following this widely accepted convention: after all, since you apply stemming, not all keywords keep\n",
        "to be proper “words” anymore (think of the case of retriev).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmupbFaDWCl1"
      },
      "source": [
        "def get_terms(text):\n",
        "  stoplist = set(stopwords.words(\"english\"))\n",
        "  terms = {}\n",
        "  st = LancasterStemmer()\n",
        "  word_list = [st.stem(word) for word in word_tokenize(text.lower()) if not word in stoplist and word not in string.punctuation]\n",
        "  # Estimate the counts for each term and populate the dictionary\n",
        "  for word in word_list:\n",
        "    terms[word] = terms.get(word, 0) + 1\n",
        "  \n",
        "  return terms"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Lb5pktWv5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0797ce-09db-4bea-8504-98e530f40c12"
      },
      "source": [
        "doc_terms = {}\n",
        "query_terms = {}\n",
        "\n",
        "# Populate the term frequency dictionaries for all documents and all queries\n",
        "for doc_id in documents.keys():\n",
        "  doc_terms[doc_id] = get_terms(documents.get(doc_id))\n",
        "for query_id in queries.keys():\n",
        "  query_terms[query_id] = get_terms(queries.get(query_id))\n",
        "\n",
        "print(len(doc_terms))\n",
        "print(doc_terms.get(\"1\"))\n",
        "print(len(doc_terms.get(\"1\")))\n",
        "\n",
        "print(len(query_terms))\n",
        "print(query_terms.get(\"1\"))\n",
        "print(len(query_terms.get(\"1\")))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            "{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n",
            "43\n",
            "112\n",
            "{'problem': 1, 'concern': 1, 'mak': 1, 'describ': 1, 'titl': 3, 'difficul': 1, 'involv': 1, 'autom': 1, 'retriev': 1, 'artic': 2, 'approxim': 1, 'us': 1, 'relev': 1, 'cont': 1}\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZRPIDsSaE8E"
      },
      "source": [
        "Now, let’s represent all queries and all documents in the same shared space: for example, Table represents one query and two documents in a space where the columns of the table (dimensions in the Python data structure) are shared among all three. For instance, column 1 (first dimension) keeps the counts of the term “much” across the query and both documents, column 2 (second dimension) keeps the counts for “information”, and so on.\n",
        "\n",
        "Now let’s add all terms from the data set as columns, and keep the counts for each of them in each query and each document as rows. In terms of Python data structures, this means that each document and each query will keep the whole dictionary of terms in the collection with the associated term frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOwXVnfdHdp"
      },
      "source": [
        "# First, collect the shared vocabulary of terms used in documents and queries; return it as a sorted list for convenience\n",
        "def collect_vocabulary():\n",
        "  all_terms = []\n",
        "  for doc_id in doc_terms.keys():\n",
        "    for term in doc_terms.get(doc_id).keys():\n",
        "      all_terms.append(term)\n",
        "  for query_id in query_terms.keys():\n",
        "    for term in query_terms.get(query_id).keys():\n",
        "      all_terms.append(term)\n",
        "  \n",
        "  return sorted(set(all_terms))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OYTuTkrJon5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10db397-ecc4-4ede-c044-3fd2786f94f1"
      },
      "source": [
        "all_terms = collect_vocabulary()\n",
        "\n",
        "# the length of the shared vocabulary (you should end up with 8881 terms in total), and check the first several terms in the vocabulary\n",
        "print(len(all_terms))\n",
        "print(all_terms[:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8874\n",
            "[\"''\", \"'60\", \"'70\", \"'a\", \"'anyhow\", \"'apparent\", \"'b\", \"'basic\", \"'better\", \"'bibliograph\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwqBjcl7J4w1"
      },
      "source": [
        "def vectorize(input_features, vocabulary):\n",
        "  output = {}\n",
        "  for item_id in input_features.keys():\n",
        "    features = input_features.get(item_id)\n",
        "    \"\"\"\n",
        "    Now each query and each document can be represented with a dictionary with the same set of keys – the\n",
        "    terms from the shared vocabulary. The values will either be equal to the term frequency in the particular\n",
        "    query and document, or will be 0 if the term is not in the query or document\n",
        "    \"\"\"\n",
        "    output_vector = []\n",
        "    for word in vocabulary:\n",
        "      if word in features.keys():\n",
        "        output_vector.append(int(features.get(word)))\n",
        "      else:\n",
        "        output_vector.append(0)\n",
        "    output[item_id] = output_vector\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIVFg-eeK_d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393e807b-59f1-4046-a983-ae475c00c2dd"
      },
      "source": [
        "# Using the vectorize method you can represent all queries and documents in this shared space\n",
        "doc_vectors = vectorize(doc_terms, all_terms)\n",
        "query_vectors = vectorize(query_terms, all_terms)\n",
        "\n",
        "# let's check some statistics on these data structures: you should still have 1460 doc_vectors and 112 query_vectors, with 8881 terms each\n",
        "print(len(doc_vectors))\n",
        "print(len(doc_vectors.get(\"1460\")))\n",
        "\n",
        "print(len(query_vectors))\n",
        "print(len(query_vectors.get(\"112\")))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            "8874\n",
            "112\n",
            "8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T63z8qzDNEO7"
      },
      "source": [
        "Now, another way to think about each of these term dictionaries associated with each document and each query is as vectors: that is, each document and each query is represented as a vector in a shared space, with the number of dimensions equal to the length of the shared vocabulary (8881) and the term frequencies in each dimension representing the coordinates.It reinterprets the query and two documents from Table 3.6 as vectors in two dimensions associated with terms “system” and “cost” (but you can imagine how these vectors are extended to other dimensions, too):\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/19.png?raw=1' width='800'/>\n",
        "\n",
        "Now you can estimate the relevance, or similarity, of the query and documents using the distance between them in the vector space. But before you do that, there is one more observation due."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqrNLmQdI8Jl"
      },
      "source": [
        "###Weighing words with inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ7TLHikI9W1"
      },
      "source": [
        "In a collection of documents you are working with, some terms are much more frequently used across all documents than others. \n",
        "\n",
        "For instance, since this is a collection of articles on information science and information retrieval systems, such terms as `information` or `system` may occur in many documents while other terms like `cost` may occur in fewer\n",
        "documents. Which ones are more helpful in search then? \n",
        "\n",
        "Imagine that you were to find the relevant documents for the query 15, `How much do information retrieval and dissemination systems cost?` If `information` and `system` occur in lots of documents, then you better focus your attention on those documents that contain other terms from the query, e.g., `dissemination` and `cost`, because it is those documents that contain these words that are more relevant. \n",
        "\n",
        "In other words, you would like to give these rarer terms like `dissemination`\n",
        "and `cost` higher weight so that the search algorithm knows it should trust their vote for relevance more. The most straightforward way to assign such weights to the terms is to make it proportionate to the number of documents where the term occurs: the higher the number of documents, the lower the discriminative power of the term, so the lower the weight should be.\n",
        "\n",
        "Take the term `inform` as an example (this is a stem for such words as `inform` and `information`). It occurs in 651 out of 1460, so its document frequency (df) equals `651/1460~0.45`. \n",
        "\n",
        "On the other hand, the term `dissemin` (stem of `dissemination`) only\n",
        "occurs in 68 documents, so its `df=68/1460~0.05`. `Dissemin` is a more valuable term for the search algorithm because it is rare: if a query contains it, the documents that also contain it should be given preference. \n",
        "\n",
        "To assign a higher weight to `dissemin` than to `inform`, let’s take the inverse document frequency (idf): \n",
        "\n",
        "```\n",
        "idf(\"inform\")=1/0.45~2.22,\n",
        "idf(\"dissemin\")=1/0.05~20\n",
        "```\n",
        "\n",
        "These weights show that the rare term \"dissemin\" is almost 10 times more important than the much more frequent term \"inform\". \n",
        "\n",
        "There are two more things to take into account here:\n",
        "\n",
        "- First, some terms from the shared vocabulary may not occur in any of the documents, so their df will be 0. To avoid division by 0, it is common to smooth the counts: to calculate the idf, take `(df+1)` rather than df, i.e. `idf = 1/(df+1)`, so you will never have to divide by zero, and the absolute values of idf won’t change much.\n",
        "- Second, it is common to “tone down” the differences in absolute counts, as the\n",
        "difference between very rare and very frequent terms might be huge, especially in large collections. It is assumed that the weight given to the terms should increase not linearly (i.e., by one with each document) but rather sub-linearly (i.e., more slowly). Logarithmic function achieves this effect: the relative order of the term’s importance doesn’t change, while the absolute number does.\n",
        "\n",
        "To put all the components together, here are the idf values for the terms “inform” and “dissemin” in this collection:\n",
        "\n",
        "```\n",
        "idf(\"inform\") = log10(1460/(651+1)) ~ 0.35\n",
        "idf(\"dissemin\") = log10(1460/68+1)) ~ 1.33\n",
        "```\n",
        "\n",
        "As you can see, the difference is still significant, but the counts are more comparable. The general formula then is:\n",
        "\n",
        "\n",
        "$$ idf(term) = log_{10}(\\frac{N} {df(term) + 1}) $$\n",
        "\n",
        "where $N$ is the total number of documents in the collection.\n",
        "\n",
        "\n",
        "Now, if a particular document contains 2 occurrences of the term “cost”, its idf-weighted value will be `2*1.02=2.04`, while if it contains 2 occurrences of the term “system”, its idfweighted value will be `2*0.44=0.88`, so despite the same term frequencies the more informative term “cost” will get higher overall weight.\n",
        "\n",
        "For instance, here is how idf weighting will change the weights of the terms in the documents.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/20.png?raw=1' width='800'/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWF_gPVXItlI"
      },
      "source": [
        "# Estimate idf values for each term in the vocabulary by counting how many documents contain it\n",
        "def calculate_idfs(vocabulary, doc_features):\n",
        "  doc_idfs = {}\n",
        "  for term in vocabulary:\n",
        "    doc_count = 0\n",
        "    for doc_id in doc_terms.keys():\n",
        "      terms = doc_terms.get(doc_id)\n",
        "      if term in terms.keys():\n",
        "        doc_count += 1\n",
        "    doc_idfs[term] = math.log(float(len(doc_terms.keys())) / float(1 + doc_count), 10)  # Apply the idf formula\n",
        "  return doc_idfs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EN0i7XHKGqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad169d2-17c0-48dd-fe3d-c2de0f991cc9"
      },
      "source": [
        "doc_idfs = calculate_idfs(all_terms, doc_terms)\n",
        "print(len(doc_idfs))\n",
        "# Check out the results: you should have idf values for all 8881 terms from the vocabulary; the idf for any particular term should coincide with your estimates\n",
        "print(doc_idfs.get(\"system\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8874\n",
            "0.43844122348938885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WBU4f8uKf1s"
      },
      "source": [
        "# Define a method to apply idf weighing to the input_terms (in particular, to doc_terms) data structure\n",
        "def vectorize_idf(input_terms, input_idfs, vocabulary):\n",
        "  output = {}\n",
        "  for item_id in input_terms.keys():\n",
        "    terms = input_terms.get(item_id)\n",
        "    output_vector = []\n",
        "    for term in vocabulary:\n",
        "      # For that, multiply the term frequencies with the idf weights if the term is present in the document, otherwise its term frequency stays 0\n",
        "      if term in terms.keys():\n",
        "        output_vector.append(input_idfs.get(term) * float(terms.get(term)))\n",
        "      else:\n",
        "        output_vector.append(float(0))\n",
        "    output[item_id] = output_vector\n",
        "  return output"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFOUNKlELqSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a830459b-89c7-4c8c-e129-e553f54d52b4"
      },
      "source": [
        "# Apply idf weighting to doc_terms\n",
        "doc_vectors = vectorize_idf(doc_terms, doc_idfs, all_terms)\n",
        "\n",
        "# Print out some statistics: the dimensionality of the data structure should still be 1460 documents by 8881 terms\n",
        "print(len(doc_vectors))\n",
        "print(len(doc_vectors.get(\"1460\")))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460\n",
            "8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kuPki3gMFvt"
      },
      "source": [
        "Let’s now summarize what you have implemented so far:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/21.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkWOZNfdMC0"
      },
      "source": [
        "##Step 4: Practical use of the search algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rie4YcOuCH"
      },
      "source": [
        "Now that the documents and queries are represented in the shared search space, it’s time to run the search algorithm, find the most relevant documents for each query and evaluate the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WctXoQiAOwNb"
      },
      "source": [
        "###Retrieval of the most similar documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSAh72HdP-X"
      },
      "source": [
        "How can you estimate query to document similarity based on the vector representations? \n",
        "\n",
        "The similarity can be interpreted as distance in space defined by the query and document vectors.\n",
        "- Each document and each query are represented as vectors in a shared space, with the dimensions representing terms and coordinates representing weighted term counts\n",
        "- Similarity is estimated using distances in this shared space. To eliminate the effect of different lengths, it is more reliable to use the cosine of the angle between the vectors\n",
        "- The higher the cosine, the more similar the query and the document are\n",
        "\n",
        "The cosine can be estimated using the formula:\n",
        "\n",
        "```\n",
        "cosine(vec1,vec2) = dot_product(vec1,vec2)/(length(vec1)*length(vec2))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoqKMv91SqDg"
      },
      "source": [
        "query = [1, 1]\n",
        "document = [3, 5]\n",
        "\n",
        "def length(vector):\n",
        "  sq_length = 0\n",
        "  for idx in range(0, len(vector)):\n",
        "    sq_length += math.pow(vector[idx], 2)\n",
        "  return math.sqrt(sq_length)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMc1OCqbVTeb"
      },
      "source": [
        "def dot_product(vector1, vector2):\n",
        "  if len(vector1) == len(vector2):\n",
        "    dot_prod = 0\n",
        "    for idx in range(0, len(vector1)):\n",
        "      if not vector1[idx] == 0 and not vector2[idx] == 0:\n",
        "        dot_prod += vector1[idx] * vector2[idx]\n",
        "    return dot_prod\n",
        "  else:\n",
        "    return \"Unmatching dimensionality\"\n",
        "\n",
        "def calculate_cosine(query, document):\n",
        "  cosine = dot_product(query, document) / (length(query) * length(document))\n",
        "  return cosine"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6myrzLweWOv9",
        "outputId": "fe585125-d4fc-4226-f4d3-55da3271cbfc"
      },
      "source": [
        "cosine = calculate_cosine(query, document)\n",
        "print(cosine)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9701425001453319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HX0vIOFSpJ8"
      },
      "source": [
        "Let’s calculate the cosine between the query and documents `doc_x` and `doc_y`\n",
        "(using only `tf` and ignoring the `idf` weighing for the sake of simplicity here):\n",
        "\n",
        "```\n",
        "cosine(query,doc_x) = (0+2+1+2+3+1)/(sqrt(6)*sqrt(19)) ~ 0.84\n",
        "cosine(query,doc_y) = (0+1+1+1+2+0)/(sqrt(6)*sqrt(7)) ~ 0.77\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/19.png?raw=1' width='800'/>\n",
        "\n",
        "Based on these results, `doc_x` is more similar to the query than `doc_y`, so if you apply the cosine similarity estimation for the given query to the set of two documents, you should return them ordered as `(doc_x, doc_y)`. As it is `doc_x` that is more similar and thus more relevant to the query, if you want more relevant information you should start with `doc_x`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svGByAUNdjvP",
        "outputId": "260050e0-bcce-4622-d5ca-3755e721a4f9"
      },
      "source": [
        "query = [1, 1]\n",
        "doc_x = [3, 1]\n",
        "doc_y = [2, 0]\n",
        "\n",
        "cosine_x = calculate_cosine(query, doc_x)\n",
        "cosine_y = calculate_cosine(query, doc_y)\n",
        "print(cosine_x)\n",
        "print(cosine_y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8944271909999159\n",
            "0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BxCGFkxdiyJ"
      },
      "source": [
        "Let’s apply cosine similarity to the input queries and documents in the dataset and return the resulting lists of relevant documents ordered by their relevance scores, that is cosine similarity values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JgZtFT8MWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7e721a-087e-4134-d99d-d0779f1da25f"
      },
      "source": [
        "document = doc_vectors.get(\"27\")\n",
        "query = doc_vectors.get(\"15\")\n",
        "\n",
        "cosine = calculate_cosine(query, document) / (length(query) * length(document))\n",
        "print(cosine)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00012969661671903527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc19xMOWhUNd",
        "outputId": "ec101e3c-431c-46a1-da6a-e70a48681a48"
      },
      "source": [
        "document = doc_vectors.get(\"60\")\n",
        "query = doc_vectors.get(\"3\")\n",
        "\n",
        "cosine = calculate_cosine(query, document) / (length(query) * length(document))\n",
        "print(cosine)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00018813808718247965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtX_fcbCiUA7"
      },
      "source": [
        "Let's apply the search algorithm to find relevant documents for a particular query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk7Zr9Z2Lw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7818ef3f-92d6-4461-c6cd-2f058f55c6af"
      },
      "source": [
        "results = {}\n",
        "\n",
        "# For each document in the set of documents, calculate cosine similarity between the input query and the document\n",
        "for doc_id in doc_vectors.keys():\n",
        "  document = doc_vectors.get(doc_id)\n",
        "  cosine = calculate_cosine(query, document)\n",
        "  results[doc_id] = cosine\n",
        "\n",
        "# Sort the results dictionary by cosine values (key=itemgetter(1)) in descending order starting with the highest value (reverse=True).\n",
        "for items in sorted(results.items(), key=itemgetter(1), reverse=True)[:44]:\n",
        "  print(items[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "1316\n",
            "232\n",
            "335\n",
            "1172\n",
            "1345\n",
            "172\n",
            "142\n",
            "993\n",
            "1057\n",
            "231\n",
            "30\n",
            "1214\n",
            "874\n",
            "412\n",
            "177\n",
            "1314\n",
            "409\n",
            "712\n",
            "1227\n",
            "758\n",
            "898\n",
            "1104\n",
            "64\n",
            "1253\n",
            "538\n",
            "1441\n",
            "252\n",
            "1048\n",
            "804\n",
            "1187\n",
            "1433\n",
            "1428\n",
            "176\n",
            "1444\n",
            "403\n",
            "1084\n",
            "1447\n",
            "160\n",
            "1157\n",
            "378\n",
            "952\n",
            "620\n",
            "1350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_11rdPoBV0"
      },
      "source": [
        "This piece of code returns a list of 44 documents identified by the search algorithm as relevant to query 3, ordered by cosine similarity starting with the most relevant one. A quick glance over the first 10 returned documents (that is how many you would see on the first page in the Internet browser) shows that 8 out of 10 documents are also included in the gold standard. Perhaps even more importantly the top 2 documents in the returned list are\n",
        "relevant according to the gold standard – and you might not even need to look any further than the first couple of documents!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwgAyTHA93N"
      },
      "source": [
        "### Evaluate the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ve6ZA8uA-nm"
      },
      "source": [
        "Suppose you are looking for the documents related to query 3, “What is information science? Give definitions where possible.” According to the gold standard, there are 44 documents in this set that match this query.\n",
        "\n",
        "In some situations, you might be interested in exhaustive search, that is, you will measure the success of your algorithm by its ability to find all 44 documents. However, in most situations what you would like is for the algorithm to return the relevant documents at the top of the list: it is more\n",
        "important that the first document returned by the algorithm is relevant than whether the 44th document is relevant.\n",
        "\n",
        "Since the number of relevant documents in the gold standard varies for different queries – for example, it is 44 for query 3 but there is only 1 relevant document for query 6 – you may prefer to set the number of top documents to be returned by your algorithm in advance.\n",
        "\n",
        "In addition, it is rarely the case that users are interested in documents after the first several relevant ones, so returning something between top 3 to top 10 documents would be reasonable.\n",
        "\n",
        "The number of documents that are returned by the algorithm among those top-3\n",
        "(top-10) that are also included as relevant in the gold standard is called true positives – they are truly relevant documents actually identified by your algorithm. The proportion of true positives to the total number of documents returned by the algorithm is called precision, and if you predefine the number of returned documents to be k this measure is called `precision@k` (e.g., `precision@3` or `precision@10`).\n",
        "\n",
        "```\n",
        "precision@10 = (true positives among the top 10 documents) / 10 =\n",
        "(number of documents that are actually relevant among the top 10) / 10\n",
        "```\n",
        "\n",
        "And in the general case, precision@k is:\n",
        "\n",
        "```\n",
        "precision@k = (true positives among the top k documents) / k =\n",
        "(number of documents that are actually relevant among the top k) / k\n",
        "```\n",
        "\n",
        "The higher the precision, the better the algorithm you have built, however the results may also depend on the quality of the dataset and the queries themselves.\n",
        "\n",
        "If you want the results to be more objective, it is useful to evaluate precision across all queries. This is called mean precision, because it takes the mean across all queries.\n",
        "\n",
        "For example, if the top-3 results for the first query are all relevant, `precision@3=1`; if only 2 are relevant, `precision@3=0.66`; for only one relevant result, `precision@3=0.33`. If you estimate the mean precision across 3 queries with such results, it would be equal to `0.66`.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/22.png?raw=1' width='800'/>\n",
        "\n",
        "Thus, the mean precision@k can be estimated as:\n",
        "\n",
        "```\n",
        "Mean_p@k = sum_over_queries(p@k)/number_of_queries =\n",
        "sum_over_queries(true_positives/k)/number_of_queries\n",
        "```\n",
        "\n",
        "You might also be interested in knowing how often the top results contain at least one relevant document: in the case exemplified, the user will be able to find at least one relevant document in the top-3 results, which is quite useful, therefore this ratio will be equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsGeOmF7iGpO",
        "outputId": "e3c38e2e-cebc-43bf-fb4e-7e46cf499f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prefilter – only keep the documents that contain at least one word from the query – to speed search up:\n",
        "def prefilter(doc_terms, query):\n",
        "  docs = []\n",
        "  for doc_id in doc_terms.keys():\n",
        "    found = False\n",
        "    i = 0\n",
        "    while i<len(query.keys()) and not found:\n",
        "      term = list(query.keys())[i]\n",
        "      if term in doc_terms.get(doc_id).keys():\n",
        "        docs.append(doc_id)\n",
        "        found=True\n",
        "      else:\n",
        "        i+=1\n",
        "  return docs\n",
        "\n",
        "docs = prefilter(doc_terms, query_terms.get(\"6\"))\n",
        "print(docs[:100])\n",
        "print(len(docs))\n",
        "\n",
        "prefiltered_docs = {}\n",
        "for query_id in mappings.keys():\n",
        "  prefiltered_docs[query_id] = prefilter(doc_terms, query_terms.get(str(query_id)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['5', '6', '10', '15', '16', '17', '21', '22', '25', '26', '27', '29', '30', '33', '38', '41', '42', '43', '45', '46', '47', '49', '51', '52', '56', '57', '58', '63', '64', '66', '68', '71', '74', '77', '78', '79', '80', '82', '87', '90', '91', '92', '95', '96', '97', '98', '101', '102', '104', '106', '107', '109', '114', '116', '117', '122', '123', '124', '126', '129', '131', '132', '136', '140', '141', '142', '144', '150', '151', '155', '157', '158', '159', '160', '163', '168', '169', '175', '178', '179', '180', '181', '191', '194', '197', '206', '208', '211', '212', '214', '218', '220', '228', '229', '233', '237', '240', '241', '242', '243']\n",
            "593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcWqAwsl1iV"
      },
      "source": [
        "def calculate_precision(model_output, gold_standard):\n",
        "  true_pos = 0\n",
        "  for item in model_output:\n",
        "    if item in gold_standard:\n",
        "      true_pos += 1\n",
        "  # Precision equals to the number of relevant documents from the gold standard that are also returned in the top-k results by the algorithm\n",
        "  return float(true_pos) / float(len(model_output))\n",
        "\n",
        "\n",
        "def calculate_found(model_output, gold_standard):\n",
        "  found = 0\n",
        "  for item in model_output:\n",
        "    if item in gold_standard:\n",
        "      found = 1\n",
        "  # Alternatively, give the algorithm some credit if at least one document in the top k is relevant\n",
        "  return float(found)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKdVa-gHmmuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9ba647-8c8d-4a9a-e5dd-699e4b293300"
      },
      "source": [
        "precision_all = 0.0\n",
        "found_all = 0.0\n",
        "\n",
        "# Calculate mean values across all queries\n",
        "for query_id in mappings.keys():\n",
        "  # Gold standard is the list of relevant document ids that can be extracted from the mappings data structure\n",
        "  gold_standard = mappings.get(str(query_id))\n",
        "  query = query_vectors.get(str(query_id))\n",
        "\n",
        "  results = {}\n",
        "  model_output = []\n",
        "  for doc_id in prefiltered_docs.keys():\n",
        "    document = doc_vectors.get(doc_id)\n",
        "    cosine = calculate_cosine(query, document)\n",
        "    results[doc_id] = cosine  # For each document, estimate its relevance to the query with cosine similarity as before\n",
        "  \n",
        "  # Sort the results and only consider top-k (e.g., top-3) most relevant documents\n",
        "  for items in sorted(results.items(), key=itemgetter(1), reverse=True)[:3]:\n",
        "    model_output.append(items[0])\n",
        "  \n",
        "  precision = calculate_precision(model_output, gold_standard)\n",
        "  found = calculate_found(model_output, gold_standard)\n",
        "  print(str(query_id) + \": \" + str(precision))\n",
        "\n",
        "  # Accumulate evaluation values across all queries; track the results by a print out message\n",
        "  precision_all += precision\n",
        "  found_all += found\n",
        "\n",
        "# In the end, estimate the mean values for all queries\n",
        "print(precision_all / float(len(mappings.keys())))\n",
        "print(found_all / float(len(mappings.keys())))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 0.6666666666666666\n",
            "2: 0.0\n",
            "3: 0.0\n",
            "4: 0.0\n",
            "5: 0.0\n",
            "6: 0.0\n",
            "7: 0.0\n",
            "8: 0.0\n",
            "9: 0.0\n",
            "10: 0.3333333333333333\n",
            "11: 0.3333333333333333\n",
            "12: 0.0\n",
            "13: 0.6666666666666666\n",
            "14: 0.0\n",
            "15: 0.6666666666666666\n",
            "16: 0.0\n",
            "17: 0.0\n",
            "18: 0.3333333333333333\n",
            "19: 0.3333333333333333\n",
            "20: 0.3333333333333333\n",
            "21: 0.0\n",
            "22: 0.0\n",
            "23: 0.3333333333333333\n",
            "24: 0.3333333333333333\n",
            "25: 0.0\n",
            "26: 0.6666666666666666\n",
            "27: 0.3333333333333333\n",
            "28: 0.0\n",
            "29: 0.0\n",
            "30: 1.0\n",
            "31: 0.3333333333333333\n",
            "32: 0.3333333333333333\n",
            "33: 0.0\n",
            "34: 0.6666666666666666\n",
            "35: 0.3333333333333333\n",
            "37: 1.0\n",
            "39: 0.3333333333333333\n",
            "41: 0.0\n",
            "42: 0.0\n",
            "43: 0.0\n",
            "44: 0.0\n",
            "45: 1.0\n",
            "46: 0.6666666666666666\n",
            "49: 0.0\n",
            "50: 1.0\n",
            "52: 0.0\n",
            "54: 0.3333333333333333\n",
            "55: 0.0\n",
            "56: 0.6666666666666666\n",
            "57: 0.0\n",
            "58: 0.3333333333333333\n",
            "61: 0.0\n",
            "62: 0.3333333333333333\n",
            "65: 0.0\n",
            "66: 0.3333333333333333\n",
            "67: 0.3333333333333333\n",
            "69: 0.3333333333333333\n",
            "71: 0.6666666666666666\n",
            "76: 0.3333333333333333\n",
            "79: 0.0\n",
            "81: 0.0\n",
            "82: 0.0\n",
            "84: 0.3333333333333333\n",
            "90: 0.6666666666666666\n",
            "92: 0.0\n",
            "95: 0.3333333333333333\n",
            "96: 0.0\n",
            "97: 0.3333333333333333\n",
            "98: 0.0\n",
            "99: 0.3333333333333333\n",
            "100: 0.3333333333333333\n",
            "101: 0.0\n",
            "102: 0.3333333333333333\n",
            "104: 0.3333333333333333\n",
            "109: 0.3333333333333333\n",
            "111: 0.0\n",
            "0.24999999999999994\n",
            "0.5263157894736842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b_hoGVnjS1"
      },
      "source": [
        "According to the results, on some queries the algorithm performs very well: e.g., a print out message `1: 1.0` shows that all 3 documents returned for query 1 are relevant, making `precision@3` for this query equal to 1. However, on other queries the algorithm does not perform that well: e.g. `6: 0.0` – as there is only one document relevant for query 6 according to the gold standard, the algorithm fails to put it within the first 3 and gets a score of 0 for this query. The mean value of `precision@3` for this algorithm is 0.39, and in 66% of\n",
        "the cases the algorithm finds at least one relevant document among the top 3.\n",
        "\n",
        "If you are only interested in the proportion of cases when the top most relevant\n",
        "document identified by the algorithm is actually relevant you can calculate that modifying the code only slightly: instead of sorting all the results and then taking the top-3 it simply needs to identify and store a single best result.\n",
        "\n",
        "\n",
        "Now, let's modify the code to calculate precision@1 – i.e., the mean value across the queries when the top-1 document returned by the algorithm is indeed relevant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyH5N-6NqeWt",
        "outputId": "f69da2ec-d9b8-4c6b-ed9c-e48d481fa8d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "precision_all = 0.0\n",
        "for query_id in mappings.keys():\n",
        "  gold_standard = mappings.get(str(query_id))\n",
        "  query = query_vectors.get(str(query_id))\n",
        "  result = \"\"\n",
        "  model_output = []\n",
        "  max_sim = 0.0\n",
        "  prefiltered_docs = prefilter(doc_terms, query_terms.get(str(query_id)))\n",
        "  for doc_id in prefiltered_docs:\n",
        "    document = doc_vectors.get(doc_id)\n",
        "    cosine = calculate_cosine(query, document) \n",
        "    if cosine >= max_sim:\n",
        "      max_sim = cosine\n",
        "      result = doc_id\n",
        "  model_output.append(result)\n",
        "  precision = calculate_precision(model_output, gold_standard)\n",
        "  print(f\"{str(query_id)}: {str(precision)}\")\n",
        "  precision_all += precision\n",
        "\n",
        "print(precision_all/len(mappings.keys()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 1.0\n",
            "2: 0.0\n",
            "3: 1.0\n",
            "4: 0.0\n",
            "5: 0.0\n",
            "6: 0.0\n",
            "7: 0.0\n",
            "8: 0.0\n",
            "9: 0.0\n",
            "10: 1.0\n",
            "11: 1.0\n",
            "12: 0.0\n",
            "13: 1.0\n",
            "14: 0.0\n",
            "15: 0.0\n",
            "16: 0.0\n",
            "17: 0.0\n",
            "18: 0.0\n",
            "19: 0.0\n",
            "20: 0.0\n",
            "21: 0.0\n",
            "22: 0.0\n",
            "23: 0.0\n",
            "24: 1.0\n",
            "25: 0.0\n",
            "26: 1.0\n",
            "27: 1.0\n",
            "28: 1.0\n",
            "29: 1.0\n",
            "30: 1.0\n",
            "31: 0.0\n",
            "32: 0.0\n",
            "33: 0.0\n",
            "34: 1.0\n",
            "35: 0.0\n",
            "37: 1.0\n",
            "39: 1.0\n",
            "41: 1.0\n",
            "42: 1.0\n",
            "43: 0.0\n",
            "44: 0.0\n",
            "45: 1.0\n",
            "46: 0.0\n",
            "49: 0.0\n",
            "50: 1.0\n",
            "52: 1.0\n",
            "54: 0.0\n",
            "55: 1.0\n",
            "56: 1.0\n",
            "57: 0.0\n",
            "58: 0.0\n",
            "61: 0.0\n",
            "62: 1.0\n",
            "65: 1.0\n",
            "66: 1.0\n",
            "67: 0.0\n",
            "69: 1.0\n",
            "71: 0.0\n",
            "76: 1.0\n",
            "79: 0.0\n",
            "81: 1.0\n",
            "82: 0.0\n",
            "84: 0.0\n",
            "90: 0.0\n",
            "92: 1.0\n",
            "95: 0.0\n",
            "96: 0.0\n",
            "97: 1.0\n",
            "98: 1.0\n",
            "99: 0.0\n",
            "100: 0.0\n",
            "101: 0.0\n",
            "102: 1.0\n",
            "104: 0.0\n",
            "109: 0.0\n",
            "111: 1.0\n",
            "0.42105263157894735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--qXDNH9sHZv"
      },
      "source": [
        "Finally, you may wish to know how highly, on the average, the algorithm places the relevant document in its ranking. This shows how far into the list of the returned results you should typically look to find the first relevant document. The measure that allows you to evaluate that relies on the use of the highest ranking of a relevant document identified by the algorithm. Since you already sort the returned documents by their relevance scores starting with the most relevant one, position one in this list is called first rank, position two –\n",
        "second rank, and so on.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/23.png?raw=1' width='800'/>\n",
        "\n",
        "- The first relevant documents for both query 1 and query 2 in this example are at position 1 in the ordered lists of returned documents, so their ranks are 1; for query 3, the first relevant document is found in the second position, which gives this result rank 2.\n",
        "- However, returning the first relevant document at rank 1 is better than returning the first relevant document at any further position, so your measure should reflect this by assigning a higher score to the results with the rank 1. Just like with the inverse document frequency, if you take the inverse of the ranks, you will end up with exactly such measure: for both queries 1 and 2 the algorithm returns the best possible results by placing the first relevant document at position 1, so it gets a score of 1/1=1 for that; for query 3 it returns an irrelevant document in position 1 and the first relevant\n",
        "document in position 2 – for that it gets only half the full score, 1/2.\n",
        "\n",
        "To summarize, to assign a score for the results for each query take the inverse of the rank of the first relevant document in the ordered list of results – this is called reciprocal rank:\n",
        "\n",
        "```\n",
        "reciprocal rank = 1 / rank of the first relevant document in the ordered list of results\n",
        "```\n",
        "\n",
        "- Finally, as before, you want to have a comprehensive overview of the results across all queries, so you need to take a mean reciprocal rank (MRR) for the reciprocal ranks across all queries. For the example from Figure, this will equal to `(1 + 1 + 1/2) / 3 = 0.83`.\n",
        "\n",
        "```\n",
        "MRR = sum_of_reciprocal_ranks_across_queries / number_of_queries\n",
        "```\n",
        "\n",
        "The best-case scenario is when the algorithm always puts a relevant document at the top of the list, so it assigns rank 1 in all cases. If the first relevant document is always found at rank 2, the mean will equal to 1/2; for the results at rank 3, the mean will be 1/3, and so on.\n",
        "\n",
        "The result that you get for the example from Figure, `MRR = (1+1+1/2)/3 = 0.83`, lies between 1/2 and 1 and is closer to 1. This value shows, that on the average, the ranking of the first relevant document returned by the algorithm is between 1st and 2nd rank, and is in fact more often 1st than 2nd.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSUJbreYmXqk",
        "outputId": "c6b8e8e0-4911-4da6-88ad-1e688e43ed16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MRR – rank of the first relevant entry:\n",
        "rank_all = 0.0\n",
        "for query_id in mappings.keys():\n",
        "  # extract the list of gold standard mappings for each query\n",
        "  gold_standard = mappings.get(str(query_id))\n",
        "  query = query_vectors.get(str(query_id))\n",
        "  results = {}\n",
        "  for doc_id in doc_vectors.keys():\n",
        "    document = doc_vectors.get(doc_id)\n",
        "    cosine = calculate_cosine(query, document)    \n",
        "    results[doc_id] = cosine\n",
        "  # Sort the documents returned by the algorithm in descending order starting with the most similar. The position of each document in this sorted list is called rank\n",
        "  sorted_results = sorted(results.items(), key=itemgetter(1), reverse=True)\n",
        "  index = 0\n",
        "  found = False\n",
        "  # You only need to find the first relevant document in this list, so set the flag found to False, and switch it\n",
        "  # to True as soon as you encounter the first relevant document, or reach the end of the list\n",
        "  while found==False:\n",
        "    item = sorted_results[index]\n",
        "    # Increment index (rank) with each document in the results\n",
        "    index += 1\n",
        "    if index==len(sorted_results):\n",
        "      found = True\n",
        "    # As before, the document id is the first element in the sorted tuples of (document_id, similarity score)\n",
        "    if item[0] in gold_standard:\n",
        "      found = True\n",
        "      print(f\"{str(query_id)}: {str(float(1) / float(index))}\")\n",
        "      # Estimate inverse of the rank\n",
        "      rank_all += float(1) / float(index)\n",
        "\n",
        "# Calculate and print out the mean value across all queries           \n",
        "print(rank_all/float(len(mappings.keys())))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 1.0\n",
            "2: 0.3333333333333333\n",
            "3: 1.0\n",
            "4: 0.08333333333333333\n",
            "5: 0.125\n",
            "6: 0.04\n",
            "7: 0.05555555555555555\n",
            "8: 0.03571428571428571\n",
            "9: 0.5\n",
            "10: 1.0\n",
            "11: 1.0\n",
            "12: 0.14285714285714285\n",
            "13: 1.0\n",
            "14: 0.011764705882352941\n",
            "15: 0.2\n",
            "16: 0.02857142857142857\n",
            "17: 0.25\n",
            "18: 0.25\n",
            "19: 0.25\n",
            "20: 0.5\n",
            "21: 0.0625\n",
            "22: 0.09090909090909091\n",
            "23: 0.3333333333333333\n",
            "24: 1.0\n",
            "25: 0.14285714285714285\n",
            "26: 1.0\n",
            "27: 1.0\n",
            "28: 1.0\n",
            "29: 1.0\n",
            "30: 1.0\n",
            "31: 0.5\n",
            "32: 0.3333333333333333\n",
            "33: 0.07142857142857142\n",
            "34: 1.0\n",
            "35: 0.5\n",
            "37: 1.0\n",
            "39: 1.0\n",
            "41: 1.0\n",
            "42: 1.0\n",
            "43: 0.2\n",
            "44: 0.5\n",
            "45: 1.0\n",
            "46: 0.5\n",
            "49: 0.3333333333333333\n",
            "50: 1.0\n",
            "52: 1.0\n",
            "54: 0.3333333333333333\n",
            "55: 1.0\n",
            "56: 1.0\n",
            "57: 0.1111111111111111\n",
            "58: 0.3333333333333333\n",
            "61: 0.5\n",
            "62: 1.0\n",
            "65: 1.0\n",
            "66: 1.0\n",
            "67: 0.125\n",
            "69: 1.0\n",
            "71: 0.25\n",
            "76: 1.0\n",
            "79: 0.25\n",
            "81: 1.0\n",
            "82: 0.5\n",
            "84: 0.045454545454545456\n",
            "90: 0.14285714285714285\n",
            "92: 1.0\n",
            "95: 0.5\n",
            "96: 0.043478260869565216\n",
            "97: 1.0\n",
            "98: 1.0\n",
            "99: 0.5\n",
            "100: 0.2\n",
            "101: 0.014084507042253521\n",
            "102: 1.0\n",
            "104: 0.2\n",
            "109: 0.5\n",
            "111: 1.0\n",
            "0.5647694319005727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olRSihONo2J2"
      },
      "source": [
        "The result – mean reciprocal rank of 0.58 – printed by this piece of code suggests that, on the average, the highest rank of a relevant document identified by this search algorithm is between 1st and 2nd, i.e. you will often find the relevant results within the first pair of returned documents.\n",
        "\n",
        "\n",
        "This concludes the implementation of the search algorithm, so let’s summarize what steps you have implemented:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/Essential-NLP/03-information-retrieval/images/24.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wh2UkX7_mys"
      },
      "source": [
        "##Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1v2Ov9_n_R"
      },
      "source": [
        "Apply the search algorithm to your own data. For that, you would need to read in the files one by one.\n",
        "\n",
        "Alternatively, apply the search algorithm to a different dataset from\n",
        "http://ir.dcs.gla.ac.uk/resources/test_collections/. \n",
        "\n",
        "Among these, the Cranfield dataset uses a similar data format to\n",
        "the CISI dataset, and is also relatively small and easy to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pMDWBPQCnbV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}