{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-entropy-loss-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGFnT8WSTQw/whNCTEDqIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/cross_entropy_loss_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24c1aatvQoZ"
      },
      "source": [
        "## Cross-Entropy Loss from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDb087qvmHZ"
      },
      "source": [
        "To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how much error the model has. The loss function , also referred to as the cost function , is the algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.\n",
        "\n",
        "The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk-HKrl2vy-D"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZn01YLCvUZW"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8S9cqiWv2fc"
      },
      "source": [
        "## Categorical Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5xA6L3Zv3hD"
      },
      "source": [
        "If you’re familiar with linear regression, then you already know one of the loss functions used with neural networks that do regression: squared error (or mean squared error with neural networks).\n",
        "\n",
        "The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. Categorical cross-entropy is explicitly used to compare a `ground-truth` probability ( y or `targets`) and some predicted distribution ( y-hat or `predictions`), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
        "\n",
        "The formula for calculating the categorical cross-entropy of `y` (actual/desired distribution) and `y-hat` (predicted distribution) is:\n",
        "\n",
        "$$\n",
        "L_i = - \\sum_j y_{i, j}log(\\hat{y_{i, j}})\n",
        "$$\n",
        "\n",
        "In general, the log loss error function is what we apply to the output of a binary logistic regression model — there are only two classes in the distribution, each of them applying to a single output (neuron) which is targeted as a `0` or `1`. In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy\n",
        "compares two probability distributions. \n",
        "\n",
        "In our case, we have a softmax output, let’s say it’s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NitS9Zy5ukE0"
      },
      "source": [
        "softmax_output = [0.7, 0.1, 0.2]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdzPB4nCrnB"
      },
      "source": [
        "We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). If that’s the intended prediction, then the desired probability distribution is `[1, 0, 0]`.\n",
        "\n",
        "Arrays or vectors like this are called one-hot , meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0. When comparing the model’s results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is\n",
        "multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy. \n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "To exemplify this — if we take a softmax output of `[0.7, 0.1, 0.2]` and targets of `[1, 0, 0]` , we can apply the calculations as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHAvPTjCiSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fadac85-7e67-4739-c770-89ffe5bdf98c"
      },
      "source": [
        "L = -(1 * math.log(0.7) + 0 * math.log(0.1) + 0 * math.log(0.2))\n",
        "print(L)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEcZzwQZFF2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c462ca-4f2c-4d83-868a-faa37ce4640c"
      },
      "source": [
        "L = -(-0.35667494393873245 + 0  + 0)\n",
        "print(L)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbeE3i6HFZVB"
      },
      "source": [
        "Let’s see the Python code for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5dhekhFZ2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b821736-6b64-4d5c-dbb2-a9489e6ccb1e"
      },
      "source": [
        "# An example output from the output layer of the neural network\n",
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "# Ground truth\n",
        "target_output = [1, 0, 0]\n",
        "\n",
        "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
        "         math.log(softmax_output[1]) * target_output[1] +\n",
        "         math.log(softmax_output[2]) * target_output[2])\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGD72WPH-gJ"
      },
      "source": [
        "That’s the full categorical cross-entropy calculation, but we can make a few assumptions given one-hot target vectors.\n",
        "\n",
        "The example confidence level might look like `[0.22, 0.6, 0.18]` or `[0.32, 0.36, 0.32]`. In both cases, the argmax of these vectors will return the second\n",
        "class as the prediction, but the model’s confidence about these predictions is high only for one of them. The Categorical Cross-Entropy Loss accounts for that and outputs a larger loss the lower the confidence is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i80a632pJPu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9368e004-af25-4eeb-b8fc-7453edc1f4ac"
      },
      "source": [
        "print (math.log( 1. ))\n",
        "print (math.log( 0.95 ))\n",
        "print (math.log( 0.9 ))\n",
        "print (math.log( 0.8 ))\n",
        "print ( '...' )\n",
        "print (math.log( 0.2 ))\n",
        "print (math.log( 0.1 ))\n",
        "print (math.log( 0.05 ))\n",
        "print (math.log( 0.01 ))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "-0.05129329438755058\n",
            "-0.10536051565782628\n",
            "-0.2231435513142097\n",
            "...\n",
            "-1.6094379124341003\n",
            "-2.3025850929940455\n",
            "-2.995732273553991\n",
            "-4.605170185988091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKCzLyhuJejz"
      },
      "source": [
        "When the confidence level equals 1 , meaning the model is 100% “sure” about its prediction, the loss value for this sample equals 0 . The loss value raises with the confidence level, approaching 0. You might also wonder\n",
        "why we did not print the result of log(0) — we’ll explain that shortly.\n",
        "\n",
        "Log is short for logarithm and is defined as the solution for the x-term in an equation of the form $a^x = b$.\n",
        "\n",
        "For example, $10^x = 100$ can be solved with a $log: log_10 (100)$ , which evaluates to 2. This property of the log function is especially beneficial when e (Euler’s number or `~2.71828`) is used in the base (where 10 is in the example). The logarithm with e as its base is referred to as the natural logarithm , natural log , or simply log — you may also see this written as $ln : ln(x) = log(x) = log_e (x)$.\n",
        "\n",
        "The natural log represents the solution for the x-term in the equation $e^x = b$ ; for example, $e^x = 5.2$ is solved by $log(5.2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWGmVrfP4pl9",
        "outputId": "fa14f9d9-6783-4a94-f837-412be8f35606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b = 5.2\n",
        "print(np.log(b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6486586255873816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJOXOucO5cmG"
      },
      "source": [
        "We can confirm this by exponentiating our result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkXBwAt5dFO",
        "outputId": "e77512e9-afce-49cb-83a2-0717f45aaad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(math.e ** 1.6486586255873816)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.199999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF8RV5dX5qq6"
      },
      "source": [
        "The small difference is the result of floating-point precision in Python.\n",
        "\n",
        "Consider a scenario with a neural network that performs classification between three classes, and the neural network classifies in batches of three. After running through the softmax activation function with a batch of 3 samples and 3 classes, the network’s output layer yields:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfsdsMR67uJ"
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8qYmgVC7V85"
      },
      "source": [
        "We need a way to dynamically calculate the categorical cross-entropy, which we now know is a negative log calculation. To determine which value in the softmax output to calculate the negative log from, we simply need to know our target values.\n",
        "\n",
        "In this example, there are 3 classes; let’s say we’re trying to classify something as a “dog,” “cat,” or “human.” A dog is class 0 (at index 0), a\n",
        "cat class 1 (index 1), and a human class 2 (index 2). \n",
        "\n",
        "Let’s assume the batch of three sample inputs to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as a list of target indices) would be `[0, 1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkOjIW-K7rCc"
      },
      "source": [
        "softmax_outputs = [\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "]\n",
        "class_targets = [ 0 , 1 , 1 ] # dog, cat, cat"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhR9VDU-79Gb"
      },
      "source": [
        "- The first value, 0, in class_targets means the first softmax output distribution’s intended prediction was the one at the 0th index of `[ 0.7 , 0.1 , 0.2 ]` ; the model has a 0.7 confidence score that this observation is a dog.\n",
        "\n",
        "- This continues throughout the batch, where the intended target of the 2nd softmax distribution, `[ 0.1 , 0.5 , 0.4 ]` , was at an index of 1; the model only has a 0.5 confidence score that this is a cat — the model is less certain about this observation. \n",
        "\n",
        "- In the last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case — a pretty high confidence.\n",
        "\n",
        "With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the values from the softmax distributions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KTKOxEL80e8",
        "outputId": "d727a40d-8337-4e1c-e064-b76ac5e5e645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for target_index, distribution in zip(class_targets, softmax_outputs):\n",
        "  print(distribution[target_index])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n",
            "0.5\n",
            "0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgrarSw3-XBa"
      },
      "source": [
        "This can be further simplified using NumPy (we’re creating a NumPy array of the Softmax outputs this time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdEp7-m-ZXM",
        "outputId": "1c88f9ad-8d74-41d4-9545-eb527c35acf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])\n",
        "\n",
        "class_targets = [0, 1, 1] # dog, cat, cat\n",
        "\n",
        "print(softmax_outputs[[0, 1, 2], class_targets])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk63C7t_juR"
      },
      "source": [
        "What are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to use a list filled with indices and that’s convenient for us — we could use the class_targets for this purpose as it already contains the list of indices that we are interested in. The problem is that\n",
        "this has to filter data rows in the array — the second dimension.\n",
        "\n",
        "To perform that, we also need to\n",
        "explicitly filter this array in its first dimension. This dimension contains the predictions and we, of\n",
        "course, want to retain them all. We can achieve that by using a list containing numbers from 0\n",
        "through all of the indices. We know we’re going to have as many indices as distributions in our\n",
        "entire batch, so we can use a `range ()` instead of typing each value ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZfh6_Wf_r6I",
        "outputId": "00ee0144-21a8-49c9-de06-a61769208260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULZfdCpABGU"
      },
      "source": [
        "This returns a list of the confidences at the target indices for each of the samples. Now we apply the negative log to this list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgSfzjEPACUh",
        "outputId": "6e3669e6-47d9-4eba-ab58-2812f4daa628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35667494 0.69314718 0.10536052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgHaSck2AV8X"
      },
      "source": [
        "Finally, we want an average loss per batch to have an idea about how our model is doing during training. There are many ways to calculate an average in Python; the most basic form of an average is the `arithmetic mean : sum(iterable) / len(iterable)`. NumPy has a method that computes this average on arrays, so we will use that instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSRpewU8AcIb",
        "outputId": "9d1256c5-9749-4769-d744-8bbdc061eaa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "average_loss = np.mean(neg_log)\n",
        "\n",
        "print(average_loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztx0_OzVBfuS"
      },
      "source": [
        "We have already learned that targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label’s position is filled with 1.Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. The check can be performed by counting the dimensions — if targets are single-dimensional (like a list), they are\n",
        "sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors.\n",
        "\n",
        "In this second case, instead of filtering out the confidences at the target labels. We have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis 1).\n",
        "\n",
        "We have to add a test to the code we just wrote for the number of dimensions,\n",
        "move calculations of the log values outside of this new if statement, and implement the solution for the one-hot encoded labels following the first equation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xCrzSFB5Vw",
        "outputId": "a09d9f0a-425b-4d3f-dac6-acd89df70ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])\n",
        "\n",
        "class_targets = np.array([\n",
        "  [1, 0, 0],\n",
        "  [0, 1, 0],\n",
        "  [0, 1, 0]\n",
        "])\n",
        "\n",
        "# Probabilities for target values - only if categorical labels\n",
        "if len(class_targets.shape) == 1:\n",
        "  correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
        "elif len(class_targets.shape) == 2:  # Mask values - only for one-hot encoded labels\n",
        "  correct_confidences = np.sum(softmax_outputs * class_targets, axis=1)\n",
        "\n",
        "# Losses\n",
        "neg_log = - np.log(correct_confidences)\n",
        "average_loss = np.mean(neg_log)\n",
        "\n",
        "print(average_loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1VoBJMEbdf"
      },
      "source": [
        "Before we move on, there is one additional problem to solve. The softmax output, which is also an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences.\n",
        "It is possible that the model will have full confidence for one label making all the remaining confidences zero. Similarly, it is also possible that the model will assign full confidence to a value that wasn’t the target.\n",
        "\n",
        "If we then try to calculate the loss of this confidence of 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSDpFBrQEgdW",
        "outputId": "c5d8b34c-a75c-4808-a85b-a234a1540fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "-np.log(0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}