{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-entropy-loss-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGh836QplIUpWd1DON6YMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/cross_entropy_loss_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24c1aatvQoZ"
      },
      "source": [
        "## Cross-Entropy Loss from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDb087qvmHZ"
      },
      "source": [
        "To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how much error the model has. The loss function , also referred to as the cost function , is the algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.\n",
        "\n",
        "The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk-HKrl2vy-D"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH77XpOiqLlB"
      },
      "source": [
        "!pip -q install nnfs"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTqL_OZ0uQOo"
      },
      "source": [
        "!wget -q https://github.com/rahiakela/machine-learning-algorithms/raw/main/neural-networks-from-scratch/05-calculating-loss/custom_nn.py"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZn01YLCvUZW"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "from custom_nn import *\n",
        "\n",
        "nnfs.init()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8S9cqiWv2fc"
      },
      "source": [
        "## Categorical Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5xA6L3Zv3hD"
      },
      "source": [
        "If you’re familiar with linear regression, then you already know one of the loss functions used with neural networks that do regression: squared error (or mean squared error with neural networks).\n",
        "\n",
        "The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. Categorical cross-entropy is explicitly used to compare a `ground-truth` probability ( y or `targets`) and some predicted distribution ( y-hat or `predictions`), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
        "\n",
        "The formula for calculating the categorical cross-entropy of `y` (actual/desired distribution) and `y-hat` (predicted distribution) is:\n",
        "\n",
        "$$\n",
        "L_i = - \\sum_j y_{i, j}log(\\hat{y_{i, j}})\n",
        "$$\n",
        "\n",
        "In general, the log loss error function is what we apply to the output of a binary logistic regression model — there are only two classes in the distribution, each of them applying to a single output (neuron) which is targeted as a `0` or `1`. In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy\n",
        "compares two probability distributions. \n",
        "\n",
        "In our case, we have a softmax output, let’s say it’s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NitS9Zy5ukE0"
      },
      "source": [
        "softmax_output = [0.7, 0.1, 0.2]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdzPB4nCrnB"
      },
      "source": [
        "We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). If that’s the intended prediction, then the desired probability distribution is `[1, 0, 0]`.\n",
        "\n",
        "Arrays or vectors like this are called one-hot , meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0. When comparing the model’s results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is\n",
        "multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy. \n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "To exemplify this — if we take a softmax output of `[0.7, 0.1, 0.2]` and targets of `[1, 0, 0]` , we can apply the calculations as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHAvPTjCiSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a2cfdb-695f-4acf-f634-da3ae5b95c5e"
      },
      "source": [
        "L = -(1 * math.log(0.7) + 0 * math.log(0.1) + 0 * math.log(0.2))\n",
        "print(L)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEcZzwQZFF2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c55567e-8063-424a-f30b-2efd67d9429e"
      },
      "source": [
        "L = -(-0.35667494393873245 + 0  + 0)\n",
        "print(L)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbeE3i6HFZVB"
      },
      "source": [
        "Let’s see the Python code for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5dhekhFZ2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d24c5f-fde5-463a-d812-2f9d3162cc50"
      },
      "source": [
        "# An example output from the output layer of the neural network\n",
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "# Ground truth\n",
        "target_output = [1, 0, 0]\n",
        "\n",
        "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
        "         math.log(softmax_output[1]) * target_output[1] +\n",
        "         math.log(softmax_output[2]) * target_output[2])\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGD72WPH-gJ"
      },
      "source": [
        "That’s the full categorical cross-entropy calculation, but we can make a few assumptions given one-hot target vectors.\n",
        "\n",
        "The example confidence level might look like `[0.22, 0.6, 0.18]` or `[0.32, 0.36, 0.32]`. In both cases, the argmax of these vectors will return the second\n",
        "class as the prediction, but the model’s confidence about these predictions is high only for one of them. The Categorical Cross-Entropy Loss accounts for that and outputs a larger loss the lower the confidence is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i80a632pJPu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e310d69f-368c-429c-cf08-74b85720b0fb"
      },
      "source": [
        "print (math.log( 1. ))\n",
        "print (math.log( 0.95 ))\n",
        "print (math.log( 0.9 ))\n",
        "print (math.log( 0.8 ))\n",
        "print ( '...' )\n",
        "print (math.log( 0.2 ))\n",
        "print (math.log( 0.1 ))\n",
        "print (math.log( 0.05 ))\n",
        "print (math.log( 0.01 ))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "-0.05129329438755058\n",
            "-0.10536051565782628\n",
            "-0.2231435513142097\n",
            "...\n",
            "-1.6094379124341003\n",
            "-2.3025850929940455\n",
            "-2.995732273553991\n",
            "-4.605170185988091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKCzLyhuJejz"
      },
      "source": [
        "When the confidence level equals 1 , meaning the model is 100% “sure” about its prediction, the loss value for this sample equals 0 . The loss value raises with the confidence level, approaching 0. You might also wonder\n",
        "why we did not print the result of log(0) — we’ll explain that shortly.\n",
        "\n",
        "Log is short for logarithm and is defined as the solution for the x-term in an equation of the form $a^x = b$.\n",
        "\n",
        "For example, $10^x = 100$ can be solved with a $log: log_10 (100)$ , which evaluates to 2. This property of the log function is especially beneficial when e (Euler’s number or `~2.71828`) is used in the base (where 10 is in the example). The logarithm with e as its base is referred to as the natural logarithm , natural log , or simply log — you may also see this written as $ln : ln(x) = log(x) = log_e (x)$.\n",
        "\n",
        "The natural log represents the solution for the x-term in the equation $e^x = b$ ; for example, $e^x = 5.2$ is solved by $log(5.2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWGmVrfP4pl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3e35d9-69fd-47c9-d7ca-7778206fc003"
      },
      "source": [
        "b = 5.2\n",
        "print(np.log(b))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6486586255873816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJOXOucO5cmG"
      },
      "source": [
        "We can confirm this by exponentiating our result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkXBwAt5dFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28dcecd-7279-4c6f-aa8b-7632e2fb0a71"
      },
      "source": [
        "print(math.e ** 1.6486586255873816)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.199999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF8RV5dX5qq6"
      },
      "source": [
        "The small difference is the result of floating-point precision in Python.\n",
        "\n",
        "Consider a scenario with a neural network that performs classification between three classes, and the neural network classifies in batches of three. After running through the softmax activation function with a batch of 3 samples and 3 classes, the network’s output layer yields:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfsdsMR67uJ"
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8qYmgVC7V85"
      },
      "source": [
        "We need a way to dynamically calculate the categorical cross-entropy, which we now know is a negative log calculation. To determine which value in the softmax output to calculate the negative log from, we simply need to know our target values.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "In this example, there are 3 classes; let’s say we’re trying to classify something as a “dog,” “cat,” or “human.” A dog is class 0 (at index 0), a\n",
        "cat class 1 (index 1), and a human class 2 (index 2). \n",
        "\n",
        "Let’s assume the batch of three sample inputs to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as a list of target indices) would be `[0, 1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkOjIW-K7rCc"
      },
      "source": [
        "softmax_outputs = [\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "]\n",
        "class_targets = [ 0 , 1 , 1 ] # dog, cat, cat"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhR9VDU-79Gb"
      },
      "source": [
        "- The first value, 0, in class_targets means the first softmax output distribution’s intended prediction was the one at the 0th index of `[ 0.7 , 0.1 , 0.2 ]` ; the model has a 0.7 confidence score that this observation is a dog.\n",
        "\n",
        "- This continues throughout the batch, where the intended target of the 2nd softmax distribution, `[ 0.1 , 0.5 , 0.4 ]` , was at an index of 1; the model only has a 0.5 confidence score that this is a cat — the model is less certain about this observation. \n",
        "\n",
        "- In the last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case — a pretty high confidence.\n",
        "\n",
        "With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the values from the softmax distributions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KTKOxEL80e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd2ec31-3d93-400e-c22d-058f66bcd52e"
      },
      "source": [
        "for target_index, distribution in zip(class_targets, softmax_outputs):\n",
        "  print(distribution[target_index])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n",
            "0.5\n",
            "0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgrarSw3-XBa"
      },
      "source": [
        "This can be further simplified using NumPy (we’re creating a NumPy array of the Softmax outputs this time):\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/3.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdEp7-m-ZXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28026ecb-984a-44ac-f9ad-49bc756b4b3f"
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])\n",
        "\n",
        "class_targets = [0, 1, 1] # dog, cat, cat\n",
        "\n",
        "print(softmax_outputs[[0, 1, 2], class_targets])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk63C7t_juR"
      },
      "source": [
        "What are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to use a list filled with indices and that’s convenient for us — we could use the class_targets for this purpose as it already contains the list of indices that we are interested in. The problem is that\n",
        "this has to filter data rows in the array — the second dimension.\n",
        "\n",
        "To perform that, we also need to\n",
        "explicitly filter this array in its first dimension. This dimension contains the predictions and we, of\n",
        "course, want to retain them all. We can achieve that by using a list containing numbers from 0\n",
        "through all of the indices. We know we’re going to have as many indices as distributions in our\n",
        "entire batch, so we can use a `range ()` instead of typing each value ourselves:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/4.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZfh6_Wf_r6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86afeb1-dde5-4f1b-efa8-c2901e062f9b"
      },
      "source": [
        "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULZfdCpABGU"
      },
      "source": [
        "This returns a list of the confidences at the target indices for each of the samples. Now we apply the negative log to this list:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/5.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgSfzjEPACUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f294d5-1e2a-4d71-ccd6-0f66573ed8eb"
      },
      "source": [
        "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35667494 0.69314718 0.10536052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgHaSck2AV8X"
      },
      "source": [
        "Finally, we want an average loss per batch to have an idea about how our model is doing during training. There are many ways to calculate an average in Python; the most basic form of an average is the `arithmetic mean : sum(iterable) / len(iterable)`. NumPy has a method that computes this average on arrays, so we will use that instead.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/6.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSRpewU8AcIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab5b7e9-fd4c-4231-fa3c-3ec9606b6e87"
      },
      "source": [
        "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "average_loss = np.mean(neg_log)\n",
        "\n",
        "print(average_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztx0_OzVBfuS"
      },
      "source": [
        "We have already learned that targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label’s position is filled with 1.Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. The check can be performed by counting the dimensions — if targets are single-dimensional (like a list), they are\n",
        "sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors.\n",
        "\n",
        "In this second case, instead of filtering out the confidences at the target labels. We have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis 1).\n",
        "\n",
        "We have to add a test to the code we just wrote for the number of dimensions,\n",
        "move calculations of the log values outside of this new if statement, and implement the solution for the one-hot encoded labels following the first equation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xCrzSFB5Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8f0e5b-c4ad-4c4e-d8cd-c1273d9333e8"
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.1 , 0.2 ],\n",
        "  [ 0.1 , 0.5 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])\n",
        "\n",
        "class_targets = np.array([\n",
        "  [1, 0, 0],\n",
        "  [0, 1, 0],\n",
        "  [0, 1, 0]\n",
        "])\n",
        "\n",
        "# Probabilities for target values - only if categorical labels\n",
        "if len(class_targets.shape) == 1:\n",
        "  correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
        "elif len(class_targets.shape) == 2:  # Mask values - only for one-hot encoded labels\n",
        "  correct_confidences = np.sum(softmax_outputs * class_targets, axis=1)\n",
        "\n",
        "# Losses\n",
        "neg_log = - np.log(correct_confidences)\n",
        "average_loss = np.mean(neg_log)\n",
        "\n",
        "print(average_loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1VoBJMEbdf"
      },
      "source": [
        "Before we move on, there is one additional problem to solve. The softmax output, which is also an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences.\n",
        "It is possible that the model will have full confidence for one label making all the remaining confidences zero. Similarly, it is also possible that the model will assign full confidence to a value that wasn’t the target.\n",
        "\n",
        "If we then try to calculate the loss of this confidence of 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSDpFBrQEgdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de60532c-cb37-4444-f4ce-db1f6bfce331"
      },
      "source": [
        "-np.log(0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJK29STfaza"
      },
      "source": [
        "From the mathematical point of view, $log(0)$ is undefined. We already know the following dependence: if $y=log(x)$ , then $e^y =x$. The question of\n",
        "what the resulting $y$ is in $y=log(0)$ is the same as the question of what’s the $y$ in $e^y =0$.\n",
        "\n",
        "The negative natural logarithm of 0 , in Python with NumPy, equals an infinitely big number, rather than undefined, and prints a warning\n",
        "about a division by 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp4IJXchhQSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10669a96-9726-49ab-8c44-987a0c2ff79c"
      },
      "source": [
        "np.e ** (-np.inf)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekUEraKOhkhE"
      },
      "source": [
        "When calculating a derivative of the absolute value function, which\n",
        "does not exist for an input of 0 and we’ll have to make some decisions to work around this.\n",
        "\n",
        "With optimization, we will also have a problem calculating gradients, starting with a mean value of all sample-wise losses since a single infinite value in a list will cause the average of that list to also be infinite:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QYELXV8hy5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa36131-5907-45a2-f5e2-317b071f02bd"
      },
      "source": [
        "np.mean([1, 2, 3, -np.log(0)])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJfdXE66i8gB"
      },
      "source": [
        "We could add a very small value to the confidence to prevent it from being a zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ErSoNsHi9Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6f7743-1737-490f-fc1b-92d47d904f2f"
      },
      "source": [
        "-np.log(1e-7)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.11809565095832"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igB1bytOjYpj"
      },
      "source": [
        "Adding a very small value, one-tenth of a million, to the confidence at its far edge will insignificantly impact the result, but this method yields an additional 2 issues. \n",
        "\n",
        "First, in the case where the confidence value is 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bSD7aBKjcn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a696f87b-9fd5-4085-e680-b33d180e9259"
      },
      "source": [
        "-np.log(1 + 1e-7)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.999999505838704e-08"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msjw2eUrj087"
      },
      "source": [
        "When the model is fully correct in a prediction and puts all the confidence in the correct label, loss becomes a negative value instead of being 0.\n",
        "\n",
        "The other problem here is shifting confidence towards 1 , even if by a very small value. To prevent both issues, it’s better to clip values from\n",
        "both sides by the same number, `1e-7` in our case. \n",
        "\n",
        "That means that the lowest possible value will become `1e-7` (like in the demonstration we just performed) but the highest possible value, instead\n",
        "of being `1+1e-7`, will become `1-1e-7` (so slightly less than 1 ):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go8B5tIMksEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4c7e8c-dac2-4dd0-a1f8-13ab7e956387"
      },
      "source": [
        "-np.log(1 - 1e-7)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000494736474e-07"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY-3DxmRk2-_"
      },
      "source": [
        "This will prevent loss from being exactly 0 , making it a very small value instead, but won’t make it a negative value and won’t bias overall loss towards 1 . \n",
        "\n",
        "Within our code and using numpy, we’ll accomplish that using `np.clip()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz4pp8L4k7Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0d5747-5022-4278-f725-169cb6138ff0"
      },
      "source": [
        "y_pred = softmax_outputs\n",
        "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "y_pred_clipped"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7 , 0.1 , 0.2 ],\n",
              "       [0.1 , 0.5 , 0.4 ],\n",
              "       [0.02, 0.9 , 0.08]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WMdqi0xlma6"
      },
      "source": [
        "This method can perform clipping on an array of values, so we can apply it to the predictions directly and save this as a separate array, which we’ll use shortly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT5uPhR0l_FV"
      },
      "source": [
        "## Categorical Cross-Entropy Loss Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2RGv_MQmCK-"
      },
      "source": [
        "We’ll be adding more loss functions and some of the operations that we’ll be\n",
        "performing are common for all of them. One of these operations is how we calculate the overall loss — no matter which loss function we’ll use, the overall loss is always a mean value of all sample losses.\n",
        "\n",
        "Let’s create the Loss class containing the calculate method that will call our\n",
        "loss object’s forward method and calculate the mean value of the returned sample losses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu2xSw0Th6hx"
      },
      "source": [
        "class Loss:\n",
        "\n",
        "  # Calculates the data and regularization losses given model output and ground truth values\n",
        "  def calculate(self, output, y):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    return data_loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H3gtaaxjiXJ"
      },
      "source": [
        "Let’s convert our loss code into a class for convenience down the line:\n",
        "\n",
        "We calculate the probabilities for target values - only if categorical labels.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/7.png?raw=1' width='800'/>\n",
        "\n",
        "And we calculate the probabilities for mask values - only for one-hot encoded labels.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/8.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRA1ImaIjhsi"
      },
      "source": [
        "class CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0 \n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Probabilities for target values - only if categorical labels\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    elif len(y_true.shape) == 2:  # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlMKlhnilRYe"
      },
      "source": [
        "So, we have the following calculation for both cases:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/9.png?raw=1' width='800'/>\n",
        "\n",
        "This class inherits the Loss class and performs all the error calculations that we derived throughout and can be used as an object. \n",
        "\n",
        "For example, using the manually-created output and targets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbGj1NODlVj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895d2819-d313-44c8-ff76-efc46e111b80"
      },
      "source": [
        "loss_function = CategoricalCrossEntropy()\n",
        "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuASxzU8mJEK"
      },
      "source": [
        "## Combining everything "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na7Va6mHmLhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fa5e13-7007-4551-d521-0f8c1cd90485"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "relu1 = ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
        "dense2 = Dense(3, 3)\n",
        "# Create Softmax activation (to be used with Dense layer)\n",
        "softmax = Softmax()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = CategoricalCrossEntropy()\n",
        "\n",
        "# Perform a forward pass of our training data through first layer\n",
        "dense1.forward(X)\n",
        "# Perform a forward pass through activation function, it takes the output of first dense layer here\n",
        "relu1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer, it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(relu1.output)\n",
        "# Perform a forward pass through activation function, it takes the output of second dense layer here\n",
        "softmax.forward(dense2.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(softmax.output[:5])\n",
        "\n",
        "# Perform a forward pass through loss function, it takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(softmax.output, y)\n",
        "\n",
        "print(f\"Loss: {str(loss)}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n",
            "Loss: 1.0986104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpoygggxnwL"
      },
      "source": [
        "Again, we get `~0.33` values since the model is random, and its average loss is also not great for these data, as we’ve not yet trained our model on how to correct its errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8aZkmFYxqVn"
      },
      "source": [
        "## Accuracy Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOE3xyljxtjS"
      },
      "source": [
        "While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the accuracy , which describes how often the largest confidence is the correct class in terms of a fraction. \n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/10.png?raw=1' width='800'/>\n",
        "\n",
        "Conveniently, we can reuse existing variable definitions to calculate the accuracy metric. We will use the argmax values from the softmax outputs and then compare these to the targets.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/11.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTz_fH-B505P",
        "outputId": "2d55305f-4891-4007-81e3-f6629acdc3ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Probabilities for 3 samples\n",
        "softmax_outputs = np.array([\n",
        "  [ 0.7 , 0.2 , 0.1 ],\n",
        "  [ 0.5 , 0.1 , 0.4 ],\n",
        "  [ 0.02 , 0.9 , 0.08 ]\n",
        "])\n",
        "\n",
        "# Target (ground-truth) labels for 3 samples\n",
        "class_targets = np.array([0, 1, 1])\n",
        "\n",
        "# Calculate values along second axis (axis of index 1)\n",
        "predictions = np.argmax(softmax_outputs, axis=1)\n",
        "\n",
        "# If targets are one-hot encoded - convert them\n",
        "if len(class_targets.shape) == 2:\n",
        "  class_targets = np.argmax(class_targets, axis=1)\n",
        "\n",
        "# True evaluates to 1; False to 0\n",
        "accuracy = np.mean(predictions==class_targets)\n",
        "\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7uUfC0Z6IhV"
      },
      "source": [
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/12.png?raw=1' width='800'/>\n",
        "\n",
        "We are also handling one-hot encoded targets by converting them to sparse values using `np.argmax()`.\n",
        "\n",
        "Now, we can add the following to the end of our full script to calculate its accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha-TIPao6IqX",
        "outputId": "10a3775b-3c10-40c6-b035-b3086fea3166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "relu1 = ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
        "dense2 = Dense(3, 3)\n",
        "# Create Softmax activation (to be used with Dense layer)\n",
        "softmax = Softmax()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = CategoricalCrossEntropy()\n",
        "\n",
        "# Perform a forward pass of our training data through first layer\n",
        "dense1.forward(X)\n",
        "# Perform a forward pass through activation function, it takes the output of first dense layer here\n",
        "relu1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer, it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(relu1.output)\n",
        "# Perform a forward pass through activation function, it takes the output of second dense layer here\n",
        "softmax.forward(dense2.output)\n",
        "\n",
        "# Perform a forward pass through loss function, it takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(softmax.output, y)\n",
        "\n",
        "print(f\"Loss: {str(loss)}\")\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets calculate values along first axis\n",
        "predictions = np.argmax(softmax.output, axis=1)\n",
        "\n",
        "if len(y.shape) == 2:\n",
        "  y = np.argmax(y, axis=1)\n",
        "\n",
        "# True evaluates to 1; False to 0\n",
        "accuracy = np.mean(predictions==y)\n",
        "\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0986253\n",
            "accuracy:  0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1JDiWSp8WjH"
      },
      "source": [
        "Now that you’ve learned how to perform a forward pass through our network and calculate the metrics to signal if the model is performing poorly."
      ]
    }
  ]
}