{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-entropy-loss-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5C0EWNniYkVtunAYRIV81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/cross_entropy_loss_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24c1aatvQoZ"
      },
      "source": [
        "## Cross-Entropy Loss from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDb087qvmHZ"
      },
      "source": [
        "To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how much error the model has. The loss function , also referred to as the cost function , is the algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.\n",
        "\n",
        "The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk-HKrl2vy-D"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZn01YLCvUZW"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8S9cqiWv2fc"
      },
      "source": [
        "## Categorical Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5xA6L3Zv3hD"
      },
      "source": [
        "If you’re familiar with linear regression, then you already know one of the loss functions used with neural networks that do regression: squared error (or mean squared error with neural networks).\n",
        "\n",
        "The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. Categorical cross-entropy is explicitly used to compare a `ground-truth` probability ( y or `targets`) and some predicted distribution ( y-hat or `predictions`), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
        "\n",
        "The formula for calculating the categorical cross-entropy of `y` (actual/desired distribution) and `y-hat` (predicted distribution) is:\n",
        "\n",
        "$$\n",
        "L_i = - \\sum_j y_{i, j}log(\\hat{y_{i, j}})\n",
        "$$\n",
        "\n",
        "In general, the log loss error function is what we apply to the output of a binary logistic regression model — there are only two classes in the distribution, each of them applying to a single output (neuron) which is targeted as a `0` or `1`. In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy\n",
        "compares two probability distributions. \n",
        "\n",
        "In our case, we have a softmax output, let’s say it’s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NitS9Zy5ukE0"
      },
      "source": [
        "softmax_output = [0.7, 0.1, 0.2]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdzPB4nCrnB"
      },
      "source": [
        "We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). If that’s the intended prediction, then the desired probability distribution is `[1, 0, 0]`.\n",
        "\n",
        "Arrays or vectors like this are called one-hot , meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0. When comparing the model’s results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is\n",
        "multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy. \n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/05-calculating-loss/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "To exemplify this — if we take a softmax output of `[0.7, 0.1, 0.2]` and targets of `[1, 0, 0]` , we can apply the calculations as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHAvPTjCiSp",
        "outputId": "67f78beb-9a7b-431a-d0ca-6ca74a142677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L = -(1 * math.log(0.7) + 0 * math.log(0.1) + 0 * math.log(0.2))\n",
        "print(L)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEcZzwQZFF2-",
        "outputId": "9906bcc1-e7b3-481e-ffaa-344ebdcafec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L = -(-0.35667494393873245 + 0  + 0)\n",
        "print(L)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbeE3i6HFZVB"
      },
      "source": [
        "Let’s see the Python code for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5dhekhFZ2r",
        "outputId": "b3f96225-8556-44dc-9ae2-2a53d541b0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# An example output from the output layer of the neural network\n",
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "# Ground truth\n",
        "target_output = [1, 0, 0]\n",
        "\n",
        "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
        "         math.log(softmax_output[1]) * target_output[1] +\n",
        "         math.log(softmax_output[2]) * target_output[2])\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGD72WPH-gJ"
      },
      "source": [
        "That’s the full categorical cross-entropy calculation, but we can make a few assumptions given one-hot target vectors.\n",
        "\n",
        "The example confidence level might look like `[0.22, 0.6, 0.18]` or `[0.32, 0.36, 0.32]`. In both cases, the argmax of these vectors will return the second\n",
        "class as the prediction, but the model’s confidence about these predictions is high only for one of them. The Categorical Cross-Entropy Loss accounts for that and outputs a larger loss the lower the confidence is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i80a632pJPu0",
        "outputId": "088c17ec-95ed-4f9a-94af-b741ca85feea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (math.log( 1. ))\n",
        "print (math.log( 0.95 ))\n",
        "print (math.log( 0.9 ))\n",
        "print (math.log( 0.8 ))\n",
        "print ( '...' )\n",
        "print (math.log( 0.2 ))\n",
        "print (math.log( 0.1 ))\n",
        "print (math.log( 0.05 ))\n",
        "print (math.log( 0.01 ))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "-0.05129329438755058\n",
            "-0.10536051565782628\n",
            "-0.2231435513142097\n",
            "...\n",
            "-1.6094379124341003\n",
            "-2.3025850929940455\n",
            "-2.995732273553991\n",
            "-4.605170185988091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKCzLyhuJejz"
      },
      "source": [
        "When the confidence level equals 1 , meaning the model is 100% “sure” about its prediction, the loss value for this sample equals 0 . The loss value raises with the confidence level, approaching 0. You might also wonder\n",
        "why we did not print the result of log(0) — we’ll explain that shortly."
      ]
    }
  ]
}