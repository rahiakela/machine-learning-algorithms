{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dense-layer-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPq8mqrgBLlkAjy5zRTkLa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/03-adding-layers/dense_layer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTqbdiFikqtp"
      },
      "source": [
        "##Dense Layer from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfxGxrfrk2lV"
      },
      "source": [
        "Neural networks become “deep” when they have 2 or more hidden layers . At the\n",
        "moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent.\n",
        "\n",
        "Currently, we have no hidden layers. A\n",
        "hidden layer isn’t an input or output layer; as the scientist, you see data as they are handed to the\n",
        "input layer and the resulting data from the output layer. Layers between these endpoints have\n",
        "values that we don’t necessarily deal with, hence the name “hidden.”\n",
        "\n",
        "You will often use them to diagnose issues or\n",
        "improve your neural network. To explore this concept, let’s add another layer to this neural\n",
        "network, and, for now, let’s assume these two layers that we’re going to have will be the hidden\n",
        "layers, and we just have not coded our output layer yet.\n",
        "\n",
        "Before we add another layer, let’s think about what will be coming. In the case of the first layer,\n",
        "we can see that we have an input with 4 features.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/03-adding-layers/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "Samples (feature set data) get fed through the input, which does not change it in any way, to our\n",
        "first hidden layer, which we can see has 3 sets of weights, with 4 values each.\n",
        "\n",
        "Each of those 3 unique weight sets is associated with its distinct neuron. Thus, since we have 3\n",
        "weight sets, we have 3 neurons in this first hidden layer. Each neuron has a unique set of weights,\n",
        "of which we have 4 (as there are 4 inputs to this layer), which is why our initial weights have a\n",
        "shape of `(3,4)`.\n",
        "\n",
        "Now, we wish to add another layer. To do that, we must make sure that the expected input to\n",
        "that layer matches the previous layer’s output. We have set the number of neurons in a layer by\n",
        "setting how many weight sets and biases we have.\n",
        "\n",
        "The previous layer’s influence on weight sets\n",
        "for the current layer is that each weight set needs to have a separate weight per input. This\n",
        "means a distinct weight per neuron from the previous layer (or feature if we’re talking the\n",
        "input). The previous layer has 3 weight sets and 3 biases, so we know it has 3 neurons. This then\n",
        "means, for the next layer, we can have as many weight sets as we want (because this is how\n",
        "many neurons this new layer will have), but each of those weight sets must have 3 discrete weights.\n",
        "\n",
        "To create this new layer, we are going to copy and paste our weights and biases to weights2\n",
        "and biases2 , and change their values to new made up sets. Here’s an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaQqjEeEm9-y"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [\n",
        "  [1.0, 2.0, 3.0, 2.5],\n",
        "  [2.0, 5.0, -1.0, 2.0], \n",
        "  [-1.5, 2.7, 3.3, -0.8]\n",
        "]\n",
        "\n",
        "weights = [\n",
        "  [0.2, 0.8, -0.5, 1.0],\n",
        "  [0.5, -0.91, 0.26, -0.5],\n",
        "  [-0.26, -0.27, 0.17, 0.87]\n",
        "]          \n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "weights2 = [\n",
        "  [0.1, -0.14, 0.5],\n",
        "  [-0.5, 0.12, -0.33],\n",
        "  [-0.44, 0.73, -0.13]\n",
        "]          \n",
        "biases2 = [-1.0, 2.0, -0.5]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYWNe0CpunO"
      },
      "source": [
        "Next, we will now call outputs `layer1_ouputs`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-6ZiKGspyqt",
        "outputId": "325fee0c-b5fd-4135-e976-000379b1877f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "print(layer1_outputs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lrS_mEMp3_A"
      },
      "source": [
        "As previously stated, inputs to layers are either inputs from the actual dataset you’re training with\n",
        "or outputs from a previous layer. \n",
        "\n",
        "That’s why we defined 2 versions of weights and biases but only\n",
        "1 of inputs — because the inputs for layer 2 will be the outputs from the previous layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP7biPLmqOJ_",
        "outputId": "c3ebdad2-0a0e-4d42-def7-b8e29b93ddd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "print(layer2_outputs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01zarV3tqmSc"
      },
      "source": [
        "All together now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJLCV05NqqQZ",
        "outputId": "fca39e36-2e23-4132-b325-2edd318fc680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs = [\n",
        "  [1.0, 2.0, 3.0, 2.5],\n",
        "  [2.0, 5.0, -1.0, 2.0], \n",
        "  [-1.5, 2.7, 3.3, -0.8]\n",
        "]\n",
        "\n",
        "weights = [\n",
        "  [0.2, 0.8, -0.5, 1.0],\n",
        "  [0.5, -0.91, 0.26, -0.5],\n",
        "  [-0.26, -0.27, 0.17, 0.87]\n",
        "]          \n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "weights2 = [\n",
        "  [0.1, -0.14, 0.5],\n",
        "  [-0.5, 0.12, -0.33],\n",
        "  [-0.44, 0.73, -0.13]\n",
        "]          \n",
        "biases2 = [-1.0, 2.0, -0.5]\n",
        "\n",
        "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "print(layer2_outputs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_LEQGrWq7De"
      },
      "source": [
        "At this point, our neural network could be visually represented as:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/03-adding-layers/images/2.png?raw=1' width='600'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRtaHom0rA7x"
      },
      "source": [
        "##Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHJL04bOrBhW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FCKLrVLq0Hj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}