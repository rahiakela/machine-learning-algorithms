{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-networks-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtaI9TZ9ibehOTAwiADi+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/neural_networks_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbnwqiAyQMSy"
      },
      "source": [
        "##A Single Neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiZNd6RrQfMs"
      },
      "source": [
        "Let’s say we have a single neuron, and there are three inputs to this neuron. As in most cases, when you initialize parameters in neural networks, our network will have weights initialized randomly, and biases set as zero to start.\n",
        "\n",
        "The input will be either actual training data or the outputs of neurons from the previous layer in the neural network. We’re just going to make up values to start with as input for now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBYRtb8sSQ73"
      },
      "source": [
        "inputs = [1, 2, 3]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbOogcYbWX9R"
      },
      "source": [
        "Each input also needs a weight associated with it. Inputs are the data that we pass into the model\n",
        "to get desired outputs, while the weights are the parameters that we’ll tune later on to get these\n",
        "results. Weights are one of the types of values that change inside the model during the training\n",
        "phase, along with biases that also change during training. The values for weights and biases are\n",
        "what get “trained,” and they are what make a model actually work (or not work).\n",
        "\n",
        "Let’s say the first input, at index 0, which is a 1, has a weight of\n",
        "0.2, the second input has a weight of 0.8, and the third input has a weight of -0.5. \n",
        "\n",
        "Our input and weights lists should now be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1svllTbZWqaH"
      },
      "source": [
        "inputs = [1, 2, 3]\n",
        "weights = [0.2, 0.8, -0.5]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMMJEvJXW4Tc"
      },
      "source": [
        "Next, we need the bias. At the moment, we’re modeling a single neuron with three inputs. Since\n",
        "we’re modeling a single neuron, we only have one bias, as there’s just one bias value per neuron.\n",
        "\n",
        "The bias is an additional tunable value but is not associated with any input in contrast to the\n",
        "weights. We’ll randomly select a value of 2 as the bias for this example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4XU7xdiW9fF"
      },
      "source": [
        "inputs = [1, 2, 3]\n",
        "weights = [0.2, 0.8, -0.5]\n",
        "bias = 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtJ2N4R7XGrH"
      },
      "source": [
        "This neuron sums each input multiplied by that input’s weight, then adds the bias. All the neuron\n",
        "does is take the fractions of inputs, where these fractions (weights) are the adjustable parameters,\n",
        "and adds another adjustable parameter — the bias — then outputs the result.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/1.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPn8NcBXAD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9cd706-423b-498a-d80f-3a371238aa30"
      },
      "source": [
        "output = (inputs[0] * weights[0] +\n",
        "          inputs[1] * weights[1] +\n",
        "          inputs[2] * weights[2] +\n",
        "          bias)\n",
        "print(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJN3A1PvX24l"
      },
      "source": [
        "What might we need to change if we have 4 inputs, rather than the 3 we’ve just shown? \n",
        "\n",
        "Next to\n",
        "the additional input, we need to add an associated weight, which this new input will be multiplied\n",
        "with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URjVpIV9XbSU"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZForwbsYEBa"
      },
      "source": [
        "Which could be depicted visually as:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "All together in code, including the new input and weight, to produce output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUf6vFr8YOc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe67cc02-1f08-414f-8757-9dc09606c2e1"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "output = (inputs[0] * weights[0] +\n",
        "          inputs[1] * weights[1] +\n",
        "          inputs[2] * weights[2] +\n",
        "          inputs[3] * weights[3] +\n",
        "          bias)\n",
        "print(output)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DcsGuOwYa0z"
      },
      "source": [
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/3.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyTJiXp5YhmI"
      },
      "source": [
        "##A Layer of Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXHBBu8KYic_"
      },
      "source": [
        "Neural networks typically have layers that consist of more than one neuron. Layers are nothing\n",
        "more than groups of neurons. Each neuron in a layer takes exactly the same input — the input\n",
        "given to the layer (which can be either the training data or the output from the previous layer),\n",
        "but contains its own set of weights and its own bias, producing its own unique output. The layer’s\n",
        "output is a set of each of these outputs — one per each neuron. \n",
        "\n",
        "Let’s say we have a scenario with\n",
        "3 neurons in a layer and 4 inputs:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "We’ll keep the initial 4 inputs and set of weights for the first neuron the same as we’ve been using\n",
        "so far. We’ll add 2 additional, made up, sets of weights and 2 additional biases to form 2 new\n",
        "neurons for a total of 3 in the layer. \n",
        "\n",
        "The layer’s output is going to be a list of 3 values, not just a\n",
        "single value like for a single neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZk6m7RYWmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61177d9e-ff4d-43fb-975a-93b999144856"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "\n",
        "bias1 = 2.0\n",
        "bias2 = 3.0\n",
        "bias3 = 0.5\n",
        "\n",
        "output = [\n",
        "    # Neuron 1\n",
        "    inputs[0] * weights1[0] +\n",
        "    inputs[1] * weights1[1] +\n",
        "    inputs[2] * weights1[2] +\n",
        "    inputs[3] * weights1[3] + bias1,\n",
        "\n",
        "    # Neuron 2\n",
        "    inputs[0] * weights2[0] +\n",
        "    inputs[1] * weights2[1] +\n",
        "    inputs[2] * weights2[2] +\n",
        "    inputs[3] * weights2[3] + bias2,\n",
        "\n",
        "    # Neuron 3\n",
        "    inputs[0] * weights3[0] +\n",
        "    inputs[1] * weights3[1] +\n",
        "    inputs[2] * weights3[2] +\n",
        "    inputs[3] * weights3[3] + bias3,\n",
        "]\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUr7KWc9cLNg"
      },
      "source": [
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "Each\n",
        "neuron is “connected” to the same inputs. The difference is in the separate weights and bias\n",
        "that each neuron applies to the input. This is called a fully connected neural network — every\n",
        "neuron in the current layer has connections to every neuron from the previous layer.\n",
        "\n",
        "At this point, we have only shown code for a single layer\n",
        "with very few neurons. Imagine coding many more layers and more neurons. This would get\n",
        "very challenging to code using our current methods. \n",
        "\n",
        "Instead, we could use a loop to scale and\n",
        "handle dynamically-sized inputs and layers. We’ve turned the separate weight variables into a\n",
        "list of weights so we can iterate over them, and we changed the code to use loops instead of\n",
        "the hardcoded operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-UeX_xab5YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f19be4-9867-41e5-a635-3d8aa7c15984"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights = [\n",
        "  [0.2, 0.8, -0.5, 1.0],\n",
        "  [0.5, -0.91, 0.26, -0.5],\n",
        "  [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "# Output of current layer\n",
        "layer_outputs = []\n",
        "# For each neuron\n",
        "for n_weights, n_bias in zip(weights, biases):\n",
        "  # Zeroed output of given neuron\n",
        "  n_output = 0\n",
        "  # For each input and weight to the neuron\n",
        "  for n_input, weight in zip(inputs, n_weights):\n",
        "    # Multiply this input by associated weight and add to the neuron’s output variable\n",
        "    n_output += n_input * weight\n",
        "  # Add bias\n",
        "  n_output += n_bias\n",
        "  # Put neuron’s result to the layer’s output list\n",
        "  layer_outputs.append(n_output)\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SefajBUwehkG"
      },
      "source": [
        "This does the same thing as before, just in a more dynamic and scalable way.\n",
        "\n",
        "Again, all we’re doing is, for each neuron (the outer loop in the code above, over neuron weights\n",
        "and biases), taking each input value multiplied by the associated weight for that input (the inner\n",
        "loop in the code above, over inputs and weights), adding all of these together, then adding a bias\n",
        "at the end. Finally, sending the neuron’s output to the layer’s output list.\n",
        "\n",
        "That’s it! How do we know we have three neurons? Why do we have three?\n",
        "\n",
        "We can tell we have\n",
        "three neurons because there are 3 sets of weights and 3 biases.\n",
        "\n",
        "When you make a neural network\n",
        "of your own, you also get to decide how many neurons you want for each of the layers. You can\n",
        "combine however many inputs you are given with however many neurons that you desire.\n",
        "\n",
        "**With our above code that uses loops, we could modify our number of inputs or neurons in our\n",
        "layer to be whatever we wanted, and our loop would handle it.**\n",
        "\n",
        "It would be a disservice not to show NumPy here since Python alone doesn’t do matrix/tensor/array math very efficiently.\n",
        "\n",
        "But first, the reason the most popular deep learning library in Python is\n",
        "called **TensorFlow** is that it’s all about doing operations on tensors ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A7EWxpwhAEl"
      },
      "source": [
        "##Dot Product and Vector Addition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jIBjlwIhBMl"
      },
      "source": [
        "Let’s now address vector multiplication, as that’s one of the most important operations we’ll\n",
        "perform on vectors. We can achieve the same result as in our pure Python implementation of\n",
        "multiplying each element in our inputs and weights vectors element-wise by using a dot product ,\n",
        "which we’ll explain shortly.\n",
        "\n",
        "Traditionally, we use dot products for vectors (yet another name for\n",
        "a container), and we can certainly refer to what we’re doing here as working with vectors just as\n",
        "we can call them “tensors.”\n",
        "\n",
        "When multiplying vectors, you either perform a dot product or a cross product. A cross\n",
        "product results in a vector while a dot product results in a scalar (a single value/number).\n",
        "\n",
        "First, let’s explain what a dot product of two vectors is. Mathematicians would say:\n",
        "\n",
        "$$\n",
        "\\overrightarrow a. \\overrightarrow b = \\sum_{i=1}^n a_i b_i = a_1 b_1 + a_2 b_2 + ... + a_n b_n\n",
        "$$\n",
        "\n",
        "A dot product of two vectors is a sum of products of consecutive vector elements. Both vectors\n",
        "must be of the same size (have an equal number of elements).\n",
        "\n",
        "\n",
        "Let’s write out how a dot product is calculated in Python. For it, you have two vectors, which we\n",
        "can represent as lists in Python. We then multiply their elements from the same index values and\n",
        "then add all of the resulting products. Say we have two lists acting as our vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KhCzFW_eOIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1caeb907-2b35-4e11-9de7-1242fd90a889"
      },
      "source": [
        "a = [1, 2, 3]\n",
        "b = [2, 3, 4]\n",
        "\n",
        "# calculate the dot product\n",
        "dot_product = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n",
        "print(dot_product)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rHBQYL7WAM7"
      },
      "source": [
        "Now, what if we called a “inputs” and b “weights?” Suddenly, this dot product looks like a\n",
        "succinct way to perform the operations we need and have already performed in plain Python. We\n",
        "need to multiply our weights and inputs of the same index values and add the resulting values\n",
        "together. The dot product performs this exact type of operation; thus, it makes lots of sense to use\n",
        "here.\n",
        "\n",
        "Plain Python does\n",
        "not contain methods or functions to perform such an operation, so we’ll use the NumPy package.\n",
        "\n",
        "NumPy lets us perform this in a natural way — using the plus sign with the variables containing\n",
        "vectors of the data. The addition of the two vectors is an operation performed element-wise,\n",
        "which means that both vectors have to be of the same size, and the result will become a vector of\n",
        "this size as well. The result is a vector calculated as a sum of the consecutive vector elements:\n",
        "\n",
        "$$\n",
        "\\overrightarrow a. \\overrightarrow b = [a_1 + b_1 , a_2 + b_2 , ..., a_n + b_n]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8YLe6SVWl-S"
      },
      "source": [
        "## A Single Neuron with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZebVKFjWmv9"
      },
      "source": [
        "Let’s code the solution, for a single neuron to start, using the dot product and the addition of the\n",
        "vectors with NumPy. \n",
        "\n",
        "This makes the code much simpler to read and write (and faster to run):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5KIZxsuUnCj"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5GXxyR8W1v0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9922d8ec-8111-4f65-f6ec-acacfb7a8cf8"
      },
      "source": [
        "inputs = [1.0 , 2.0 , 3.0 , 2.5]\n",
        "weights = [0.2 , 0.8 , - 0.5 , 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "outputs = np.dot(weights, inputs) + bias\n",
        "print(outputs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUdLeYCaXF1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffc6f8a-015f-4813-9706-8a405e90f71f"
      },
      "source": [
        "outputs = np.dot([0.2 , 0.8 , - 0.5 , 1.0], [1.0 , 2.0 , 3.0 , 2.5]) + 2.0\n",
        "print(outputs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1tMghMLXkUZ"
      },
      "source": [
        "## A Layer of Neurons with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV4XMymtXlIu"
      },
      "source": [
        "Now, we’d like to calculate the output of a layer of 3 neurons,\n",
        "which means the weights will be a matrix or list of weight vectors. In plain Python, we wrote this as a list of lists. With NumPy, this will be a 2-dimensional array, which we’ll call a matrix.\n",
        "\n",
        "\n",
        "The weights are now a matrix, and we need\n",
        "to perform a dot product of them and the input vector. NumPy makes this very easy for us —\n",
        "treating this matrix as a list of vectors and performing the dot product one by one with the vector\n",
        "of inputs, returning a list of dot products.\n",
        "\n",
        "The biases can be easily added to the result of the dot product operation as they are a vector of the\n",
        "same size. We can also use the plain Python list directly here, as NumPy will convert it to an\n",
        "array internally.\n",
        "\n",
        "When we add two vectors using NumPy, each i-th element is added together, resulting\n",
        "in a new vector of the same size. This is both a simplification and an optimization, giving us\n",
        "simpler and faster code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bx10xRQXfZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b25cee-e87d-42fe-b1df-73c24f36d21a"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights = [\n",
        "  [0.2, 0.8, -0.5, 1.0],\n",
        "  [0.5, -0.91, 0.26, -0.5],\n",
        "  [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "layer_outputs = np.dot(weights, inputs) + biases  # shape: (3 x 4) (4 x 1) = (1 x 3)\n",
        "print(layer_outputs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8   1.21  2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuL3RnNOSW3t"
      },
      "source": [
        "# layer_outputs = np.dot(inputs, weights) + biases # shape: (4 x 1) (3 x 4) = (1 x 3) mis-match\n",
        "# print(layer_outputs)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gc36ianH3Wo"
      },
      "source": [
        "```\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-15-c5d15def04c9> in <module>()\n",
        "----> 1 layer_outputs = np.dot(inputs, weights) + biases # shape: (4 x 1) (3 x 4) = (1 x 3) mis-match\n",
        "      2 print(layer_outputs)\n",
        "\n",
        "<__array_function__ internals> in dot(*args, **kwargs)\n",
        "\n",
        "ValueError: shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM0GCFd6UcnS"
      },
      "source": [
        "To explain the order of parameters we are passing into np.dot() , we should think of it as whatever\n",
        "comes first will decide the output shape. In our case, we are passing a list of neuron weights first\n",
        "and then the inputs, as our goal is to get a list of neuron outputs. \n",
        "\n",
        "A dot product of a matrix and a vector results in a list of dot products. The np.dot() method treats the matrix as\n",
        "a list of vectors and performs a dot product of each of those vectors with the other vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw_KDxpeHvqq"
      },
      "source": [
        "##A Batch of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks971whJHw7J"
      },
      "source": [
        "**To train, neural networks tend to receive data in batches.** So far, the example input data have\n",
        "been only one sample (or observation ) of various features called a feature set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsYglkqASjO4"
      },
      "source": [
        "inputs = [ 1 , 2 , 3 , 2.5 ]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFhKi7XAIJmn"
      },
      "source": [
        "Each of these values is a feature observation datum, and together they form a\n",
        "feature set instance , also called an observation , or most commonly, a sample.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/5b1.png?raw=1' width='800'/>\n",
        "\n",
        "Often, neural networks expect to take in many samples at a time for two reasons. One reason\n",
        "is that it’s faster to train in batches in parallel processing, and the other reason is that batches help with generalization during training.\n",
        "\n",
        "If you fit (perform a step of a training process) on one\n",
        "sample at a time, you’re highly likely to keep fitting to that individual sample, rather than\n",
        "slowly producing general tweaks to weights and biases that fit the entire dataset. \n",
        "\n",
        "Fitting or\n",
        "training in batches gives you a higher chance of making more meaningful changes to weights and biases.\n",
        "\n",
        "An example of a batch of data could look like:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/5b2.png?raw=1' width='800'/>\n",
        "\n",
        "Recall that in Python, lists are useful containers for holding a sample as well\n",
        "as multiple samples that make up a batch of observations. Such an example of a batch of\n",
        "observations, each with its own sample, looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA51Ae6xIE66"
      },
      "source": [
        "inputs = [[ 1 , 2 , 3 , 2.5 ],\n",
        "          [ 2 , 5 , - 1 , 2 ], \n",
        "          [ - 1.5 , 2.7 , 3.3 , - 0.8 ]]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_CwArUhKZ6_"
      },
      "source": [
        "This list of lists could be made into an array since it is homologous. Note that each “list” in this\n",
        "larger list is a sample representing a feature set. `[ 1 , 2 , 3 , 2.5 ] , [ 2 , 5 , - 1 , 2 ]` , and\n",
        "`[ - 1.5 , 2.7 , 3.3 , - 0.8 ]` are all samples , and are also referred to as feature set instances or\n",
        "observations .\n",
        "\n",
        "We have a matrix of inputs and a matrix of weights now, and we need to perform the dot product\n",
        "on them somehow, but how and what will the result be?\n",
        "\n",
        "Similarly, as we performed a dot product\n",
        "on a matrix and a vector, we treated the matrix as a list of vectors, resulting in a list of dot\n",
        "products.\n",
        "\n",
        "In this example, we need to manage both matrices as lists of vectors and perform dot\n",
        "products on all of them in all combinations, resulting in a list of lists of outputs, or a matrix; this\n",
        "operation is called the matrix product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJO2oOLK0JK"
      },
      "source": [
        "## Matrix Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_ObLDAK1Ib"
      },
      "source": [
        "The matrix product is an operation in which we have 2 matrices, and we are performing dot\n",
        "products of all combinations of rows from the first matrix and the columns of the 2nd matrix,\n",
        "resulting in a matrix of those atomic dot products:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/6.png?raw=1' width='400'/>\n",
        "\n",
        "To perform a matrix product, the size of the second dimension of the left matrix must match the size of the first dimension of the right matrix.\n",
        "\n",
        "For example, if the left matrix has a shape of (5, 4)\n",
        "then the right matrix must match this 4 within the first shape value (4, 7) . The shape of the\n",
        "resulting array is always the first dimension of the left array and the second dimension of the right array, (5, 7).\n",
        "\n",
        "In the above example, the left matrix has a shape of (5, 4) , and the upper-right matrix\n",
        "has a shape of (4, 5) . The second dimension of the left array and the first dimension of the second\n",
        "array are both 4 , they match, and the resulting array has a shape of (5, 5).\n",
        "\n",
        "In\n",
        "mathematics, we can have something called a column vector and row vector, which we’ll explain\n",
        "better shortly. They’re vectors, but represented as matrices with one of the dimensions having a size of 1:\n",
        "\n",
        "$a =\n",
        " \\begin{pmatrix}\n",
        "  1 & 2 & 3 & 4\n",
        " \\end{pmatrix}$\n",
        "\n",
        " $b =\n",
        " \\begin{pmatrix}\n",
        "  1 \\\\\n",
        "  2 \\\\\n",
        "  3 \\\\\n",
        "  4 \n",
        " \\end{pmatrix}$\n",
        "\n",
        " a is a row vector. It looks very similar to a vector a.\n",
        "\n",
        " The difference in notation between a row vector and vector are\n",
        "commas between values and the arrow above symbol a is missing on a row vector. It’s called a\n",
        "row vector as it’s a vector of a row of a matrix. b , on the other hand, is called a column vector\n",
        "because it’s a column of a matrix. As row and column vectors are technically matrices, we do not\n",
        "denote them with vector arrows anymore.\n",
        "\n",
        "When we perform the matrix product on them, the result becomes a matrix as well, but containing just a single value, the same value as in the dot product.\n",
        "\n",
        "$ab =\n",
        " \\begin{pmatrix}\n",
        "  1 & 2 & 3\n",
        " \\end{pmatrix}\n",
        " \\begin{pmatrix}\n",
        "  2 \\\\\n",
        "  3 \\\\\n",
        "  4 \n",
        " \\end{pmatrix} = \\begin{pmatrix}\n",
        "  20\n",
        " \\end{pmatrix}$\n",
        "\n",
        " <img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/02-coding-neurons/images/7.png?raw=1' width='400'/>\n",
        "\n",
        " In other words, row and column vectors are matrices with one of their dimensions being of a\n",
        "size of 1; and, we perform the matrix product on them instead of the dot product , which\n",
        "results in a matrix containing a single value. \n",
        "\n",
        "In this case, we performed a matrix multiplication\n",
        "of matrices with shapes `(1, 3)` and `(3, 1)` , then the resulting array has the shape `(1, 1)` or a size of `1x1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Sf3WKPOS1P"
      },
      "source": [
        "###Transposition for the Matrix Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45uRBxTYOUI7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCzOuAmaKU37"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}