{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relu-activation-function-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOoIQqtFPu4G0GTqvFh9sBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/relu_activation_function_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVN7MSLMJtcs"
      },
      "source": [
        "## ReLU Activation Function from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOw70_GPK0Ct"
      },
      "source": [
        "The rectified linear activation function is simpler than the sigmoid. It’s quite literally $y=x$ , clipped at $\\theta$ from the negative side. If $x$ is less than or equal to $\\theta$ , then $y$ is $\\theta$ — otherwise, $y$ is equal to $x$.\n",
        "\n",
        "$$\n",
        "y = {\\displaystyle \\textstyle {\\begin{cases} x, \\space \\space  x > 0 \\\\ 0, \\space \\space x < 0 \\end{cases}}}\n",
        "$$\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "This simple yet powerful activation function is the most widely used activation function at the time of writing for various reasons — mainly speed and efficiency.\n",
        "\n",
        "The ReLU activation function is extremely close to being a linear activation\n",
        "function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fNuHPzUHgC"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkbCKevUTHd"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zyu2BvrUIfx"
      },
      "source": [
        "from nnfs.datasets import spiral_data\n",
        "import numpy as np\n",
        "import nnfs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nnfs.init()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQ-JkNxO6mB"
      },
      "source": [
        "## ReLU Activation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2wyHCSO_MH"
      },
      "source": [
        "Despite the fancy sounding name, the rectified linear activation function is straightforward to code. Most closely to its definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgWiNgVPH6-",
        "outputId": "9c52f70e-acf5-43c3-f2d5-f6dedceb409c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "  if i > 0:     # if the current value is greater than 0, appending the current value\n",
        "    output.append(i)\n",
        "  else:         # if it’s not, appending 0\n",
        "    output.append(0)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXAA0LmQs-Y"
      },
      "source": [
        "This can be written more simply, as we just need to take the largest of two values: 0 or neuron value. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LkxmcD7QvHs",
        "outputId": "3d4d5c17-dde9-4f0e-b595-c8f2d65e3bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "    output.append(max(0, i))\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzIcdbwkRHTO"
      },
      "source": [
        "NumPy contains an equivalent — `np.maximum()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoQxoq5mRGom",
        "outputId": "14c49070-306c-40cd-f6fd-11675c0b12e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = np.maximum(0, inputs)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKHq89P3SvWW"
      },
      "source": [
        "This method compares each element of the input list (or an array) and returns an object of the same shape filled with new values. \n",
        "\n",
        "We will use it in our new rectified linear activation class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHxwxrlSm-A"
      },
      "source": [
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "    return self.output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEOJYfDETSDY",
        "outputId": "6003ade1-d8c6-407b-9914-2cc358cd7c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "relu = ReLU()\n",
        "print(relu.forward(inputs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VM6YmV4T7e8"
      },
      "source": [
        "Let’s apply this activation function to the dense layer’s outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5EzXLztVFgq"
      },
      "source": [
        "class Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"Layer initialization: Initialize weights and biases\"\"\"\n",
        "    # Note that we’re initializing weights to be (inputs, neurons), rather than ( neurons, inputs)\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    # a bias can ensure that a neuron fires initially. so initializing it with zero\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-zA_dBbTtIG",
        "outputId": "ca17db82-ab92-4eae-d61e-bd76e857cf0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "relu = ReLU()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "relu.forward(dense1.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(f\"Before ReLU:\\n {dense1.output[:5]}\")\n",
        "print(f\"After ReLU:\\n {relu.output[:5]}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU:\n",
            " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [ 1.2344425e-04 -4.2612613e-05  8.7741073e-06]\n",
            " [ 2.5981691e-04 -1.5019374e-04  2.1353657e-05]\n",
            " [ 4.0532288e-04 -3.3390927e-04  3.8064085e-05]\n",
            " [ 5.5259373e-04 -6.4534455e-04  6.0963972e-05]]\n",
            "After ReLU:\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [1.2344425e-04 0.0000000e+00 8.7741073e-06]\n",
            " [2.5981691e-04 0.0000000e+00 2.1353657e-05]\n",
            " [4.0532288e-04 0.0000000e+00 3.8064085e-05]\n",
            " [5.5259373e-04 0.0000000e+00 6.0963972e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c5CVCrQX4Pu"
      },
      "source": [
        "As you can see, negative values have been clipped (modified to be zero). That’s all there is to the rectified linear activation function used in the hidden layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkt9ugvwX7OZ"
      },
      "source": [
        "##Softmax Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXo40PH9TxER"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}