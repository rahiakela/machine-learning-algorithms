{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relu-activation-function-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbjR8pALw9OHiTGgEdhsr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/activation_functions_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVN7MSLMJtcs"
      },
      "source": [
        "## ReLU Activation Function from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOw70_GPK0Ct"
      },
      "source": [
        "The rectified linear activation function is simpler than the sigmoid. It’s quite literally $y=x$ , clipped at $\\theta$ from the negative side. If $x$ is less than or equal to $\\theta$ , then $y$ is $\\theta$ — otherwise, $y$ is equal to $x$.\n",
        "\n",
        "$$\n",
        "y = {\\displaystyle \\textstyle {\\begin{cases} x, \\space \\space  x > 0 \\\\ 0, \\space \\space x < 0 \\end{cases}}}\n",
        "$$\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "This simple yet powerful activation function is the most widely used activation function at the time of writing for various reasons — mainly speed and efficiency.\n",
        "\n",
        "The ReLU activation function is extremely close to being a linear activation\n",
        "function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fNuHPzUHgC"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkbCKevUTHd"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zyu2BvrUIfx"
      },
      "source": [
        "from nnfs.datasets import spiral_data\n",
        "import numpy as np\n",
        "import nnfs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nnfs.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQ-JkNxO6mB"
      },
      "source": [
        "## ReLU Activation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2wyHCSO_MH"
      },
      "source": [
        "Despite the fancy sounding name, the rectified linear activation function is straightforward to code. Most closely to its definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgWiNgVPH6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9c22c3-c922-44ef-8ddc-ca7187e7c368"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "  if i > 0:     # if the current value is greater than 0, appending the current value\n",
        "    output.append(i)\n",
        "  else:         # if it’s not, appending 0\n",
        "    output.append(0)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXAA0LmQs-Y"
      },
      "source": [
        "This can be written more simply, as we just need to take the largest of two values: 0 or neuron value. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LkxmcD7QvHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bc224d-bc91-4082-fae4-e58f92a50424"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "    output.append(max(0, i))\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzIcdbwkRHTO"
      },
      "source": [
        "NumPy contains an equivalent — `np.maximum()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoQxoq5mRGom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbaf67c-8717-4400-b23c-394203f335af"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = np.maximum(0, inputs)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKHq89P3SvWW"
      },
      "source": [
        "This method compares each element of the input list (or an array) and returns an object of the same shape filled with new values. \n",
        "\n",
        "We will use it in our new rectified linear activation class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHxwxrlSm-A"
      },
      "source": [
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "    return self.output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEOJYfDETSDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7954a1-6201-4424-fb16-2c8f48f5f7ef"
      },
      "source": [
        "relu = ReLU()\n",
        "print(relu.forward(inputs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VM6YmV4T7e8"
      },
      "source": [
        "Let’s apply this activation function to the dense layer’s outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5EzXLztVFgq"
      },
      "source": [
        "class Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"Layer initialization: Initialize weights and biases\"\"\"\n",
        "    # Note that we’re initializing weights to be (inputs, neurons), rather than ( neurons, inputs)\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    # a bias can ensure that a neuron fires initially. so initializing it with zero\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-zA_dBbTtIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc797d24-1fa4-4729-eee6-3e51cadacc01"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "relu = ReLU()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "relu.forward(dense1.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(f\"Before ReLU:\\n {dense1.output[:5]}\")\n",
        "print(f\"After ReLU:\\n {relu.output[:5]}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU:\n",
            " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n",
            "After ReLU:\n",
            " [[0.         0.         0.        ]\n",
            " [0.         0.00011395 0.        ]\n",
            " [0.         0.00031729 0.        ]\n",
            " [0.         0.00052666 0.        ]\n",
            " [0.         0.00071401 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c5CVCrQX4Pu"
      },
      "source": [
        "As you can see, negative values have been clipped (modified to be zero). That’s all there is to the rectified linear activation function used in the hidden layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkt9ugvwX7OZ"
      },
      "source": [
        "##Softmax Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH5bwLGFg5pN"
      },
      "source": [
        "In our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification.\n",
        "\n",
        "In this case, the rectified linear unit is unbounded, not normalized with other units, and exclusive. \n",
        "- **Not normalized** implies the values can be anything, an output of [12, 99, 318] is without context\n",
        "- **exclusive** means each output is independent of the others\n",
        "\n",
        "To address this lack of context, the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes.\n",
        "\n",
        "In the case of classification, what we want to see is a prediction of which class the network “thinks” the input represents. \n",
        "\n",
        "This distribution returned by the softmax activation function represents confidence scores for each class and will add up to 1.\n",
        "\n",
        "For example, if our network has a confidence distribution for two classes: \n",
        "`[0.45, 0.55]`, the prediction is the 2nd class, but the confidence in this\n",
        "prediction isn’t very high. Maybe our program would not act in this case since it’s not very confident.\n",
        "\n",
        "Here’s the function for the Softmax :\n",
        "\n",
        "$$\n",
        "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^L e^{z_{i,j}}}\n",
        "$$\n",
        "\n",
        "The first step for us is to “exponentiate” the outputs. We do this with Euler’s number, $e$, which is roughly `2.71828182846` and referred to as the “exponential growth” number.\n",
        "\n",
        "Both the numerator and the denominator of the Softmax function contain $e$ raised to the power of $z$ , where $z$ , given indices, means a singular output value — the index $i$ means the current sample and the index $j$ means the current output in this sample. \n",
        "\n",
        "The numerator exponentiates the current output value and the denominator takes a sum of all of the exponentiated outputs for a given sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXo40PH9TxER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd278a1-9ed6-43fa-a5cb-5b2dd83e4134"
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# e - mathematical constant, we use E here to match a common coding style where constants are uppercased\n",
        "E = 2.71828182846  # you can also use math.e\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = []\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E ** output)  # ** - power operator in Python\n",
        "\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZMC7pNl1zt"
      },
      "source": [
        "To calculate the probabilities, we need non-negative values. Imagine the output as `[ 4.8 , 1.21 , - 2.385 ]` — even after normalization, the last\n",
        "value will still be negative since we’ll just divide all of them by their sum.\n",
        "\n",
        "A negative probability (or confidence) does not make much sense. An exponential value of any number is always non-negative — it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "The exponential function is a monotonic function. This means that, with higher input values, outputs are also higher, so we won’t change the predicted class after applying it while making sure that we get non-negative values. It also adds stability to the result as the normalized exponentiation is more about the difference between numbers than their magnitudes.\n",
        "\n",
        "Once we’ve exponentiated, we want to convert these numbers to a probability distribution (converting the values into the vector of confidences, one for each class, which add up to 1 for everything in the vector). What that means is that we’re about to perform a normalization where we take a given\n",
        "value and divide it by the sum of all of the values.\n",
        "\n",
        "Since each output value normalizes to a fraction of the sum, all of the values are now in the range of 0 to 1 and add up to 1 — they share the probability of 1 between themselves.\n",
        "\n",
        "Let’s add the sum and normalization to the Softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEgwXY_cnmjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8e3661-a551-4cd7-c674-f64ee45ea846"
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# e - mathematical constant, we use E here to match a common coding style where constants are uppercased\n",
        "E = 2.71828182846  # you can also use math.e\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = []\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E ** output)  # ** - power operator in Python\n",
        "\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)\n",
        "\n",
        "# Now normalize values\n",
        "norm_base = sum(exp_values)  # we sum all values\n",
        "norm_values = []\n",
        "for value in exp_values:\n",
        "  norm_values.append(value / norm_base)\n",
        "\n",
        "print(\"Normalized exponentiated values:\")\n",
        "print(norm_values)\n",
        "print(f\"Sum of normalized values:{sum(norm_values)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n",
            "Normalized exponentiated values:\n",
            "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
            "Sum of normalized values:1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmbC1g3do0z9"
      },
      "source": [
        "We can perform the same set of operations with the use of NumPy in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCDqmqFEo1g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08c052d-3369-47d0-dd8e-0f4088264ec0"
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)\n",
        "\n",
        "# Now normalize values\n",
        "norm_base = exp_values / np.sum(exp_values)  # we sum all values\n",
        "print(\"Normalized exponentiated values:\")\n",
        "print(norm_values)\n",
        "print(f\"Sum of normalized values:{np.sum(norm_values)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041752   3.35348465  10.85906266]\n",
            "Normalized exponentiated values:\n",
            "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
            "Sum of normalized values:1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLEvctXZpuoc"
      },
      "source": [
        "Notice the results are similar, but faster to calculate and the code is easier to read with NumPy.\n",
        "\n",
        "We can exponentiate all of the values with a single call of the `np.exp()`, then immediately normalize them with the sum. To train in batches, we need to convert this functionality to accept layer outputs in batches.\n",
        "\n",
        "```python\n",
        "# Get unnormalized probabilities\n",
        "exp_values = np.exp(inputs)\n",
        "\n",
        "# Normalize them for each sample\n",
        "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "```\n",
        "\n",
        "We should also address what axis and keepdims mean in the above. Let’s first discuss the axis . Axis is easier to show than tell, but, in a 2D array/matrix, axis 0 refers to the rows, and axis 1 refers to the columns. \n",
        "\n",
        "Let’s see some examples of how axis affects the sum using NumPy. First, we\n",
        "will just show the default, which is None.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDxsjIpGpYg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c99bfd7-d099-481d-9462-0c0b81993c17"
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = np.array([\n",
        "  [4.8, 1.21, 2.385],\n",
        "  [8.9 , - 1.81 , 0.2],\n",
        "  [1.41 , 1.051 , 0.026]\n",
        "]) \n",
        "\n",
        "print(\"Sum without axis:\")\n",
        "print(np.sum(layer_outputs))\n",
        "\n",
        "print(\"This will be identical to the above since default is None:\")\n",
        "print(np.sum(layer_outputs, axis=None))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum without axis:\n",
            "18.172\n",
            "This will be identical to the above since default is None:\n",
            "18.172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKw7-x549m4Y"
      },
      "source": [
        "With no axis specified, we are just summing all of the values, even if they’re in varying dimensions.\n",
        "\n",
        "Next, `axis = 0`. This means to sum row-wise, along axis 0.\n",
        "\n",
        "In the case of our 2D array, where we have only a single other dimension, the columns, the output vector will sum these columns. \n",
        "\n",
        "This means we’ll perform 4.8+8.9+1.41 and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIqQ6CEP9V-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95139e8c-1801-4433-800c-a5b365d6a2e2"
      },
      "source": [
        "print(\"Another way to think of it w/ a matrix == axis 0: columns:\")\n",
        "print(np.sum(layer_outputs, axis=0))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Another way to think of it w/ a matrix == axis 0: columns:\n",
            "[15.11   0.451  2.611]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI2_5PZM-4r8"
      },
      "source": [
        "This isn’t what we want, though. We want sums of the rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQOOIfPH-TYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591da5b9-cd4e-4a26-b2d3-12e5799b7e49"
      },
      "source": [
        "print(\"But we want to sum the rows instead, like this w/ raw py:\")\n",
        "for i in layer_outputs:\n",
        "  print(sum(i))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "But we want to sum the rows instead, like this w/ raw py:\n",
            "8.395\n",
            "7.29\n",
            "2.4869999999999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zFVV_vwBn5q"
      },
      "source": [
        "As you probably guessed, we’re going to sum along axis 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlkuEXjNBoa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4523d2-4f64-4bab-8362-238ecb88cb07"
      },
      "source": [
        "print(\"So we can sum axis 1, but note the current shape:\")\n",
        "print(np.sum(layer_outputs, axis=1))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So we can sum axis 1, but note the current shape:\n",
            "[8.395 7.29  2.487]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUpST142CbYT"
      },
      "source": [
        "As pointed out by “ note the current shape ,” we did get the sums that we expected, but actually, we want to simplify the outputs to a single value per sample. We’re trying to sum all the outputs from a layer for each sample in a batch; converting the layer’s output array with row length equal to the number of neurons in the layer, to just one value. \n",
        "\n",
        "We need a column vector with these values since it will let us normalize the whole batch of samples, sample-wise, with a single calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKQgd9cDCiKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56e145e-9613-4e76-bdd7-4b032b7f1093"
      },
      "source": [
        "layer_outputs.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia_jpziSCo_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f826201-883a-4455-ebd9-4a5a99f31596"
      },
      "source": [
        "print(\"Sum axis 1, but keep the same dimensions as input:\")\n",
        "print(layer_outputs)\n",
        "print(np.sum(layer_outputs, axis=1, keepdims=True))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum axis 1, but keep the same dimensions as input:\n",
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n",
            "[[8.395]\n",
            " [7.29 ]\n",
            " [2.487]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm6j5K3mDpXJ"
      },
      "source": [
        "Now, if we divide the array containing a batch of the outputs with this array, NumPy will perform this sample-wise. That means that it’ll divide all of the values from each output row by the corresponding row from the sum array. Since\n",
        "this sum in each row is a single value, it’ll be used for the division with every value from the corresponding output row). \n",
        "\n",
        "We can combine all of this into a softmax class, like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTWSxTIkDyhg"
      },
      "source": [
        "# Softmax activation\n",
        "class Softmax:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjU6CKKEF0Xy"
      },
      "source": [
        "Finally, we also included a subtraction of the largest of the inputs before we did the exponentiation. There are two main pervasive challenges with neural networks: “dead neurons” and very large numbers (referred to as “exploding” values). “Dead” neurons and enormous numbers can wreak havoc down the line and render a network useless over time. The exponential function used in softmax activation is one of the sources of exploding values. \n",
        "\n",
        "Let’s see some examples of how and why this can easily happen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H55wFdhZF-Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce2f708-b477-4901-9644-094f4da7687c"
      },
      "source": [
        "print(np.exp(1))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.718281828459045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvsHhmFVGKNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df045894-6d33-4110-e622-f859aa6da15f"
      },
      "source": [
        "print(np.exp(10))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22026.465794806718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qRpIcayGOHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb1c602-dc75-45a3-f235-c205179eb689"
      },
      "source": [
        "print(np.exp(100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6881171418161356e+43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kNf3-61GS1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e69990-b734-45f2-9b7c-4862d1c9a5ea"
      },
      "source": [
        "print(np.exp(1000))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L1E1uneGhKD"
      },
      "source": [
        "It doesn’t take a very large number, in this case, a mere 1,000 , to cause an overflow error. \n",
        "\n",
        "We know the exponential function tends toward 0 as its input value approaches negative infinity, and the output is 1 when the input is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWaY2D6SGwIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65841a4e-8452-4d54-c1c4-83463b5e4ecc"
      },
      "source": [
        "print(np.exp(-np.inf), np.exp(0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GVdntuHA__"
      },
      "source": [
        "We can use this property to prevent the exponential function from overflowing.\n",
        "\n",
        "With Softmax, thanks to the normalization, we can subtract any value from all of the inputs, and it will not change the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMYe1JAPHW60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cece98-8d9c-4609-a9e4-783841018302"
      },
      "source": [
        "softmax = Softmax()\n",
        "softmax.forward([[1, 2, 3]])\n",
        "print(softmax.output)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pKVF21UIAat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177f42ee-a751-487d-8a05-b49c3728bfd2"
      },
      "source": [
        "# subtracted 3 - max from the list\n",
        "softmax.forward([[-2, -1, 0]])\n",
        "print(softmax.output)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyN9ylTbIaEU"
      },
      "source": [
        "This is another useful property of the exponentiated and normalized function. \n",
        "\n",
        "There’s one more thing to mention in addition to these calculations. \n",
        "\n",
        "What happens if we divide the layer’s output data, `[1, 2, 3]` , for example, by 2?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkUPkLJIq-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c99768-d207-4f6d-85ae-b96a3bc73be6"
      },
      "source": [
        "softmax.forward([[.5, 1, 1.5]])\n",
        "print(softmax.output)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.18632372 0.30719589 0.50648039]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HySPfRBEJDvp"
      },
      "source": [
        "The output confidences have changed due to the nonlinearity nature of the exponentiation. This is one example of why we need to scale all of the input data to a neural network in the same way.\n",
        "\n",
        "Now, we can add another dense layer as the output layer, setting it to contain as many inputs as the previous layer has outputs and as many outputs as our data includes classes. \n",
        "\n",
        "Then we can apply the softmax activation to the output of this new layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NoHX62WI1IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da84a31d-59cd-49be-da6f-72086f2500cd"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "relu = ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
        "dense2 = Dense(3, 3)\n",
        "\n",
        "# Create Softmax activation (to be used with Dense layer):\n",
        "softmax = Softmax()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Make a forward pass through activation function it takes the output of first dense layer here\n",
        "relu.forward(dense1.output)\n",
        "\n",
        "# Make a forward pass through second Dense layer it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(relu.output)\n",
        "\n",
        "# Make a forward pass through activation function it takes the output of second dense layer here\n",
        "softmax.forward(dense2.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(softmax.output[:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyXmnnH0Xza-"
      },
      "source": [
        "These outputs are also our “confidence scores.” To determine which classification the model has chosen to be the prediction, we perform an argmax on these outputs, which checks which of the classes in the output distribution has the highest confidence and returns its index -the predicted class index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96WA61txX_kK"
      },
      "source": [
        "## Putting all things together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIrS5VHoYDSr",
        "outputId": "b4101f28-1c5f-4aed-ff2f-2ec81ee00afd"
      },
      "source": [
        "# Dense layer\n",
        "class Dense:\n",
        "\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  \n",
        "# ReLU activation\n",
        "class ReLU:\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "# Softmax activation\n",
        "class Softmax:\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "relu = ReLU()\n",
        "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
        "dense2 = Dense(3, 3)\n",
        "# Create Softmax activation (to be used with Dense layer):\n",
        "softmax = Softmax()\n",
        "\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "# Make a forward pass through activation function it takes the output of first dense layer here\n",
        "relu.forward(dense1.output)\n",
        "# Make a forward pass through second Dense layer it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(relu.output)\n",
        "# Make a forward pass through activation function it takes the output of second dense layer here\n",
        "softmax.forward(dense2.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(softmax.output[:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333412 0.3333327  0.33333313]\n",
            " [0.33333495 0.333332   0.33333302]\n",
            " [0.3333358  0.3333313  0.33333293]\n",
            " [0.33333617 0.33333114 0.3333327 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duyota6Kbk_N"
      },
      "source": [
        "We’ve completed what we need for forward-passing data through our model. We used the Rectified Linear (ReLU ) activation function on the hidden layer, which works on a per-neuron basis. We additionally used the Softmax activation function for the output layer since it accepts non-normalized values as input and outputs a probability distribution, which we’re using as confidence scores for each class.\n",
        "\n",
        "Recall that, although neurons are interconnected, they each have\n",
        "their respective weights and biases and are not “normalized” with each other.\n",
        "\n",
        "As you can see, our example model is currently random. To remedy this, **we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time**. Thus, **our next step is to quantify how wrong the model is through what’s defined as a loss function.**"
      ]
    }
  ]
}