{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relu-activation-function-from-scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNCqZbfCVHL1DFqmNxRU1iW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/activation_functions_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVN7MSLMJtcs"
      },
      "source": [
        "## ReLU Activation Function from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOw70_GPK0Ct"
      },
      "source": [
        "The rectified linear activation function is simpler than the sigmoid. It’s quite literally $y=x$ , clipped at $\\theta$ from the negative side. If $x$ is less than or equal to $\\theta$ , then $y$ is $\\theta$ — otherwise, $y$ is equal to $x$.\n",
        "\n",
        "$$\n",
        "y = {\\displaystyle \\textstyle {\\begin{cases} x, \\space \\space  x > 0 \\\\ 0, \\space \\space x < 0 \\end{cases}}}\n",
        "$$\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "This simple yet powerful activation function is the most widely used activation function at the time of writing for various reasons — mainly speed and efficiency.\n",
        "\n",
        "The ReLU activation function is extremely close to being a linear activation\n",
        "function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fNuHPzUHgC"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkbCKevUTHd"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zyu2BvrUIfx"
      },
      "source": [
        "from nnfs.datasets import spiral_data\n",
        "import numpy as np\n",
        "import nnfs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nnfs.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQ-JkNxO6mB"
      },
      "source": [
        "## ReLU Activation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2wyHCSO_MH"
      },
      "source": [
        "Despite the fancy sounding name, the rectified linear activation function is straightforward to code. Most closely to its definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgWiNgVPH6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b88bfc-e75f-4918-9edd-e60334fb179b"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "  if i > 0:     # if the current value is greater than 0, appending the current value\n",
        "    output.append(i)\n",
        "  else:         # if it’s not, appending 0\n",
        "    output.append(0)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXAA0LmQs-Y"
      },
      "source": [
        "This can be written more simply, as we just need to take the largest of two values: 0 or neuron value. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LkxmcD7QvHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f7920a-5764-4c97-99f7-28e570c5781a"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "    output.append(max(0, i))\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzIcdbwkRHTO"
      },
      "source": [
        "NumPy contains an equivalent — `np.maximum()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoQxoq5mRGom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7937987-df93-4d11-f75d-2adacec6ffea"
      },
      "source": [
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "\n",
        "output = np.maximum(0, inputs)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKHq89P3SvWW"
      },
      "source": [
        "This method compares each element of the input list (or an array) and returns an object of the same shape filled with new values. \n",
        "\n",
        "We will use it in our new rectified linear activation class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHxwxrlSm-A"
      },
      "source": [
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "    return self.output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEOJYfDETSDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682c62cc-54c7-40aa-f219-79cbc0272302"
      },
      "source": [
        "relu = ReLU()\n",
        "print(relu.forward(inputs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VM6YmV4T7e8"
      },
      "source": [
        "Let’s apply this activation function to the dense layer’s outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5EzXLztVFgq"
      },
      "source": [
        "class Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"Layer initialization: Initialize weights and biases\"\"\"\n",
        "    # Note that we’re initializing weights to be (inputs, neurons), rather than ( neurons, inputs)\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    # a bias can ensure that a neuron fires initially. so initializing it with zero\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU activation class\n",
        "class ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-zA_dBbTtIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba62d14-2dcf-4ed1-d53b-5e19a6e3e706"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "relu = ReLU()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "relu.forward(dense1.output)\n",
        "\n",
        "# Let's see output of the first few samples\n",
        "print(f\"Before ReLU:\\n {dense1.output[:5]}\")\n",
        "print(f\"After ReLU:\\n {relu.output[:5]}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU:\n",
            " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n",
            "After ReLU:\n",
            " [[0.         0.         0.        ]\n",
            " [0.         0.00011395 0.        ]\n",
            " [0.         0.00031729 0.        ]\n",
            " [0.         0.00052666 0.        ]\n",
            " [0.         0.00071401 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c5CVCrQX4Pu"
      },
      "source": [
        "As you can see, negative values have been clipped (modified to be zero). That’s all there is to the rectified linear activation function used in the hidden layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkt9ugvwX7OZ"
      },
      "source": [
        "##Softmax Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH5bwLGFg5pN"
      },
      "source": [
        "In our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification.\n",
        "\n",
        "In this case, the rectified linear unit is unbounded, not normalized with other units, and exclusive. \n",
        "- **Not normalized** implies the values can be anything, an output of [12, 99, 318] is without context\n",
        "- **exclusive** means each output is independent of the others\n",
        "\n",
        "To address this lack of context, the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes.\n",
        "\n",
        "In the case of classification, what we want to see is a prediction of which class the network “thinks” the input represents. \n",
        "\n",
        "This distribution returned by the softmax activation function represents confidence scores for each class and will add up to 1.\n",
        "\n",
        "For example, if our network has a confidence distribution for two classes: \n",
        "`[0.45, 0.55]`, the prediction is the 2nd class, but the confidence in this\n",
        "prediction isn’t very high. Maybe our program would not act in this case since it’s not very confident.\n",
        "\n",
        "Here’s the function for the Softmax :\n",
        "\n",
        "$$\n",
        "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^L e^{z_{i,j}}}\n",
        "$$\n",
        "\n",
        "The first step for us is to “exponentiate” the outputs. We do this with Euler’s number, $e$, which is roughly `2.71828182846` and referred to as the “exponential growth” number.\n",
        "\n",
        "Both the numerator and the denominator of the Softmax function contain $e$ raised to the power of $z$ , where $z$ , given indices, means a singular output value — the index $i$ means the current sample and the index $j$ means the current output in this sample. \n",
        "\n",
        "The numerator exponentiates the current output value and the denominator takes a sum of all of the exponentiated outputs for a given sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXo40PH9TxER",
        "outputId": "f5a6b7d7-c74e-4bf3-a3d5-24d3c2042df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# e - mathematical constant, we use E here to match a common coding style where constants are uppercased\n",
        "E = 2.71828182846  # you can also use math.e\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = []\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E ** output)  # ** - power operator in Python\n",
        "\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZMC7pNl1zt"
      },
      "source": [
        "To calculate the probabilities, we need non-negative values. Imagine the output as `[ 4.8 , 1.21 , - 2.385 ]` — even after normalization, the last\n",
        "value will still be negative since we’ll just divide all of them by their sum.\n",
        "\n",
        "A negative probability (or confidence) does not make much sense. An exponential value of any number is always non-negative — it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values:\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-algorithms/blob/main/neural-networks-from-scratch/04-activation-function/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "The exponential function is a monotonic function. This means that, with higher input values, outputs are also higher, so we won’t change the predicted class after applying it while making sure that we get non-negative values. It also adds stability to the result as the normalized exponentiation is more about the difference between numbers than their magnitudes.\n",
        "\n",
        "Once we’ve exponentiated, we want to convert these numbers to a probability distribution (converting the values into the vector of confidences, one for each class, which add up to 1 for everything in the vector). What that means is that we’re about to perform a normalization where we take a given\n",
        "value and divide it by the sum of all of the values.\n",
        "\n",
        "Since each output value normalizes to a fraction of the sum, all of the values are now in the range of 0 to 1 and add up to 1 — they share the probability of 1 between themselves.\n",
        "\n",
        "Let’s add the sum and normalization to the Softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEgwXY_cnmjv",
        "outputId": "cff7c2f9-e928-4596-fdb5-6a0b192ec084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# e - mathematical constant, we use E here to match a common coding style where constants are uppercased\n",
        "E = 2.71828182846  # you can also use math.e\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = []\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E ** output)  # ** - power operator in Python\n",
        "\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)\n",
        "\n",
        "# Now normalize values\n",
        "norm_base = sum(exp_values)  # we sum all values\n",
        "norm_values = []\n",
        "for value in exp_values:\n",
        "  norm_values.append(value / norm_base)\n",
        "\n",
        "print(\"Normalized exponentiated values:\")\n",
        "print(norm_values)\n",
        "print(f\"Sum of normalized values:{sum(norm_values)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n",
            "Normalized exponentiated values:\n",
            "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
            "Sum of normalized values:1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmbC1g3do0z9"
      },
      "source": [
        "We can perform the same set of operations with the use of NumPy in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCDqmqFEo1g5",
        "outputId": "a9cf5f16-48f8-46c8-f307-e17e6ed66c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Values from the previous output when we described what a neural network is\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print(\"exponentiated values:\")\n",
        "print(exp_values)\n",
        "\n",
        "# Now normalize values\n",
        "norm_base = exp_values / np.sum(exp_values)  # we sum all values\n",
        "print(\"Normalized exponentiated values:\")\n",
        "print(norm_values)\n",
        "print(f\"Sum of normalized values:{np.sum(norm_values)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041752   3.35348465  10.85906266]\n",
            "Normalized exponentiated values:\n",
            "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
            "Sum of normalized values:1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLEvctXZpuoc"
      },
      "source": [
        "Notice the results are similar, but faster to calculate and the code is easier to read with NumPy.\n",
        "\n",
        "We can exponentiate all of the values with a single call of the `np.exp()`, then immediately normalize them with the sum. To train in batches, we need to convert this functionality to accept layer outputs in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDxsjIpGpYg2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}